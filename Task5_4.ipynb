{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import osmnx as ox\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, LineString, box\n",
    "import networkx as nx\n",
    "from dataclasses import dataclass\n",
    "import ast\n",
    "import rtree\n",
    "import logging\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from math import pi\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "from folium.features import DivIcon\n",
    "from branca.colormap import LinearColormap\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "SIGMA_Z = 15.0  # Increased sigma_z for more tolerance in emission\n",
    "MAX_DISTANCE = 50.0  # Increased max_distance for broader candidate search\n",
    "TURN_ANGLE_THRESHOLD = pi / 4  # 45 degrees threshold for transition penalty\n",
    "MIN_TRANSITION_PROB = 1e-5  # Non-zero transition probability for flexibility\n",
    "\n",
    "class EnhancedViterbiMatcher:\n",
    "    def __init__(self, graph, edges_gdf, config=None):\n",
    "        \"\"\"Initialize matcher with improved configuration\"\"\"\n",
    "        self.graph = graph\n",
    "        self.edges_gdf = edges_gdf.copy()\n",
    "        \n",
    "        if isinstance(self.edges_gdf.index, pd.MultiIndex):\n",
    "            self.edges_gdf = self.edges_gdf.reset_index(drop=True)\n",
    "        self.edges_gdf.index = range(len(self.edges_gdf))\n",
    "        \n",
    "        # Enhanced default configuration\n",
    "        default_config = {\n",
    "            'max_candidates': 20,          # Increased from 10\n",
    "            'max_distance': 1000.0,         # Increased from 50.0\n",
    "            'sigma_z': 50.0,              # Adjusted for better GPS noise handling\n",
    "            'beta': 2.0,                  # Increased for better transition scoring\n",
    "            'min_prob_norm': 1e-7,        # Lowered for more flexibility\n",
    "            'max_speed': 50.0,            # Maximum expected speed (m/s)\n",
    "            'min_speed': 0.1,             # Minimum expected speed (m/s)\n",
    "            'angle_tolerance': np.pi/2,    # 90 degrees angle tolerance\n",
    "            'max_angle_penalty': 0.5,      # Maximum penalty for sharp turns\n",
    "            'distance_decay': 0.85,        # Distance decay factor\n",
    "            'sequential_matching': True    # Enable sequential matching for long trajectories\n",
    "        }\n",
    "        \n",
    "        if config:\n",
    "            default_config.update(config)\n",
    "        self.config = default_config\n",
    "        \n",
    "        self._init_spatial_index()\n",
    "        self.edge_to_nodes = self._build_edge_to_nodes()\n",
    "        self.node_to_edges = self._build_node_to_edges()\n",
    "        \n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def _init_spatial_index(self):\n",
    "        \"\"\"Initialize R-tree spatial index with improved error handling\"\"\"\n",
    "        try:\n",
    "            self.spatial_index = rtree.index.Index()\n",
    "            for idx, edge in self.edges_gdf.iterrows():\n",
    "                if edge.geometry is not None and not edge.geometry.is_empty:\n",
    "                    self.spatial_index.insert(idx, edge.geometry.bounds)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error initializing spatial index: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _build_edge_to_nodes(self) -> Dict[int, set]:\n",
    "        \"\"\"Build mapping from edge IDs to their endpoint nodes with validation\"\"\"\n",
    "        edge_to_nodes = {}\n",
    "        for idx, edge in self.edges_gdf.iterrows():\n",
    "            if edge.geometry is not None and not edge.geometry.is_empty:\n",
    "                coords = list(edge.geometry.coords)\n",
    "                if len(coords) >= 2:  # Ensure valid linestring\n",
    "                    edge_to_nodes[idx] = {\n",
    "                        self._get_node_id(coords[0]),\n",
    "                        self._get_node_id(coords[-1])\n",
    "                    }\n",
    "        return edge_to_nodes\n",
    "\n",
    "    def _build_node_to_edges(self) -> Dict[tuple, set]:\n",
    "        \"\"\"Build mapping from nodes to connected edge IDs\"\"\"\n",
    "        node_to_edges = {}\n",
    "        for edge_id, nodes in self.edge_to_nodes.items():\n",
    "            for node in nodes:\n",
    "                if node not in node_to_edges:\n",
    "                    node_to_edges[node] = set()\n",
    "                node_to_edges[node].add(edge_id)\n",
    "        return node_to_edges\n",
    "\n",
    "    def _get_node_id(self, coord: tuple) -> tuple:\n",
    "        \"\"\"Convert coordinate to node ID with improved precision\"\"\"\n",
    "        return tuple(round(x, 6) for x in coord)\n",
    "\n",
    "    def _find_candidates(self, point: Point) -> List[dict]:\n",
    "        \"\"\"Enhanced candidate finding with adaptive search radius\"\"\"\n",
    "        candidates = []\n",
    "        initial_distance = 30.0  # Start with a reduced search radius\n",
    "        max_attempts = 3\n",
    "        current_distance = initial_distance\n",
    "        \n",
    "        for attempt in range(max_attempts):\n",
    "            bounds = (\n",
    "                point.x - current_distance,\n",
    "                point.y - current_distance,\n",
    "                point.x + current_distance,\n",
    "                point.y + current_distance\n",
    "            )\n",
    "            \n",
    "            for idx in self.spatial_index.intersection(bounds):\n",
    "                edge = self.edges_gdf.loc[idx]\n",
    "                if edge.geometry is not None:\n",
    "                    dist = point.distance(edge.geometry)\n",
    "                    if dist <= current_distance:\n",
    "                        proj_point = edge.geometry.interpolate(\n",
    "                            edge.geometry.project(point)\n",
    "                        )\n",
    "                        candidates.append({\n",
    "                            'edge_id': idx,\n",
    "                            'distance': dist,\n",
    "                            'proj_point': proj_point,\n",
    "                            'edge': edge\n",
    "                        })\n",
    "            \n",
    "            if candidates:\n",
    "                break\n",
    "                \n",
    "            current_distance *= 1.5  # Increase search radius for next attempt\n",
    "        \n",
    "        candidates.sort(key=lambda x: x['distance'])\n",
    "        return candidates[:self.config['max_candidates']]\n",
    "\n",
    "    def _calculate_emission_prob(self, point: Point, candidate: dict) -> float:\n",
    "        \"\"\"Enhanced emission probability calculation with improved scaling\"\"\"\n",
    "        distance = candidate['distance']\n",
    "        sigma_z = self.config['sigma_z']\n",
    "        \n",
    "        # Distance-based probability with decay\n",
    "        distance_factor = np.exp(-distance * self.config['distance_decay'])\n",
    "        \n",
    "        # Gaussian probability\n",
    "        gaussian_prob = np.exp(-0.5 * (distance / sigma_z) ** 2)\n",
    "        \n",
    "        # Combined probability\n",
    "        prob = gaussian_prob * distance_factor\n",
    "        \n",
    "        return max(prob, self.config['min_prob_norm'])\n",
    "\n",
    "    def _calculate_transition_prob(self, prev_edge: int, curr_edge: int,\n",
    "                                 prev_point: Point, curr_point: Point) -> float:\n",
    "        \"\"\"Enhanced transition probability with improved angle handling\"\"\"\n",
    "        prev_nodes = self.edge_to_nodes[prev_edge]\n",
    "        curr_nodes = self.edge_to_nodes[curr_edge]\n",
    "        \n",
    "        connected = bool(prev_nodes.intersection(curr_nodes))\n",
    "        connectivity_score = 1.0 if connected else 0.3\n",
    "        \n",
    "        dir1 = np.array(prev_point.coords[-1]) - np.array(prev_point.coords[0])\n",
    "        dir2 = np.array(curr_point.coords[-1]) - np.array(curr_point.coords[0])\n",
    "        \n",
    "        norm1 = np.linalg.norm(dir1)\n",
    "        norm2 = np.linalg.norm(dir2)\n",
    "        \n",
    "        if norm1 == 0 or norm2 == 0:\n",
    "            angle_score = 1.0\n",
    "        else:\n",
    "            cos_angle = np.dot(dir1, dir2) / (norm1 * norm2)\n",
    "            angle = np.arccos(np.clip(cos_angle, -1.0, 1.0))\n",
    "            \n",
    "            angle_score = 1.0 - (angle / self.config['angle_tolerance']) * self.config['max_angle_penalty']\n",
    "            angle_score = max(angle_score, 1.0 - self.config['max_angle_penalty'])\n",
    "        \n",
    "        prob = connectivity_score * angle_score\n",
    "        return max(prob, self.config['min_prob_norm'])\n",
    "\n",
    "    def _viterbi_matching(self, points: List[Point], candidates_by_point: List[List[dict]]) -> List[Dict]:\n",
    "        \"\"\"Improved Viterbi algorithm with better numerical stability\"\"\"\n",
    "        n_points = len(points)\n",
    "        states = [{} for _ in range(n_points)]\n",
    "        \n",
    "        # Initialize first state\n",
    "        for candidate in candidates_by_point[0]:\n",
    "            edge_id = candidate['edge_id']\n",
    "            log_emission = np.log(self._calculate_emission_prob(points[0], candidate))\n",
    "            states[0][edge_id] = {\n",
    "                'log_prob': log_emission,\n",
    "                'prev': None,\n",
    "                'emission': log_emission,\n",
    "                'transition': 0.0\n",
    "            }\n",
    "        \n",
    "        # Forward pass with log probabilities\n",
    "        for t in range(1, n_points):\n",
    "            for candidate in candidates_by_point[t]:\n",
    "                curr_edge = candidate['edge_id']\n",
    "                log_emission = np.log(self._calculate_emission_prob(points[t], candidate))\n",
    "                \n",
    "                best_log_prob = float('-inf')\n",
    "                best_prev = None\n",
    "                best_transition = None\n",
    "                \n",
    "                for prev_edge, prev_state in states[t-1].items():\n",
    "                    trans_prob = self._calculate_transition_prob(\n",
    "                        prev_edge, curr_edge, points[t-1], points[t]\n",
    "                    )\n",
    "                    log_transition = np.log(trans_prob)\n",
    "                    \n",
    "                    log_prob = prev_state['log_prob'] + log_transition + log_emission\n",
    "                    \n",
    "                    if log_prob > best_log_prob:\n",
    "                        best_log_prob = log_prob\n",
    "                        best_prev = prev_edge\n",
    "                        best_transition = log_transition\n",
    "                \n",
    "                if best_prev is not None:\n",
    "                    states[t][curr_edge] = {\n",
    "                        'log_prob': best_log_prob,\n",
    "                        'prev': best_prev,\n",
    "                        'emission': log_emission,\n",
    "                        'transition': best_transition\n",
    "                    }\n",
    "        \n",
    "        # Convert log probabilities to normalized confidence scores\n",
    "        if states[-1]:\n",
    "            log_probs = np.array([state['log_prob'] for state in states[-1].values()])\n",
    "            max_log_prob = np.max(log_probs)\n",
    "            normalized_probs = np.exp(log_probs - max_log_prob)\n",
    "            normalized_probs /= np.sum(normalized_probs)\n",
    "            \n",
    "            for edge_id, norm_prob in zip(states[-1].keys(), normalized_probs):\n",
    "                states[-1][edge_id]['confidence'] = norm_prob\n",
    "        \n",
    "        return states\n",
    "\n",
    "    def _backtrack(self, states: List[Dict]) -> List[int]:\n",
    "        \"\"\"Backtrack to find the best path with improved handling of edge cases\"\"\"\n",
    "        if not states or not states[-1]:\n",
    "            return []\n",
    "        \n",
    "        path = []\n",
    "        current_edge = max(states[-1].items(), key=lambda x: x[1]['log_prob'])[0]\n",
    "        \n",
    "        for t in range(len(states) - 1, -1, -1):\n",
    "            path.append(current_edge)\n",
    "            if t > 0 and states[t][current_edge]['prev'] is not None:\n",
    "                current_edge = states[t][current_edge]['prev']\n",
    "        \n",
    "        return list(reversed(path))\n",
    "\n",
    "    def _sequential_matching(self, points: List[Point]) -> Dict:\n",
    "        \"\"\"Match long trajectories in sequential segments with overlap\"\"\"\n",
    "        segment_size = 30\n",
    "        overlap = 10\n",
    "        all_edges = []\n",
    "        segment_confidences = []\n",
    "        \n",
    "        for i in range(0, len(points), segment_size - overlap):\n",
    "            segment = points[i:i + segment_size]\n",
    "            if len(segment) < 2:\n",
    "                continue\n",
    "                \n",
    "            candidates = [self._find_candidates(p) for p in segment]\n",
    "            if not all(candidates):\n",
    "                continue\n",
    "                \n",
    "            states = self._viterbi_matching(segment, candidates)\n",
    "            path = self._backtrack(states)\n",
    "            \n",
    "            if path:\n",
    "                if states[-1] and path[-1] in states[-1]:\n",
    "                    segment_confidences.append(states[-1][path[-1]].get('confidence', 0.0))\n",
    "                    \n",
    "                if all_edges and overlap > 0:\n",
    "                    all_edges = all_edges[:-overlap]\n",
    "                all_edges.extend(path)\n",
    "        \n",
    "        if not all_edges:\n",
    "            return {'success': False, 'edges': [], 'confidence': 0.0}\n",
    "        \n",
    "        overall_confidence = np.mean(segment_confidences) if segment_confidences else 0.0\n",
    "            \n",
    "        return {\n",
    "            'success': True,\n",
    "            'edges': all_edges,\n",
    "            'confidence': overall_confidence\n",
    "        }\n",
    "\n",
    "    def match_trajectory(self, points: List[Tuple[float, float]]) -> Dict:\n",
    "        \"\"\"Match trajectory with improved error handling and validation\"\"\"\n",
    "        try:\n",
    "            if len(points) < 2:\n",
    "                return {'success': False, 'edges': [], 'confidence': 0.0}\n",
    "\n",
    "            point_objects = [Point(p) for p in points]\n",
    "            \n",
    "            if self.config['sequential_matching'] and len(points) > 50:\n",
    "                return self._sequential_matching(point_objects)\n",
    "            \n",
    "            candidates_by_point = [self._find_candidates(p) for p in point_objects]\n",
    "            \n",
    "            if not all(candidates_by_point):\n",
    "                return {'success': False, 'edges': [], 'confidence': 0.0}\n",
    "            \n",
    "            states = self._viterbi_matching(point_objects, candidates_by_point)\n",
    "            path = self._backtrack(states)\n",
    "            \n",
    "            if not path:\n",
    "                return {'success': False, 'edges': [], 'confidence': 0.0}\n",
    "            \n",
    "            confidence = states[-1][path[-1]].get('confidence', 0.0)\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'edges': path,\n",
    "                'confidence': confidence\n",
    "            }\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in match_trajectory: {str(e)}\")\n",
    "            return {'success': False, 'edges': [], 'confidence': 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_identifier(edge_data) -> str:\n",
    "    \"\"\"Get a unique identifier for an edge, falling back to alternatives if OSMID is not available\"\"\"\n",
    "    if 'OSMID' in edge_data:\n",
    "        return str(edge_data['OSMID'])\n",
    "    elif 'osmid' in edge_data:  # Try lowercase version\n",
    "        return str(edge_data['osmid'])\n",
    "    elif 'name' in edge_data:\n",
    "        return f\"road_{edge_data['name']}\"\n",
    "    else:\n",
    "        # Create a unique identifier from the edge geometry\n",
    "        coords = list(edge_data.geometry.coords)\n",
    "        start = coords[0]\n",
    "        end = coords[-1]\n",
    "        return f\"edge_{start[0]:.4f}_{start[1]:.4f}_{end[0]:.4f}_{end[1]:.4f}\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "from folium.features import DivIcon\n",
    "from branca.colormap import LinearColormap\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_trajectory_with_time(df):\n",
    "    \"\"\"Process trajectories with proper edge ID and timestamp handling\"\"\"\n",
    "    try:\n",
    "        processed_data = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            try:\n",
    "                # Get edge IDs and convert to ints\n",
    "                edge_ids = ast.literal_eval(row['eid'])\n",
    "                edge_ids = [int(eid) for eid in edge_ids]\n",
    "                \n",
    "                # Get timestamps and convert to seconds\n",
    "                timestamps = [t * 15 for t in ast.literal_eval(row['tpath'])]  # Each unit is 15 seconds\n",
    "                \n",
    "                # Get matched geometry\n",
    "                mgeom = row['mgeom']\n",
    "                if not isinstance(mgeom, str) or not mgeom.startswith('LINESTRING'):\n",
    "                    continue\n",
    "                    \n",
    "                processed_data.append({\n",
    "                    'edge_ids': edge_ids,\n",
    "                    'timestamps': timestamps,\n",
    "                    'duration': timestamps[-1] - timestamps[0],\n",
    "                    'mgeom': mgeom\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing trajectory {idx}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        return processed_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in process_trajectory_with_time: {str(e)}\")\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RouteAnalyzer:\n",
    "    def __init__(self, matcher, df: pd.DataFrame):\n",
    "        \"\"\"Initialize RouteAnalyzer with matcher and trajectory data\"\"\"\n",
    "        self.matcher = matcher\n",
    "        self.graph = matcher.graph\n",
    "        self.edges_gdf = matcher.edges_gdf.copy()\n",
    "        self.edges_utm = self.edges_gdf.to_crs('EPSG:32629')  # UTM zone 29N for Porto\n",
    "        self.processed_df = self._process_trajectory_data(df)\n",
    "        self.segment_stats = self._initialize_segment_stats()\n",
    "        \n",
    "        # Create output directory\n",
    "        self.output_dir = 'map_matching_results'\n",
    "        self.analysis_dir = os.path.join(self.output_dir, 'route_analysis')\n",
    "        os.makedirs(self.analysis_dir, exist_ok=True)\n",
    "    \n",
    "    def _process_trajectory_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        processed_data = []\n",
    "        for _, row in df.iterrows():\n",
    "            try:\n",
    "                edge_ids = self._parse_edge_list(row['eid'])\n",
    "                timestamps = self._parse_timestamp_list(row['tpath'])\n",
    "                \n",
    "                if edge_ids and timestamps and len(edge_ids) > 0 and len(timestamps) > 0:\n",
    "                    processed_data.append({\n",
    "                        'edge_ids': edge_ids,\n",
    "                        'timestamps': timestamps,\n",
    "                        'duration': max(timestamps) - min(timestamps)\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        return pd.DataFrame(processed_data)\n",
    "\n",
    "    def _parse_edge_list(self, edge_str: str) -> List[int]:\n",
    "        try:\n",
    "            if isinstance(edge_str, str):\n",
    "                clean_str = edge_str.strip().replace(' ', '')\n",
    "                if clean_str.startswith('[') and clean_str.endswith(']'):\n",
    "                    edges = ast.literal_eval(clean_str)\n",
    "                    return [int(edge) for edge in edges if str(edge).strip()]\n",
    "            return []\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "    def _parse_timestamp_list(self, time_str: str) -> List[int]:\n",
    "        try:\n",
    "            if isinstance(time_str, str):\n",
    "                clean_str = time_str.strip().replace(' ', '')\n",
    "                if clean_str.startswith('[') and clean_str.endswith(']'):\n",
    "                    times = ast.literal_eval(clean_str)\n",
    "                    return [int(t) * 15 for t in times if str(t).strip()]\n",
    "            return []\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "    def _initialize_segment_stats(self) -> Dict:\n",
    "        stats = {}\n",
    "        for _, row in self.processed_df.iterrows():\n",
    "            try:\n",
    "                edge_ids = row['edge_ids']\n",
    "                timestamps = row['timestamps']\n",
    "                \n",
    "                if not edge_ids or len(timestamps) < 2:\n",
    "                    continue\n",
    "                \n",
    "                total_length = 0\n",
    "                valid_edges = []\n",
    "                \n",
    "                for eid in edge_ids:\n",
    "                    if eid in self.edges_utm.index:\n",
    "                        length = self.edges_utm.loc[eid].geometry.length\n",
    "                        if length > 0:\n",
    "                            total_length += length\n",
    "                            valid_edges.append(eid)\n",
    "                \n",
    "                if total_length > 0 and valid_edges:\n",
    "                    duration = timestamps[-1] - timestamps[0]\n",
    "                    \n",
    "                    for edge_id in valid_edges:\n",
    "                        if edge_id not in stats:\n",
    "                            stats[edge_id] = {\n",
    "                                'count': 0,\n",
    "                                'total_time': 0,\n",
    "                                'length': self.edges_utm.loc[edge_id].geometry.length\n",
    "                            }\n",
    "                        \n",
    "                        segment_time = duration * (self.edges_utm.loc[edge_id].geometry.length / total_length)\n",
    "                        stats[edge_id]['count'] += 1\n",
    "                        stats[edge_id]['total_time'] += segment_time\n",
    "                        \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        for edge_id in stats:\n",
    "            if stats[edge_id]['count'] > 0:\n",
    "                stats[edge_id]['avg_time'] = stats[edge_id]['total_time'] / stats[edge_id]['count']\n",
    "                if stats[edge_id]['length'] > 0:\n",
    "                    stats[edge_id]['time_per_100m'] = (stats[edge_id]['avg_time'] / stats[edge_id]['length']) * 100\n",
    "                    stats[edge_id]['speed_ms'] = stats[edge_id]['length'] / stats[edge_id]['avg_time']\n",
    "        \n",
    "        return stats\n",
    "\n",
    "    def get_most_traversed_segments(self, n: int = 10) -> List[Dict]:\n",
    "        segments = []\n",
    "        for edge_id, stats in self.segment_stats.items():\n",
    "            if stats['count'] > 0 and stats['length'] > 0:\n",
    "                segments.append({\n",
    "                    'edge_id': edge_id,\n",
    "                    'count': stats['count'],\n",
    "                    'length': stats['length'],\n",
    "                    'avg_time': stats['avg_time'],\n",
    "                    'speed_ms': stats.get('speed_ms', 0),\n",
    "                    'time_per_100m': stats.get('time_per_100m', 0),\n",
    "                    'geometry': self.edges_utm.loc[edge_id].geometry\n",
    "                })\n",
    "        \n",
    "        segments.sort(key=lambda x: x['count'], reverse=True)\n",
    "        return segments[:n]\n",
    "\n",
    "    def get_slowest_segments(self, n: int = 10) -> List[Dict]:\n",
    "        segments = []\n",
    "        min_length = 50\n",
    "        min_speed = 0.1\n",
    "        max_speed = 33.3\n",
    "        \n",
    "        for edge_id, stats in self.segment_stats.items():\n",
    "            if (stats['count'] > 0 and \n",
    "                stats['length'] >= min_length and \n",
    "                'speed_ms' in stats and\n",
    "                min_speed <= stats['speed_ms'] <= max_speed):\n",
    "                \n",
    "                segments.append({\n",
    "                    'edge_id': edge_id,\n",
    "                    'count': stats['count'],\n",
    "                    'length': stats['length'],\n",
    "                    'avg_time': stats['avg_time'],\n",
    "                    'speed_ms': stats['speed_ms'],\n",
    "                    'time_per_100m': stats['time_per_100m'],\n",
    "                    'geometry': self.edges_utm.loc[edge_id].geometry\n",
    "                })\n",
    "        \n",
    "        segments.sort(key=lambda x: x['time_per_100m'], reverse=True)\n",
    "        return segments[:n]\n",
    "\n",
    "    def visualize_segments_enhanced(self, segments: List[Dict], title: str, \n",
    "                              filename: str, metric_name: str, \n",
    "                              color_scheme: List[str]) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Enhanced visualization method with fixed color mapping\n",
    "        \"\"\"\n",
    "        if not segments:\n",
    "            print(f\"No segments to visualize for {title}\")\n",
    "            return None\n",
    "        \n",
    "        # Convert edges to WGS84 for visualization\n",
    "        edges_wgs84 = self.edges_gdf.to_crs('EPSG:4326')\n",
    "        \n",
    "        # Create base map\n",
    "        center_lat, center_lon = 41.1579, -8.6291\n",
    "        m = folium.Map(\n",
    "            location=[center_lat, center_lon],\n",
    "            zoom_start=13,\n",
    "            tiles='cartodbpositron'\n",
    "        )\n",
    "        \n",
    "        # Add background network\n",
    "        for _, edge in edges_wgs84.iterrows():\n",
    "            if edge.geometry is not None:\n",
    "                coords = [(y, x) for x, y in edge.geometry.coords]\n",
    "                folium.PolyLine(\n",
    "                    coords,\n",
    "                    weight=1,\n",
    "                    color='lightgray',\n",
    "                    opacity=0.1\n",
    "                ).add_to(m)\n",
    "        \n",
    "        # Calculate colormap values based on metric\n",
    "        if 'time' in metric_name.lower():\n",
    "            metric_values = [seg['time_per_100m'] for seg in segments]\n",
    "            caption = 'Time per 100m (seconds)'\n",
    "        else:\n",
    "            metric_values = [seg['count'] for seg in segments]\n",
    "            caption = 'Traverse Count'\n",
    "        \n",
    "        # Create colormap\n",
    "        colormap = LinearColormap(\n",
    "            colors=color_scheme,\n",
    "            vmin=min(metric_values),\n",
    "            vmax=max(metric_values),\n",
    "            caption=caption\n",
    "        )\n",
    "        \n",
    "        # Add the legend\n",
    "        legend_html = \"\"\"\n",
    "        <div style=\"\n",
    "            position: fixed;\n",
    "            bottom: 50px;\n",
    "            right: 50px;\n",
    "            width: 300px;\n",
    "            z-index: 1000;\n",
    "            background-color: white;\n",
    "            padding: 15px;\n",
    "            border-radius: 6px;\n",
    "            box-shadow: 0 2px 5px rgba(0,0,0,0.2);\n",
    "            font-family: Arial;\n",
    "            font-size: 12px;\n",
    "            max-height: 400px;\n",
    "            overflow-y: auto;\n",
    "        \">\n",
    "            <h4 style=\"margin: 0 0 10px 0;\"><strong>Road Segments</strong></h4>\n",
    "        \"\"\"\n",
    "        \n",
    "        for rank, segment in enumerate(segments, 1):\n",
    "            try:\n",
    "                geom = segment['geometry']\n",
    "                if geom is not None:\n",
    "                    geom_wgs84 = gpd.GeoSeries([geom], crs='EPSG:32629').to_crs('EPSG:4326')[0]\n",
    "                    coords = [(y, x) for x, y in geom_wgs84.coords]\n",
    "                    \n",
    "                    # Get the metric value and color for this segment\n",
    "                    value = (segment['time_per_100m'] if 'time' in metric_name.lower() \n",
    "                            else segment['count'])\n",
    "                    color = colormap(value)\n",
    "                    \n",
    "                    # Calculate time in minutes\n",
    "                    time_minutes = segment['avg_time'] / 60\n",
    "                    \n",
    "                    # Add segment to legend\n",
    "                    legend_html += f\"\"\"\n",
    "                    <div style=\"margin-bottom: 12px;\">\n",
    "                        <div style=\"margin-bottom: 4px;\">\n",
    "                            <span style=\"\n",
    "                                display: inline-block;\n",
    "                                width: 12px;\n",
    "                                height: 12px;\n",
    "                                background-color: {color};\n",
    "                                margin-right: 5px;\n",
    "                                border: 1px solid #666;\n",
    "                            \"></span>\n",
    "                            <strong>#{rank}</strong>\n",
    "                        </div>\n",
    "                        <div style=\"margin-left: 17px;\">\n",
    "                            <strong>OSMID:</strong> {segment['edge_id']}<br>\n",
    "                            <strong>Speed:</strong> {segment['speed_ms']:.1f} m/s<br>\n",
    "                            <strong>Length:</strong> {segment['length']:.0f}m<br>\n",
    "                            <strong>Time:</strong> {time_minutes:.1f} min<br>\n",
    "                            <strong>Time/100m:</strong> {segment['time_per_100m']:.1f} sec\n",
    "                        </div>\n",
    "                    </div>\n",
    "                    \"\"\"\n",
    "                    \n",
    "                    # Add segment to map\n",
    "                    popup_text = (\n",
    "                        f\"<div style='font-family: Arial; font-size: 12px;'>\"\n",
    "                        f\"<strong>Rank: {rank}</strong><br>\"\n",
    "                        f\"Edge ID: {segment['edge_id']}<br>\"\n",
    "                        f\"Count: {segment['count']}<br>\"\n",
    "                        f\"Length: {segment['length']:.1f}m<br>\"\n",
    "                        f\"Speed: {segment['speed_ms']:.1f} m/s<br>\"\n",
    "                        f\"Time/100m: {segment['time_per_100m']:.1f}s\"\n",
    "                        f\"</div>\"\n",
    "                    )\n",
    "                    \n",
    "                    folium.PolyLine(\n",
    "                        coords,\n",
    "                        weight=6,\n",
    "                        color=color,\n",
    "                        opacity=0.9,\n",
    "                        popup=popup_text\n",
    "                    ).add_to(m)\n",
    "                    \n",
    "                    # Add rank label\n",
    "                    if len(coords) > 1:\n",
    "                        mid_point = coords[len(coords)//2]\n",
    "                        folium.DivIcon(\n",
    "                            html=f\"\"\"\n",
    "                            <div style=\"\n",
    "                                background-color: rgba(255, 255, 255, 0);\n",
    "                                border: 2px solid {color};\n",
    "                                border-radius: 4px;\n",
    "                                padding: 3px 6px;\n",
    "                                font-size: 10px;\n",
    "                                font-weight: bold;\n",
    "                            \">\n",
    "                                #{rank}\n",
    "                            </div>\n",
    "                            \"\"\"\n",
    "                        ).add_to(folium.Marker(\n",
    "                            mid_point,\n",
    "                            icon=DivIcon(\n",
    "                                icon_size=(30, 20),\n",
    "                                icon_anchor=(15, 10)\n",
    "                            )\n",
    "                        ).add_to(m))\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Error visualizing segment: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Close the legend div\n",
    "        legend_html += \"</div>\"\n",
    "        m.get_root().html.add_child(folium.Element(legend_html))\n",
    "        \n",
    "        # Add title\n",
    "        title_html = f\"\"\"\n",
    "        <div style=\"\n",
    "            position: fixed; \n",
    "            top: 20px; \n",
    "            left: 60px; \n",
    "            width: 320px;\n",
    "            z-index: 1000;\n",
    "            padding: 15px;\n",
    "            background-color: white;\n",
    "            border-radius: 6px;\n",
    "            box-shadow: 0 2px 5px rgba(0,0,0,0.2);\n",
    "        \">\n",
    "            <h3 style=\"margin: 0;\">{title}</h3>\n",
    "            <p style=\"margin: 8px 0 0 0;\">\n",
    "                Showing top {len(segments)} segments\n",
    "            </p>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        m.get_root().html.add_child(folium.Element(title_html))\n",
    "        \n",
    "        # Add colormap to map\n",
    "        colormap.add_to(m)\n",
    "        \n",
    "        # Save and return\n",
    "        output_path = os.path.join(self.analysis_dir, filename)\n",
    "        m.save(output_path)\n",
    "        return output_path\n",
    "\n",
    "# Now define the debug analysis function\n",
    "def debug_analysis():\n",
    "    try:\n",
    "        print(\"Starting debug analysis...\")\n",
    "        \n",
    "        file_path = \"Perry_Github/Urban_Assignment2/data/map_matching_1500.csv\"\n",
    "        print(\"\\nLoading trajectory data...\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Loaded {len(df)} trajectories\")\n",
    "        \n",
    "        print(\"\\nLoading road network...\")\n",
    "        G = ox.graph_from_place('Porto, Portugal', network_type='drive')\n",
    "        nodes, edges = ox.graph_to_gdfs(G)\n",
    "        print(f\"Loaded network with {len(edges)} edges\")\n",
    "        \n",
    "        print(\"\\nInitializing matcher and analyzer...\")\n",
    "        matcher = EnhancedViterbiMatcher(G, edges)\n",
    "        analyzer = RouteAnalyzer(matcher, df)\n",
    "        \n",
    "        print(\"\\nAnalysis Results:\")\n",
    "        print(f\"Total segments analyzed: {len(analyzer.segment_stats)}\")\n",
    "        \n",
    "        most_traversed = analyzer.get_most_traversed_segments()\n",
    "        slowest_segments = analyzer.get_slowest_segments()\n",
    "        \n",
    "        print(f\"Most traversed segments: {len(most_traversed)}\")\n",
    "        print(f\"Slowest segments: {len(slowest_segments)}\")\n",
    "        \n",
    "        most_traversed_map = analyzer.visualize_segments_enhanced(\n",
    "            most_traversed,\n",
    "            \"Most Frequently Traversed Road Segments\",\n",
    "            \"most_traversed_segments.html\",\n",
    "            \"traverse frequency\",\n",
    "            ['#fff7ec', '#fee8c8', '#fdd49e', '#fdbb84', '#fc8d59', '#ef6548', '#d7301f', '#990000']\n",
    "        )\n",
    "        \n",
    "        slowest_segments_map = analyzer.visualize_segments_enhanced(\n",
    "            slowest_segments,\n",
    "            \"Slowest Road Segments\",\n",
    "            \"slowest_segments.html\",\n",
    "            \"average travel time\",\n",
    "            ['#f7fcfd', '#e0ecf4', '#bfd3e6', '#9ebcda', '#8c96c6', '#8c6bb1', '#88419d', '#6e016b']\n",
    "        )\n",
    "        \n",
    "        results = {\n",
    "            'most_traversed': {\n",
    "                'segments': most_traversed,\n",
    "                'map_path': most_traversed_map\n",
    "            },\n",
    "            'slowest_segments': {\n",
    "                'segments': slowest_segments,\n",
    "                'map_path': slowest_segments_map\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if most_traversed:\n",
    "            print(\"\\nTop 3 most traversed segments:\")\n",
    "            for i, seg in enumerate(most_traversed[:3], 1):\n",
    "                print(f\"{i}. Count: {seg['count']}, \"\n",
    "                      f\"Length: {seg['length']:.2f}m, \"\n",
    "                      f\"Speed: {seg['speed_ms']:.2f} m/s\")\n",
    "        \n",
    "        if slowest_segments:\n",
    "            print(\"\\nTop 3 slowest segments:\")\n",
    "            for i, seg in enumerate(slowest_segments[:3], 1):\n",
    "                print(f\"{i}. Time per 100m: {seg['time_per_100m']:.2f}s, \"\n",
    "                      f\"Speed: {seg['speed_ms']:.2f} m/s, \"\n",
    "                      f\"Length: {seg['length']:.2f}m\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in debug analysis: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_map_matching_pipeline(input_file: str, place: str, nrows: int = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Run the complete map matching pipeline with the given parameters\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set up logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Load road network\n",
    "        logger.info(f\"Loading road network for {place}...\")\n",
    "        G = ox.graph_from_place(place, network_type='drive')\n",
    "        nodes, edges = ox.graph_to_gdfs(G)\n",
    "        \n",
    "        # Add edge IDs if they don't exist\n",
    "        if 'OSMID' not in edges.columns and 'osmid' not in edges.columns:\n",
    "            logger.info(\"Creating unique identifiers for edges...\")\n",
    "            edges['OSMID'] = edges.apply(get_edge_identifier, axis=1)\n",
    "        \n",
    "        # Convert to UTM coordinates\n",
    "        utm_crs = 'EPSG:32629'  # UTM zone 29N for Porto\n",
    "        edges = edges.to_crs(utm_crs)\n",
    "        \n",
    "        # Load trajectory data\n",
    "        logger.info(f\"Loading trajectory data from {input_file}...\")\n",
    "        df = pd.read_csv(input_file, nrows=nrows)\n",
    "        \n",
    "        # Initialize matcher with default configuration\n",
    "        matcher = EnhancedViterbiMatcher(G, edges)\n",
    "        \n",
    "        # Process trajectories\n",
    "        matched_results = []\n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "            trajectory_data = process_trajectory_with_time(row)\n",
    "            if trajectory_data:\n",
    "                point_gdf = gpd.GeoDataFrame(\n",
    "                    geometry=[Point(x, y) for x, y in trajectory_data['coords']],\n",
    "                    crs='EPSG:4326'\n",
    "                ).to_crs(utm_crs)\n",
    "                \n",
    "                utm_coords = [(p.x, p.y) for p in point_gdf.geometry]\n",
    "                result = matcher.match_trajectory(utm_coords)\n",
    "                \n",
    "                if result['success']:\n",
    "                    matched_results.append({\n",
    "                        'match_result': result,\n",
    "                        'original_coords': trajectory_data['coords'],\n",
    "                        'timestamps': trajectory_data['timestamps'],\n",
    "                        'start_timestamp': trajectory_data['start_timestamp']\n",
    "                    })\n",
    "        \n",
    "        if not matched_results:\n",
    "            logger.warning(\"No trajectories were successfully matched\")\n",
    "            return None\n",
    "        \n",
    "        # Perform analysis\n",
    "        output_dir = 'map_matching_results'\n",
    "        route_analyzer = RouteAnalyzer(matcher, matched_results, output_dir)\n",
    "        analysis_results = route_analyzer.analyze_and_visualize()\n",
    "        \n",
    "        return {\n",
    "            'matched_results': matched_results,\n",
    "            'analysis_results': analysis_results,\n",
    "            'output_dir': output_dir\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in map matching pipeline: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_map_matching_file(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Process the map matching file with improved data parsing\"\"\"\n",
    "    try:\n",
    "        print(f\"Reading file: {file_path}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Initial shape: {df.shape}\")\n",
    "        \n",
    "        def extract_duration(row):\n",
    "            \"\"\"Calculate total duration from matched segments\"\"\"\n",
    "            try:\n",
    "                # Get timestamps from tpath\n",
    "                tpath = ast.literal_eval(row['tpath'])\n",
    "                if not tpath:\n",
    "                    return 0\n",
    "                # Each increment in tpath represents 15 seconds\n",
    "                return max(tpath) * 15  # Convert to seconds\n",
    "            except:\n",
    "                return 0\n",
    "        \n",
    "        def extract_edge_ids(row):\n",
    "            \"\"\"Extract valid edge IDs\"\"\"\n",
    "            try:\n",
    "                return [int(eid) for eid in ast.literal_eval(row['eid'])]\n",
    "            except:\n",
    "                return []\n",
    "        \n",
    "        print(\"Processing trajectories...\")\n",
    "        processed_data = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            try:\n",
    "                duration = extract_duration(row)\n",
    "                edge_ids = extract_edge_ids(row)\n",
    "                \n",
    "                if duration > 0 and edge_ids:\n",
    "                    # Extract matched geometry\n",
    "                    mgeom = row['mgeom']\n",
    "                    if isinstance(mgeom, str) and mgeom.startswith('LINESTRING'):\n",
    "                        processed_data.append({\n",
    "                            'trajectory_id': idx,\n",
    "                            'duration': duration,\n",
    "                            'edge_ids': edge_ids,\n",
    "                            'mgeom': mgeom\n",
    "                        })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {idx}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Create processed DataFrame\n",
    "        processed_df = pd.DataFrame(processed_data)\n",
    "        \n",
    "        print(\"\\nProcessing results:\")\n",
    "        print(f\"Initial rows: {len(df)}\")\n",
    "        print(f\"Valid trajectories: {len(processed_df)}\")\n",
    "        \n",
    "        if len(processed_df) == 0:\n",
    "            print(\"\\nDetailed error analysis:\")\n",
    "            sample_row = df.iloc[0]\n",
    "            print(\"\\nSample row contents:\")\n",
    "            print(f\"tpath: {sample_row['tpath']}\")\n",
    "            print(f\"eid: {sample_row['eid']}\")\n",
    "            print(f\"mgeom: {sample_row['mgeom']}\")\n",
    "        \n",
    "        return processed_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def parse_edge_list(edge_str: str) -> List[int]:\n",
    "    \"\"\"\n",
    "    Safely parse edge ID list from string representation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if isinstance(edge_str, str):\n",
    "            # Remove any whitespace and handle potential formatting issues\n",
    "            clean_str = edge_str.strip().replace(' ', '')\n",
    "            if clean_str.startswith('[') and clean_str.endswith(']'):\n",
    "                # Parse string as list\n",
    "                edges = ast.literal_eval(clean_str)\n",
    "                # Ensure all elements are integers\n",
    "                return [int(edge) for edge in edges if str(edge).strip()]\n",
    "        return []\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def parse_timestamp_list(time_str: str) -> List[int]:\n",
    "    \"\"\"\n",
    "    Safely parse timestamp list from string representation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if isinstance(time_str, str):\n",
    "            clean_str = time_str.strip().replace(' ', '')\n",
    "            if clean_str.startswith('[') and clean_str.endswith(']'):\n",
    "                times = ast.literal_eval(clean_str)\n",
    "                # Convert to seconds (each unit represents 15 seconds)\n",
    "                return [int(t) * 15 for t in times if str(t).strip()]\n",
    "        return []\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def process_trajectory_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process trajectory data with proper validation\n",
    "    \"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            # Parse edge IDs and timestamps\n",
    "            edge_ids = parse_edge_list(row['eid'])\n",
    "            timestamps = parse_timestamp_list(row['tpath'])\n",
    "            \n",
    "            # Validate data\n",
    "            if edge_ids and timestamps and len(edge_ids) > 0 and len(timestamps) > 0:\n",
    "                processed_data.append({\n",
    "                    'edge_ids': edge_ids,\n",
    "                    'timestamps': timestamps,\n",
    "                    'duration': max(timestamps) - min(timestamps),\n",
    "                    'mgeom': row['mgeom'] if 'mgeom' in row else None\n",
    "                })\n",
    "        except Exception as e:\n",
    "            logging.debug(f\"Error processing row: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(processed_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def debug_analysis():\n",
    "    \"\"\"Run debug analysis with proper initialization\"\"\"\n",
    "    try:\n",
    "        print(\"Starting debug analysis...\")\n",
    "        \n",
    "        # Load trajectory data\n",
    "        file_path = \"Perry_Github/Urban_Assignment2/data/map_matching_1500.csv\"\n",
    "        print(\"\\nLoading trajectory data...\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Loaded {len(df)} trajectories\")\n",
    "        \n",
    "        # Load road network\n",
    "        print(\"\\nLoading road network...\")\n",
    "        G = ox.graph_from_place('Porto, Portugal', network_type='drive')\n",
    "        nodes, edges = ox.graph_to_gdfs(G)\n",
    "        print(f\"Loaded network with {len(edges)} edges\")\n",
    "        \n",
    "        # Initialize matcher\n",
    "        print(\"\\nInitializing matcher and analyzer...\")\n",
    "        matcher = EnhancedViterbiMatcher(G, edges)\n",
    "        \n",
    "        # Initialize analyzer with processed data\n",
    "        analyzer = RouteAnalyzer(matcher, df)\n",
    "        \n",
    "        # Get analysis results\n",
    "        most_traversed = analyzer.get_most_traversed_segments()\n",
    "        slowest_segments = analyzer.get_slowest_segments()\n",
    "        \n",
    "        print(\"\\nAnalysis Results:\")\n",
    "        print(f\"Total segments analyzed: {len(analyzer.segment_stats)}\")\n",
    "        print(f\"Most traversed segments: {len(most_traversed)}\")\n",
    "        print(f\"Slowest segments: {len(slowest_segments)}\")\n",
    "        \n",
    "        results = {\n",
    "            'most_traversed': {'segments': most_traversed},\n",
    "            'slowest_segments': {'segments': slowest_segments}\n",
    "        }\n",
    "        \n",
    "        # Print detailed results\n",
    "        if most_traversed:\n",
    "            print(\"\\nTop 3 most traversed segments:\")\n",
    "            for i, seg in enumerate(most_traversed[:3], 1):\n",
    "                print(f\"{i}. Count: {seg['count']}, \"\n",
    "                      f\"Length: {seg['length']:.2f}m, \"\n",
    "                      f\"Speed: {seg['speed_ms']:.2f} m/s\")\n",
    "        \n",
    "        if slowest_segments:\n",
    "            print(\"\\nTop 3 slowest segments:\")\n",
    "            for i, seg in enumerate(slowest_segments[:3], 1):\n",
    "                print(f\"{i}. Time per 100m: {seg['time_per_100m']:.2f}s, \"\n",
    "                      f\"Speed: {seg['speed_ms']:.2f} m/s, \"\n",
    "                      f\"Length: {seg['length']:.2f}m\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in debug analysis: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Function to run the complete analysis\n",
    "def run_analysis(file_path: str, place: str):\n",
    "    \"\"\"Run the complete analysis with proper initialization\"\"\"\n",
    "    try:\n",
    "        print(\"Loading road network...\")\n",
    "        G = ox.graph_from_place(place, network_type='drive')\n",
    "        nodes, edges = ox.graph_to_gdfs(G)\n",
    "        print(f\"Loaded network with {len(edges)} edges\")\n",
    "        \n",
    "        print(\"\\nProcessing map matching data...\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        print(\"\\nInitializing matcher and analyzer...\")\n",
    "        matcher = EnhancedViterbiMatcher(G, edges)\n",
    "        analyzer = RouteAnalyzer(matcher, df)\n",
    "        \n",
    "        most_traversed = analyzer.get_most_traversed_segments()\n",
    "        slowest_segments = analyzer.get_slowest_segments()\n",
    "        \n",
    "        return {\n",
    "            'most_traversed': {'segments': most_traversed},\n",
    "            'slowest_segments': {'segments': slowest_segments}\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in analysis pipeline: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def analyze_trajectories(file_path: str, place: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Main analysis function with improved error handling\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set up logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Load road network\n",
    "        logger.info(\"Loading road network...\")\n",
    "        G = ox.graph_from_place(place, network_type='drive')\n",
    "        nodes, edges = ox.graph_to_gdfs(G)\n",
    "        \n",
    "        # Load and process trajectory data\n",
    "        logger.info(\"Processing trajectory data...\")\n",
    "        raw_df = pd.read_csv(file_path)\n",
    "        processed_df = process_trajectory_data(raw_df)\n",
    "        \n",
    "        if processed_df.empty:\n",
    "            logger.error(\"No valid trajectories found in the data\")\n",
    "            return None\n",
    "        \n",
    "        # Initialize analyzer and run analysis\n",
    "        logger.info(\"Running analysis...\")\n",
    "        analyzer = RouteAnalyzer(G, edges, processed_df)\n",
    "        \n",
    "        most_traversed = analyzer.get_most_traversed_segments()\n",
    "        slowest_segments = analyzer.get_slowest_segments()\n",
    "        \n",
    "        return {\n",
    "            'most_traversed': most_traversed,\n",
    "            'slowest_segments': slowest_segments,\n",
    "            'total_trajectories': len(processed_df),\n",
    "            'total_segments': len(analyzer.segment_stats)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in analysis: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def verify_file_structure():\n",
    "    \"\"\"\n",
    "    Verify file structure and data format\n",
    "    \"\"\"\n",
    "    file_path = \"Perry_Github/Urban_Assignment2/data/map_matching_1500.csv\"\n",
    "    \n",
    "    try:\n",
    "        if not os.path.exists(file_path):\n",
    "            alternative_path = \"data/map_matching_1500.csv\"\n",
    "            if os.path.exists(alternative_path):\n",
    "                file_path = alternative_path\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"File not found in either path: {file_path} or {alternative_path}\")\n",
    "        \n",
    "        print(f\"Reading file from: {file_path}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(\"\\nFile Information:\")\n",
    "        print(f\"Number of rows: {len(df)}\")\n",
    "        print(f\"Columns: {df.columns.tolist()}\")\n",
    "        print(\"\\nFirst few rows:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        return file_path, df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error verifying file: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# \n",
    "def debug_analysis():\n",
    "    \"\"\"Run debug analysis with visualization\"\"\"\n",
    "    try:\n",
    "        print(\"Starting debug analysis...\")\n",
    "        \n",
    "        # Load trajectory data\n",
    "        file_path = \"Perry_Github/Urban_Assignment2/data/map_matching_1500.csv\"\n",
    "        print(\"\\nLoading trajectory data...\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Loaded {len(df)} trajectories\")\n",
    "        \n",
    "        # Load road network\n",
    "        print(\"\\nLoading road network...\")\n",
    "        G = ox.graph_from_place('Porto, Portugal', network_type='drive')\n",
    "        nodes, edges = ox.graph_to_gdfs(G)\n",
    "        print(f\"Loaded network with {len(edges)} edges\")\n",
    "        \n",
    "        # Initialize matcher and analyzer\n",
    "        print(\"\\nInitializing matcher and analyzer...\")\n",
    "        matcher = EnhancedViterbiMatcher(G, edges)\n",
    "        analyzer = RouteAnalyzer(matcher, df)\n",
    "        \n",
    "        print(\"\\nAnalysis Results:\")\n",
    "        print(f\"Total segments analyzed: {len(analyzer.segment_stats)}\")\n",
    "        \n",
    "        # Get analysis results\n",
    "        most_traversed = analyzer.get_most_traversed_segments()\n",
    "        slowest_segments = analyzer.get_slowest_segments()\n",
    "        \n",
    "        print(f\"Most traversed segments: {len(most_traversed)}\")\n",
    "        print(f\"Slowest segments: {len(slowest_segments)}\")\n",
    "        \n",
    "        # Create visualizations\n",
    "        most_traversed_map = analyzer.visualize_segments_enhanced(\n",
    "            most_traversed,\n",
    "            \"Most Frequently Traversed Road Segments\",\n",
    "            \"most_traversed_segments.html\",\n",
    "            \"traverse frequency\",\n",
    "            ['#fff7ec', '#fee8c8', '#fdd49e', '#fdbb84', '#fc8d59', '#ef6548', '#d7301f', '#990000']\n",
    "        )\n",
    "        \n",
    "        slowest_segments_map = analyzer.visualize_segments_enhanced(\n",
    "            slowest_segments,\n",
    "            \"Slowest Road Segments\",\n",
    "            \"slowest_segments.html\",\n",
    "            \"average travel time\",\n",
    "            ['#f7fcfd', '#e0ecf4', '#bfd3e6', '#9ebcda', '#8c96c6', '#8c6bb1', '#88419d', '#6e016b']\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        results = {\n",
    "            'most_traversed': {\n",
    "                'segments': most_traversed,\n",
    "                'map_path': most_traversed_map\n",
    "            },\n",
    "            'slowest_segments': {\n",
    "                'segments': slowest_segments,\n",
    "                'map_path': slowest_segments_map\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Print detailed statistics\n",
    "        if most_traversed:\n",
    "            print(\"\\nTop 3 most traversed segments:\")\n",
    "            for i, seg in enumerate(most_traversed[:3], 1):\n",
    "                print(f\"{i}. Count: {seg['count']}, \"\n",
    "                      f\"Length: {seg['length']:.2f}m, \"\n",
    "                      f\"Speed: {seg['speed_ms']:.2f} m/s\")\n",
    "        \n",
    "        if slowest_segments:\n",
    "            print(\"\\nTop 3 slowest segments:\")\n",
    "            for i, seg in enumerate(slowest_segments[:3], 1):\n",
    "                print(f\"{i}. Time per 100m: {seg['time_per_100m']:.2f}s, \"\n",
    "                      f\"Speed: {seg['speed_ms']:.2f} m/s, \"\n",
    "                      f\"Length: {seg['length']:.2f}m\")\n",
    "        \n",
    "        print(\"\\nVisualization files created:\")\n",
    "        if most_traversed_map:\n",
    "            print(f\"Most traversed segments map: {most_traversed_map}\")\n",
    "        if slowest_segments_map:\n",
    "            print(f\"Slowest segments map: {slowest_segments_map}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in debug analysis: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "def analyze_and_visualize_map_matching(file_path: str, place: str):\n",
    "    \"\"\"Main analysis function with improved error handling\"\"\"\n",
    "    try:\n",
    "        # Load road network\n",
    "        print(\"Loading road network...\")\n",
    "        G = ox.graph_from_place(place, network_type='drive')\n",
    "        nodes, edges = ox.graph_to_gdfs(G)\n",
    "        print(f\"Loaded network with {len(edges)} edges\")\n",
    "        \n",
    "        # Process trajectory data\n",
    "        print(\"\\nProcessing map matching data...\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Convert edge IDs and paths from string to list\n",
    "        df['edge_ids'] = df['eid'].apply(ast.literal_eval)\n",
    "        df['timestamps'] = df['tpath'].apply(lambda x: [t * 15 for t in ast.literal_eval(x)])\n",
    "        \n",
    "        print(f\"Initial shape: {df.shape}\")\n",
    "        \n",
    "        # Initialize matcher and analyzer\n",
    "        print(\"\\nInitializing matcher and analyzer...\")\n",
    "        matcher = EnhancedViterbiMatcher(G, edges)\n",
    "        analyzer = RouteAnalyzer(matcher, df)\n",
    "        \n",
    "        print(\"\\nRunning analysis...\")\n",
    "        most_traversed = analyzer.get_most_traversed_segments()\n",
    "        slowest_segments = analyzer.get_slowest_segments()\n",
    "        \n",
    "        # Generate visualizations\n",
    "        most_traversed_map = analyzer.visualize_segments_enhanced(\n",
    "            most_traversed,\n",
    "            \"Most Frequently Traversed Road Segments\",\n",
    "            \"most_traversed_segments.html\",\n",
    "            \"traverse frequency\",\n",
    "            ['#fff7ec', '#fee8c8', '#fdd49e', '#fdbb84', '#fc8d59', '#ef6548', '#d7301f', '#990000']\n",
    "        )\n",
    "        \n",
    "        slowest_segments_map = analyzer.visualize_segments_enhanced(\n",
    "            slowest_segments,\n",
    "            \"Slowest Road Segments\",\n",
    "            \"slowest_segments.html\",\n",
    "            \"average travel time\",\n",
    "            ['#f7fcfd', '#e0ecf4', '#bfd3e6', '#9ebcda', '#8c96c6', '#8c6bb1', '#88419d', '#6e016b']\n",
    "        )\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\nAnalysis Results:\")\n",
    "        print(f\"Most traversed segments: {len(most_traversed)}\")\n",
    "        print(\"Top 3 most traversed:\")\n",
    "        for i, seg in enumerate(most_traversed[:3], 1):\n",
    "            print(f\"{i}. Count: {seg['count']}, Length: {seg['length']:.2f}m\")\n",
    "        \n",
    "        print(f\"\\nSlowest segments: {len(slowest_segments)}\")\n",
    "        print(\"Top 3 slowest:\")\n",
    "        for i, seg in enumerate(slowest_segments[:3], 1):\n",
    "            print(f\"{i}. Time per 100m: {seg['time_per_100m']:.2f}s, Speed: {seg['speed_ms']:.2f}m/s\")\n",
    "        \n",
    "        return {\n",
    "            'most_traversed': {'segments': most_traversed, 'map_path': most_traversed_map},\n",
    "            'slowest_segments': {'segments': slowest_segments, 'map_path': slowest_segments_map}\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in analysis: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "# Add this function to test the data processing\n",
    "def test_data_processing():\n",
    "    \"\"\"\n",
    "    Test the data processing to verify correct handling\n",
    "    \"\"\"\n",
    "    file_path = \"Perry_Github/Urban_Assignment2/data/map_matching_1500.csv\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    print(\"\\nTesting data processing...\")\n",
    "    print(\"\\nSample raw duration:\")\n",
    "    print(df['duration'].iloc[0])\n",
    "    \n",
    "    print(\"\\nSample raw tpath:\")\n",
    "    print(df['tpath'].iloc[0])\n",
    "    \n",
    "    print(\"\\nSample raw eid:\")\n",
    "    print(df['eid'].iloc[0])\n",
    "    \n",
    "    processed_df = process_map_matching_file(file_path)\n",
    "    \n",
    "    if not processed_df.empty:\n",
    "        print(\"\\nProcessed data sample:\")\n",
    "        sample = processed_df.iloc[0]\n",
    "        print(f\"Duration: {sample['duration']}\")\n",
    "        print(f\"Path: {sample['tpath'][:5]}...\")\n",
    "        print(f\"Edge IDs: {sample['eid'][:5]}...\")\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "\n",
    "def parse_mgeom(value: str) -> Optional[LineString]:\n",
    "    \"\"\"\n",
    "    Parse LINESTRING format into a LineString object with improved error handling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not isinstance(value, str):\n",
    "            return None\n",
    "            \n",
    "        # Handle empty LINESTRING\n",
    "        if value == 'LINESTRING()':\n",
    "            return None\n",
    "            \n",
    "        # Handle single point LINESTRING\n",
    "        if 'LINESTRING(' in value and ')' in value:\n",
    "            # Extract coordinates\n",
    "            coords_str = value[value.find('(')+1:value.find(')')]\n",
    "            if not coords_str:\n",
    "                return None\n",
    "                \n",
    "            coords = []\n",
    "            for coord in coords_str.split(','):\n",
    "                if coord.strip():\n",
    "                    try:\n",
    "                        x, y = map(float, coord.strip().split())\n",
    "                        coords.append((x, y))\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "            \n",
    "            # Need at least 2 points for a valid LineString\n",
    "            if len(coords) >= 2:\n",
    "                return LineString(coords)\n",
    "            else:\n",
    "                logging.debug(f\"Not enough points in LINESTRING: {value}\")\n",
    "                return None\n",
    "    except Exception as e:\n",
    "        logging.debug(f\"Error parsing geometry: {str(e)} for value: {value}\")\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_segment_statistics(segment_stats: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Validate and debug segment statistics\n",
    "    Returns a dictionary with validation results\n",
    "    \"\"\"\n",
    "    validation_results = {\n",
    "        'traverse_counts': [],\n",
    "        'times_per_100m': [],\n",
    "        'raw_data': []\n",
    "    }\n",
    "    \n",
    "    for edge_id, stats in segment_stats.items():\n",
    "        if stats['traverse_count'] > 0:\n",
    "            # Validate traverse count\n",
    "            validation_results['traverse_counts'].append({\n",
    "                'edge_id': edge_id,\n",
    "                'count': stats['traverse_count'],\n",
    "                'length': stats['length']\n",
    "            })\n",
    "            \n",
    "            # Validate timing calculations\n",
    "            if 'avg_time' in stats and stats['length'] > 0:\n",
    "                time_per_100m = (stats['avg_time'] / stats['length']) * 100\n",
    "                validation_results['times_per_100m'].append({\n",
    "                    'edge_id': edge_id,\n",
    "                    'time_per_100m': time_per_100m,\n",
    "                    'avg_time': stats['avg_time'],\n",
    "                    'length': stats['length']\n",
    "                })\n",
    "                \n",
    "            # Store raw data for debugging\n",
    "            validation_results['raw_data'].append({\n",
    "                'edge_id': edge_id,\n",
    "                'stats': stats\n",
    "            })\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "def analyze_segment_metrics(results: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze and verify segment metrics\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'traverse_counts': {\n",
    "            'max': 0,\n",
    "            'min': float('inf'),\n",
    "            'total_segments': 0\n",
    "        },\n",
    "        'timing': {\n",
    "            'max_time_per_100m': 0,\n",
    "            'min_time_per_100m': float('inf'),\n",
    "            'total_segments': 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Analyze traverse counts\n",
    "    if 'most_traversed' in results and 'segments' in results['most_traversed']:\n",
    "        counts = [seg['count'] for seg in results['most_traversed']['segments']]\n",
    "        metrics['traverse_counts'].update({\n",
    "            'max': max(counts) if counts else 0,\n",
    "            'min': min(counts) if counts else 0,\n",
    "            'total_segments': len(counts),\n",
    "            'all_counts': counts\n",
    "        })\n",
    "    \n",
    "    # Analyze timing\n",
    "    if 'slowest_segments' in results and 'segments' in results['slowest_segments']:\n",
    "        times = [seg['time_per_100m'] for seg in results['slowest_segments']['segments']]\n",
    "        metrics['timing'].update({\n",
    "            'max_time_per_100m': max(times) if times else 0,\n",
    "            'min_time_per_100m': min(times) if times else 0,\n",
    "            'total_segments': len(times),\n",
    "            'all_times': times\n",
    "        })\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def debug_trajectory_timing(matched_results: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Debug timing calculations from raw trajectory data\n",
    "    \"\"\"\n",
    "    timing_debug = {\n",
    "        'trajectory_times': [],\n",
    "        'segment_times': [],\n",
    "        'anomalies': []\n",
    "    }\n",
    "    \n",
    "    for result in matched_results:\n",
    "        if not result['match_result']['success']:\n",
    "            continue\n",
    "            \n",
    "        timestamps = result['timestamps']\n",
    "        if not timestamps or len(timestamps) < 2:\n",
    "            continue\n",
    "            \n",
    "        total_time = timestamps[-1] - timestamps[0]\n",
    "        timing_debug['trajectory_times'].append(total_time)\n",
    "        \n",
    "        # Calculate segment times\n",
    "        edges = result['match_result']['edges']\n",
    "        if edges:\n",
    "            avg_time_per_segment = total_time / len(edges)\n",
    "            timing_debug['segment_times'].append(avg_time_per_segment)\n",
    "            \n",
    "            # Flag potential anomalies\n",
    "            if avg_time_per_segment < 1.0 or avg_time_per_segment > 300.0:  # Less than 1 second or more than 5 minutes\n",
    "                timing_debug['anomalies'].append({\n",
    "                    'trajectory_id': len(timing_debug['trajectory_times']) - 1,\n",
    "                    'avg_time': avg_time_per_segment,\n",
    "                    'total_time': total_time,\n",
    "                    'num_segments': len(edges)\n",
    "                })\n",
    "    \n",
    "    return timing_debug\n",
    "\n",
    "\n",
    "def run_analysis_with_debugging():\n",
    "    \"\"\"\n",
    "    Run analysis with enhanced debugging and data validation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set up logging\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        \n",
    "        logging.info(\"Starting analysis...\")\n",
    "        \n",
    "        # Verify file path\n",
    "        file_path = \"Perry_Github/Urban_Assignment2/data/map_matching_1500.csv\"\n",
    "        if not os.path.exists(file_path):\n",
    "            logging.error(f\"File not found: {file_path}\")\n",
    "            return None\n",
    "            \n",
    "        logging.info(f\"File found: {file_path}\")\n",
    "        \n",
    "        # Run analysis\n",
    "        results = analyze_and_visualize_map_matching(file_path, 'Porto, Portugal')\n",
    "        \n",
    "        if results:\n",
    "            logging.info(\"\\nAnalysis completed successfully\")\n",
    "            \n",
    "            # Validate and display results\n",
    "            for segment_type, data in results.items():\n",
    "                segments = data['segments']\n",
    "                if segments:\n",
    "                    logging.info(f\"\\n{segment_type.replace('_', ' ').title()} Segments:\")\n",
    "                    logging.info(\"Top 3 segments:\")\n",
    "                    for i, seg in enumerate(segments[:3], 1):\n",
    "                        logging.info(f\"\\nSegment {i}:\")\n",
    "                        logging.info(f\"- Length: {seg['length']:.2f} meters\")\n",
    "                        logging.info(f\"- Average time: {seg['avg_time']:.2f} seconds\")\n",
    "                        logging.info(f\"- Time per 100m: {seg['time_per_100m']:.2f} seconds\")\n",
    "                        logging.info(f\"- Speed: {seg['speed_ms']:.2f} m/s\")\n",
    "                        logging.info(f\"- Traverse count: {seg['count']}\")\n",
    "            \n",
    "            return results\n",
    "        else:\n",
    "            logging.error(\"Analysis failed to produce results\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error running analysis: {str(e)}\")\n",
    "        logging.error(\"Stack trace:\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "\n",
    "# Function to run the complete analysis\n",
    "def run_analysis(file_path: str, place: str):\n",
    "    \"\"\"Run the complete analysis pipeline with detailed logging\"\"\"\n",
    "    try:\n",
    "        print(\"Loading road network...\")\n",
    "        G = ox.graph_from_place(place, network_type='drive')\n",
    "        nodes, edges = ox.graph_to_gdfs(G)\n",
    "        print(f\"Loaded network with {len(edges)} edges\")\n",
    "        \n",
    "        print(\"\\nProcessing map matching data...\")\n",
    "        processed_df = process_map_matching_file(file_path)\n",
    "        \n",
    "        if processed_df.empty:\n",
    "            print(\"No valid trajectories found in the data\")\n",
    "            return None\n",
    "            \n",
    "        print(\"\\nInitializing matcher and analyzer...\")\n",
    "        matcher = EnhancedViterbiMatcher(G, edges)\n",
    "        analyzer = RouteAnalyzer(matcher, processed_df)\n",
    "        \n",
    "        print(\"\\nRunning analysis...\")\n",
    "        results = analyzer.analyze_and_visualize()\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in analysis pipeline: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "def run_analysis_with_validation():\n",
    "    \"\"\"\n",
    "    Run analysis with enhanced validation and debugging\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = analyze_and_visualize_map_matching(\n",
    "            file_path=\"Perry_Github/Urban_Assignment2/data/map_matching_1500.csv\",\n",
    "            place='Porto, Portugal'\n",
    "        )\n",
    "\n",
    "        if results:\n",
    "            print(\"\\nRunning validation checks...\")\n",
    "            \n",
    "            # Detailed validation of timing calculations\n",
    "            for segment_type, data in results.items():\n",
    "                segments = data['segments']\n",
    "                if segments:\n",
    "                    print(f\"\\n{segment_type.replace('_', ' ').title()} Segments:\")\n",
    "                    print(\"Sample of segment statistics:\")\n",
    "                    for i, seg in enumerate(segments[:3], 1):\n",
    "                        print(f\"\\nSegment {i}:\")\n",
    "                        print(f\"- Length: {seg['length']:.2f} meters\")\n",
    "                        print(f\"- Average time: {seg['avg_time']:.2f} seconds\")\n",
    "                        print(f\"- Time per 100m: {seg['time_per_100m']:.2f} seconds\")\n",
    "                        print(f\"- Speed: {seg['speed_ms']:.2f} m/s\")\n",
    "                        print(f\"- Traverse count: {seg['count']}\")\n",
    "            \n",
    "            # Validate overall metrics\n",
    "            most_traversed = results['most_traversed']['segments']\n",
    "            slowest = results['slowest_segments']['segments']\n",
    "            \n",
    "            print(\"\\nOverall Statistics:\")\n",
    "            if most_traversed:\n",
    "                print(f\"Maximum traverse count: {max(seg['count'] for seg in most_traversed)}\")\n",
    "            if slowest:\n",
    "                max_time = max(seg['time_per_100m'] for seg in slowest)\n",
    "                print(f\"Maximum time per 100m: {max_time:.2f} seconds\")\n",
    "                print(f\"Corresponding to speed: {(100 / max_time):.2f} m/s\")\n",
    "            \n",
    "            return results\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error running analysis with validation: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def get_slowest_segments(self, n: int = 10) -> List[Dict]:\n",
    "    \"\"\"Return the n slowest road segments with fixed time calculation\"\"\"\n",
    "    segments = []\n",
    "    min_length = 50  # Only consider segments longer than 50m\n",
    "    \n",
    "    for edge_id, stats in self.segment_stats.items():\n",
    "        if (stats['traverse_count'] > 0 and \n",
    "            stats.get('avg_time', 0) > 0 and \n",
    "            stats['length'] >= min_length):\n",
    "            \n",
    "            # Validate length and time\n",
    "            length = float(stats['length'])\n",
    "            avg_time = float(stats['avg_time'])\n",
    "            \n",
    "            if length > 0 and avg_time > 0:\n",
    "                # Calculate time per 100m with validation\n",
    "                time_per_100m = (avg_time / length) * 100\n",
    "                \n",
    "                # Add reasonable bounds\n",
    "                min_speed = 0.1  # m/s (very slow walking)\n",
    "                max_speed = 33.3  # m/s (120 km/h)\n",
    "                \n",
    "                # Convert time_per_100m to speed for validation\n",
    "                speed = 100 / time_per_100m if time_per_100m > 0 else float('inf')\n",
    "                \n",
    "                # Only add if speed is within reasonable bounds\n",
    "                if min_speed <= speed <= max_speed:\n",
    "                    segments.append({\n",
    "                        'edge_id': edge_id,\n",
    "                        'OSMID': str(edge_id),\n",
    "                        'avg_time': avg_time,\n",
    "                        'length': length,\n",
    "                        'geometry': self.edges_gdf.loc[edge_id].geometry,\n",
    "                        'count': stats['traverse_count'],\n",
    "                        'speed_ms': speed,\n",
    "                        'time_per_100m': time_per_100m\n",
    "                    })\n",
    "    \n",
    "    # Sort by time per 100m (higher times = slower segments)\n",
    "    segments.sort(key=lambda x: x['time_per_100m'], reverse=True)\n",
    "    return segments[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting debug analysis...\n",
      "\n",
      "Loading trajectory data...\n",
      "Loaded 1497 trajectories\n",
      "\n",
      "Loading road network...\n",
      "Loaded network with 10533 edges\n",
      "\n",
      "Initializing matcher and analyzer...\n",
      "\n",
      "Analysis Results:\n",
      "Total segments analyzed: 2651\n",
      "Most traversed segments: 10\n",
      "Slowest segments: 10\n",
      "\n",
      "Top 3 most traversed segments:\n",
      "1. Count: 732, Length: 64.00m, Speed: 3.39 m/s\n",
      "2. Count: 401, Length: 29.52m, Speed: 1.94 m/s\n",
      "3. Count: 376, Length: 13.23m, Speed: 2.52 m/s\n",
      "\n",
      "Top 3 slowest segments:\n",
      "1. Time per 100m: 778.00s, Speed: 0.13 m/s, Length: 162.25m\n",
      "2. Time per 100m: 679.73s, Speed: 0.15 m/s, Length: 79.54m\n",
      "3. Time per 100m: 462.18s, Speed: 0.22 m/s, Length: 84.88m\n",
      "\n",
      "Visualization files created:\n",
      "Most traversed segments map: map_matching_results/route_analysis/most_traversed_segments.html\n",
      "Slowest segments map: map_matching_results/route_analysis/slowest_segments.html\n",
      "\n",
      "Final Results Summary:\n",
      "\n",
      "most_traversed:\n",
      "Number of segments: 10\n",
      "Top segment statistics:\n",
      "- Count: 732\n",
      "- Time per 100m: 29.54s\n",
      "- Speed: 3.39m/s\n",
      "\n",
      "slowest_segments:\n",
      "Number of segments: 10\n",
      "Top segment statistics:\n",
      "- Count: 1\n",
      "- Time per 100m: 778.00s\n",
      "- Speed: 0.13m/s\n",
      "Loading road network...\n",
      "Loaded network with 10533 edges\n",
      "\n",
      "Processing map matching data...\n",
      "Initial shape: (1497, 21)\n",
      "\n",
      "Initializing matcher and analyzer...\n",
      "\n",
      "Running analysis...\n",
      "\n",
      "Analysis Results:\n",
      "Most traversed segments: 10\n",
      "Top 3 most traversed:\n",
      "1. Count: 732, Length: 64.00m\n",
      "2. Count: 401, Length: 29.52m\n",
      "3. Count: 376, Length: 13.23m\n",
      "\n",
      "Slowest segments: 10\n",
      "Top 3 slowest:\n",
      "1. Time per 100m: 778.00s, Speed: 0.13m/s\n",
      "2. Time per 100m: 679.73s, Speed: 0.15m/s\n",
      "3. Time per 100m: 462.18s, Speed: 0.22m/s\n"
     ]
    }
   ],
   "source": [
    "# Run the analysis with debugging\n",
    "if __name__ == \"__main__\":\n",
    "   # Run debug analysis\n",
    "    results = debug_analysis()\n",
    "    \n",
    "    if results:\n",
    "        print(\"\\nFinal Results Summary:\")\n",
    "        for key, data in results.items():\n",
    "            print(f\"\\n{key}:\")\n",
    "            segments = data['segments']\n",
    "            print(f\"Number of segments: {len(segments)}\")\n",
    "            if segments:\n",
    "                print(\"Top segment statistics:\")\n",
    "                segment = segments[0]\n",
    "                print(f\"- Count: {segment['count']}\")\n",
    "                print(f\"- Time per 100m: {segment['time_per_100m']:.2f}s\")\n",
    "                print(f\"- Speed: {segment['speed_ms']:.2f}m/s\")\n",
    "        \n",
    "        analyze_and_visualize_map_matching(\"map_matching_1500.csv\",\"Porto, Portugal\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urbancom2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
