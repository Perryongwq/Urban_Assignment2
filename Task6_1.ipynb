{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import osmnx as ox\n",
    "import folium\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "# Suppress warnings from osmnx\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_graph(bounds):\n",
    "    \"\"\"\n",
    "    Create the street network within the bounding box.\n",
    "\n",
    "    Args:\n",
    "        bounds (tuple): A tuple containing the bounding box coordinates (x1, x2, y1, y2).\n",
    "\n",
    "    Returns:\n",
    "        networkx.MultiDiGraph: The street network graph.\n",
    "    \"\"\"\n",
    "    x1, x2, y1, y2 = bounds\n",
    "    boundary_polygon = Polygon([(x1, y1), (x2, y1), (x2, y2), (x1, y2)])\n",
    "    return ox.graph_from_polygon(boundary_polygon, network_type='drive')\n",
    "\n",
    "\n",
    "def load_data(file_path, nrows=None):\n",
    "    \"\"\"\n",
    "    Load the trajectory data from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file.\n",
    "        nrows (int, optional): Number of rows to read. Defaults to None (all rows).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded DataFrame.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_path, nrows=nrows)\n",
    "\n",
    "\n",
    "def clean_trajectory_data(df):\n",
    "    \"\"\"\n",
    "    Convert trajectory data from string to NumPy arrays.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing trajectory data.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array with cleaned trajectory data.\n",
    "    \"\"\"\n",
    "    train_data = df.to_numpy()\n",
    "\n",
    "    for i in range(len(train_data)):\n",
    "        # Extract the trajectory string from column 8 (0-based index)\n",
    "        traj_str = train_data[i, 8]\n",
    "        # Remove leading/trailing characters and split by comma\n",
    "        data = traj_str[2:-2].replace(']', '').replace('[', '').split(',')\n",
    "\n",
    "        if len(data) > 1:\n",
    "            try:\n",
    "                data = np.asarray(data, dtype=float).reshape((len(data) // 2, 2))\n",
    "            except ValueError:\n",
    "                # Handle cases where data cannot be reshaped properly\n",
    "                data = np.asarray([[0.0, 0.0]])\n",
    "        else:\n",
    "            data = np.asarray([[0.0, 0.0]])\n",
    "\n",
    "        train_data[i, 8] = data\n",
    "\n",
    "    return train_data\n",
    "\n",
    "\n",
    "def remove_outliers(train_data, threshold_multiplier=5):\n",
    "    \"\"\"\n",
    "    Remove outlying GPS coordinates based on a distance threshold.\n",
    "\n",
    "    Args:\n",
    "        train_data (np.ndarray): Array containing trajectory data.\n",
    "        threshold_multiplier (float, optional): Multiplier for the average distance to determine outliers. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array with outliers removed.\n",
    "    \"\"\"\n",
    "    for i in range(len(train_data)):\n",
    "        GPS_trajectory = train_data[i, 8]\n",
    "        num_points = len(GPS_trajectory)\n",
    "\n",
    "        if num_points > 1:\n",
    "            route_dist = 0.0\n",
    "            # Calculate the total route distance\n",
    "            for j in range(num_points - 1):\n",
    "                lon_1, lat_1 = GPS_trajectory[j]\n",
    "                lon_2, lat_2 = GPS_trajectory[j + 1]\n",
    "                route_dist += np.linalg.norm([lon_1 - lon_2, lat_1 - lat_2])\n",
    "\n",
    "            ave_dist = route_dist / (num_points - 1)\n",
    "            j = 0\n",
    "\n",
    "            # Compare points and remove outliers\n",
    "            while j < (len(GPS_trajectory) - 1):\n",
    "                lon_1, lat_1 = GPS_trajectory[j]\n",
    "                lon_2, lat_2 = GPS_trajectory[j + 1]\n",
    "                dist = np.linalg.norm([lon_1 - lon_2, lat_1 - lat_2])\n",
    "\n",
    "                if dist > threshold_multiplier * ave_dist:\n",
    "                    GPS_trajectory = np.delete(GPS_trajectory, j + 1, 0)\n",
    "                    if j > 0:\n",
    "                        j -= 1  # Move back one step to re-evaluate after deletion\n",
    "                else:\n",
    "                    j += 1  # Move to the next point\n",
    "\n",
    "            train_data[i, 8] = GPS_trajectory\n",
    "\n",
    "    return train_data\n",
    "\n",
    "\n",
    "def save_cleaned_data(train_data, original_df, output_path):\n",
    "    \"\"\"\n",
    "    Save the cleaned train_data back to a CSV file in the original format.\n",
    "\n",
    "    Args:\n",
    "        train_data (np.ndarray): The cleaned trajectory data.\n",
    "        original_df (pd.DataFrame): The original DataFrame loaded from the CSV.\n",
    "        output_path (str): The path where the cleaned CSV will be saved.\n",
    "    \"\"\"\n",
    "    # Create a copy of the original DataFrame to maintain all columns\n",
    "    cleaned_df = original_df.copy()\n",
    "\n",
    "    # Convert the NumPy arrays back to string format for column 8\n",
    "    cleaned_df.iloc[:, 8] = [str(traj.tolist()) for traj in train_data[:, 8]]\n",
    "\n",
    "    # Save the cleaned DataFrame to a new CSV file\n",
    "    cleaned_df.to_csv(output_path, index=False)\n",
    "    print(f\"Cleaned data saved to {output_path}\")\n",
    "\n",
    "\n",
    "def plot_trajectories_folium(G, train_data, traj_indices, output_path, title='Trajectories'):\n",
    "    \"\"\"\n",
    "    Plot the trajectories using Folium.\n",
    "\n",
    "    Args:\n",
    "        G (networkx.MultiDiGraph): The street network graph.\n",
    "        train_data (np.ndarray): Array containing trajectory data.\n",
    "        traj_indices (list): List of indices of trajectories to plot.\n",
    "        output_path (str): Path to save the Folium HTML map.\n",
    "        title (str, optional): Title of the plot. Defaults to 'Trajectories'.\n",
    "    \"\"\"\n",
    "    # Calculate map boundaries\n",
    "    all_lons = []\n",
    "    all_lats = []\n",
    "    for traj_idx in traj_indices:\n",
    "        traj = train_data[traj_idx, 8]\n",
    "        if isinstance(traj, np.ndarray) and len(traj) > 0:\n",
    "            all_lons.extend(traj[:, 0])\n",
    "            all_lats.extend(traj[:, 1])\n",
    "\n",
    "    if not all_lons or not all_lats:\n",
    "        raise ValueError(\"No valid trajectories to plot.\")\n",
    "\n",
    "    x_min, x_max = min(all_lons), max(all_lons)\n",
    "    y_min, y_max = min(all_lats), max(all_lats)\n",
    "    map_center = [(y_min + y_max) / 2, (x_min + x_max) / 2]\n",
    "\n",
    "    # Initialize Folium map\n",
    "    folium_map = folium.Map(location=map_center, zoom_start=14, control_scale=True)\n",
    "    folium_map.fit_bounds([[y_min, x_min], [y_max, x_max]])\n",
    "\n",
    "    # Generate a color map\n",
    "    num_traj = len(traj_indices)\n",
    "    color_map = plt.get_cmap('tab10')(np.linspace(0, 1, num_traj))\n",
    "\n",
    "    # Plot each trajectory\n",
    "    for i, traj_idx in enumerate(traj_indices):\n",
    "        traj = train_data[traj_idx, 8]\n",
    "        if isinstance(traj, np.ndarray) and len(traj) > 1:\n",
    "            traj_coords = traj[:, [1, 0]].tolist()\n",
    "            folium.PolyLine(\n",
    "                traj_coords,\n",
    "                color=mcolors.rgb2hex(color_map[i % len(color_map)]),\n",
    "                weight=5,\n",
    "                opacity=0.8,\n",
    "                tooltip=f'Trip {traj_idx + 1}'\n",
    "            ).add_to(folium_map)\n",
    "\n",
    "    folium_map.save(output_path)\n",
    "    print(f\"Interactive Folium map saved to {output_path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the data cleaning and plotting process.\n",
    "    \"\"\"\n",
    "    bounds = (-8.70, -8.57, 41.13, 41.19)\n",
    "    folder = '/content/data'\n",
    "    train_file = os.path.join(folder, \"train-1500.csv\")\n",
    "    cleaned_train_file = os.path.join(folder, \"train-1500-cleaned.csv\")\n",
    "    output_dir = '/content/data/'\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    output_file = os.path.join(output_dir, 'Trajectories_Before_Outlier_Removal_Folium.html')\n",
    "    outlier_file = os.path.join(output_dir, 'Trajectories_After_Outlier_Removal_Folium.html')\n",
    "\n",
    "    G = load_graph(bounds)\n",
    "    df = load_data(train_file, nrows=1500)\n",
    "\n",
    "    train_data = clean_trajectory_data(df)\n",
    "    traj_indices = list(range(15))\n",
    "\n",
    "    plot_trajectories_folium(G, train_data, traj_indices, output_file, 'Trajectories Before Outlier Removal')\n",
    "    train_data = remove_outliers(train_data)\n",
    "    plot_trajectories_folium(G, train_data, traj_indices, outlier_file, 'Trajectories After Outlier Removal')\n",
    "\n",
    "    save_cleaned_data(train_data, df, cleaned_train_file)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all the requirements with:\n",
    "! sudo apt-get install libboost-dev libboost-serialization-dev \\\n",
    "gdal-bin libgdal-dev make cmake libbz2-dev libexpat1-dev swig\n",
    "\n",
    "#! sudo apt-get install libboost-dev libboost-serialization-dev \\\n",
    "#gdal-bin libgdal-dev make cmake libbz2-dev libexpat1-dev swig python-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/cyang-kth/fmm.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!! This could take ~ 6 mintues !!!!\n",
    "# it may not work and requires for password, etc.\n",
    "# in this case, go to your folder using console to compile and install FMM\n",
    "\n",
    "import os\n",
    "# change working directory\n",
    "os.chdir(\"fmm\")\n",
    "\n",
    "if not os.path.exists('build'):\n",
    "  os.mkdir('build')\n",
    "# ! mkdir build\n",
    "os.chdir(\"build\")\n",
    "# ! cd build\n",
    "! cmake ..\n",
    "! make -j4\n",
    "! sudo make install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import osmnx as ox\n",
    "import time\n",
    "from shapely.geometry import Polygon\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def save_graph_shapefile_directional(G, filepath=None, encoding=\"utf-8\"):\n",
    "    # default filepath if none was provided\n",
    "    if filepath is None:\n",
    "        filepath = os.path.join(ox.settings.data_folder, \"graph_shapefile\")\n",
    "\n",
    "    # if save folder does not already exist, create it (shapefiles\n",
    "    # get saved as set of files)\n",
    "    if not filepath == \"\" and not os.path.exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "    filepath_nodes = os.path.join(filepath, \"nodes.shp\")\n",
    "    filepath_edges = os.path.join(filepath, \"edges.shp\")\n",
    "\n",
    "    # convert undirected graph to gdfs and stringify non-numeric columns\n",
    "    gdf_nodes, gdf_edges = ox.utils_graph.graph_to_gdfs(G)\n",
    "    gdf_nodes = ox.io._stringify_nonnumeric_cols(gdf_nodes)\n",
    "    gdf_edges = ox.io._stringify_nonnumeric_cols(gdf_edges)\n",
    "\n",
    "    # We need an unique ID for each edge\n",
    "    gdf_edges[\"fid\"] = np.arange(0, gdf_edges.shape[0], dtype='int')\n",
    "\n",
    "    # save the nodes and edges as separate ESRI shapefiles\n",
    "    gdf_nodes.to_file(filepath_nodes, encoding=encoding)\n",
    "    gdf_edges.to_file(filepath_edges, encoding=encoding)\n",
    "\n",
    "print(\"osmnx version\",ox.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place =\"Porto, Portugal\"\n",
    "\n",
    "start_time = time.time()\n",
    "G = ox.graph_from_place(place, network_type='drive', which_result=2)\n",
    "\n",
    "save_graph_shapefile_directional(G, filepath='/content/data/porto')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "ox.plot_graph(G)\n",
    "ox.save_graphml(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "from fmm import (\n",
    "    Network,\n",
    "    NetworkGraph,\n",
    "    UBODTGenAlgorithm,\n",
    "    UBODT,\n",
    "    FastMapMatch,\n",
    "    FastMapMatchConfig,\n",
    "    STMATCH,\n",
    "    STMATCHConfig\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Set up paths to save to Google Drive\n",
    "google_drive_path = '/content/drive/My Drive'  # Adjust if your Google Drive path is different\n",
    "\n",
    "data_dir = '/content/data'\n",
    "\n",
    "save_dir = os.path.join(google_drive_path, 'data')\n",
    "\n",
    "# Ensure the data directory exists\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Paths to your data and output files in Google Drive\n",
    "network_file_path = os.path.join(data_dir, \"porto\", \"edges.shp\")\n",
    "ubodt_file_path = os.path.join(data_dir, \"ubodt.txt\")\n",
    "train_csv_path = os.path.join(data_dir, \"train-1500-cleaned.csv\")\n",
    "output_csv_path = os.path.join(save_dir, \"matched-results-1500_cleaned.csv\")\n",
    "\n",
    "# Read network data\n",
    "network = Network(network_file_path, \"fid\", \"u\", \"v\")\n",
    "print(\"Nodes {} edges {}\".format(network.get_node_count(), network.get_edge_count()))\n",
    "graph = NetworkGraph(network)\n",
    "\n",
    "# Precompute UBODT table\n",
    "\n",
    "print(\"Generate the ubodt file\")\n",
    "ubodt_gen = UBODTGenAlgorithm(network, graph)\n",
    "status = ubodt_gen.generate_ubodt(ubodt_file_path, 0.03, binary=False, use_omp=True)\n",
    "print(f\"UBODT Generation Status: {status}\")\n",
    "ubodt = UBODT.read_ubodt_csv(ubodt_file_path)\n",
    "\n",
    "# Create FMM model\n",
    "fmm_model = FastMapMatch(network, graph, ubodt)\n",
    "\n",
    "# Define map matching configurations for FMM\n",
    "k = 16\n",
    "radius = 0.005\n",
    "gps_error = 0.0005\n",
    "fmm_config = FastMapMatchConfig(k, radius, gps_error)\n",
    "\n",
    "# Create STMATCH model\n",
    "stmatch_model = STMATCH(network, graph)\n",
    "\n",
    "# Define STMATCH map matching configurations\n",
    "k_stmatch = 8\n",
    "radius_stmatch = 0.01\n",
    "gps_error_stmatch = 0.001\n",
    "vmax = 0.003\n",
    "factor = 1.5\n",
    "stmatch_config = STMATCHConfig(k_stmatch, radius_stmatch, gps_error_stmatch, vmax, factor)\n",
    "\n",
    "# Read trajectory data from CSV\n",
    "train1500 = []\n",
    "with open(train_csv_path, \"r\", newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        polyline_str = row.get(\"POLYLINE\")\n",
    "        if polyline_str:\n",
    "            try:\n",
    "                row[\"POLYLINE\"] = json.loads(polyline_str)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Invalid POLYLINE format in row {row}\")\n",
    "                row[\"POLYLINE\"] = []\n",
    "        else:\n",
    "            row[\"POLYLINE\"] = []\n",
    "        train1500.append(row)\n",
    "\n",
    "\n",
    "for i, trajectory in enumerate(train1500):\n",
    "    polyline = trajectory[\"POLYLINE\"]\n",
    "    if not polyline:\n",
    "        print(f\"Empty POLYLINE for trajectory index {i}\")\n",
    "        continue\n",
    "\n",
    "    # Convert polyline to WKT format\n",
    "    wkt = \"LINESTRING(\" + \",\".join([\"{} {}\".format(point[0], point[1]) for point in polyline]) + \")\"\n",
    "\n",
    "    # Try FMM\n",
    "    result = fmm_model.match_wkt(wkt, fmm_config)\n",
    "    train1500[i][\"MATCHING_ALGORITHM\"] = \"fmm\"\n",
    "\n",
    "    if not list(result.cpath):\n",
    "        # Try STMATCH if FMM fails\n",
    "        result = stmatch_model.match_wkt(wkt, stmatch_config)\n",
    "        train1500[i][\"MATCHING_ALGORITHM\"] = \"stmatch\"\n",
    "        if not list(result.cpath):\n",
    "            print(f\"No match found for trajectory index {i}, WKT: {wkt}\")\n",
    "            continue  # Skip to next trajectory if no match found\n",
    "\n",
    "    # Adding matched results according to your output field specifications\n",
    "    train1500[i][\"MATCHED_RESULTS\"] = {\n",
    "        \"id\": i,\n",
    "        \"ogeom\": wkt,\n",
    "        \"opath\": list(result.opath),\n",
    "        \"error\": [c.error for c in result.candidates],\n",
    "        \"offset\": [c.offset for c in result.candidates],\n",
    "        \"length\": [c.length for c in result.candidates],\n",
    "        \"spdist\": [c.spdist for c in result.candidates],\n",
    "        \"duration\": [0] * len(result.candidates),  # Placeholder, replace with actual duration if available\n",
    "        \"speed\": [0] * len(result.candidates),  # Placeholder, replace with actual speed if available\n",
    "        \"pgeom\": result.pgeom.export_wkt(),\n",
    "        \"cpath\": list(result.cpath),\n",
    "        \"tpath\": list(result.indices),\n",
    "        \"mgeom\": result.mgeom.export_wkt(),\n",
    "        \"ep\": [c.ep for c in result.candidates],\n",
    "        \"tp\": [c.tp for c in result.candidates],\n",
    "        \"matching_algorithm\": train1500[i][\"MATCHING_ALGORITHM\"],\n",
    "        \"eid\": [c.edge_id for c in result.candidates],\n",
    "        \"source\": [c.source for c in result.candidates],\n",
    "        \"target\": [c.target for c in result.candidates]\n",
    "    }\n",
    "    print(f\"Processed trajectory index {i}\")\n",
    "\n",
    "# Save the results to a new CSV file\n",
    "output_dir = os.path.dirname(output_csv_path)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define CSV headers based on the modified field names\n",
    "headers = [\"id\", \"ogeom\", \"opath\", \"error\", \"offset\", \"length\", \"spdist\", \"duration\", \"speed\",\n",
    "           \"pgeom\", \"cpath\", \"tpath\", \"mgeom\", \"ep\", \"tp\", \"MATCHING_ALGORITHM\", \"eid\", \"source\", \"target\"]\n",
    "\n",
    "with open(output_csv_path, \"w\", newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=headers)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for trajectory in train1500:  \n",
    "        if \"MATCHED_RESULTS\" in trajectory:\n",
    "            matched_results = trajectory[\"MATCHED_RESULTS\"]\n",
    "            flattened_row = {\n",
    "                \"id\": matched_results[\"id\"],\n",
    "                \"ogeom\": matched_results[\"ogeom\"],\n",
    "                \"opath\": json.dumps(matched_results[\"opath\"]),\n",
    "                \"error\": json.dumps(matched_results[\"error\"]),\n",
    "                \"offset\": json.dumps(matched_results[\"offset\"]),\n",
    "                \"length\": json.dumps(matched_results[\"length\"]),\n",
    "                \"spdist\": json.dumps(matched_results[\"spdist\"]),\n",
    "                \"duration\": json.dumps(matched_results[\"duration\"]),\n",
    "                \"speed\": json.dumps(matched_results[\"speed\"]),\n",
    "                \"pgeom\": matched_results[\"pgeom\"],\n",
    "                \"cpath\": json.dumps(matched_results[\"cpath\"]),\n",
    "                \"tpath\": json.dumps(matched_results[\"tpath\"]),\n",
    "                \"mgeom\": matched_results[\"mgeom\"],\n",
    "                \"ep\": json.dumps(matched_results[\"ep\"]),\n",
    "                \"tp\": json.dumps(matched_results[\"tp\"]),\n",
    "                \"MATCHING_ALGORITHM\": matched_results[\"matching_algorithm\"],\n",
    "                \"eid\": json.dumps(matched_results[\"eid\"]),\n",
    "                \"source\": json.dumps(matched_results[\"source\"]),\n",
    "                \"target\": json.dumps(matched_results[\"target\"])\n",
    "            }\n",
    "            writer.writerow(flattened_row)\n",
    "\n",
    "print(f\"Results saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing geometry: IllegalArgumentException: point array must contain 0 or >1 elements\n",
      "\n",
      "Error parsing geometry: IllegalArgumentException: point array must contain 0 or >1 elements\n",
      "\n",
      "Individual map for Trip 1 saved to data/Task6\\fmap_route_1.html\n",
      "Individual map for Trip 2 saved to data/Task6\\fmap_route_2.html\n",
      "Individual map for Trip 3 saved to data/Task6\\fmap_route_3.html\n",
      "Individual map for Trip 4 saved to data/Task6\\fmap_route_4.html\n",
      "Individual map for Trip 5 saved to data/Task6\\fmap_route_5.html\n",
      "Individual map for Trip 6 saved to data/Task6\\fmap_route_6.html\n",
      "Individual map for Trip 7 saved to data/Task6\\fmap_route_7.html\n",
      "Individual map for Trip 8 saved to data/Task6\\fmap_route_8.html\n",
      "Individual map for Trip 9 saved to data/Task6\\fmap_route_9.html\n",
      "Individual map for Trip 10 saved to data/Task6\\fmap_route_10.html\n",
      "Individual map for Trip 11 saved to data/Task6\\fmap_route_11.html\n",
      "Individual map for Trip 12 saved to data/Task6\\fmap_route_12.html\n",
      "Individual map for Trip 13 saved to data/Task6\\fmap_route_13.html\n",
      "Individual map for Trip 14 saved to data/Task6\\fmap_route_14.html\n",
      "Individual map for Trip 15 saved to data/Task6\\fmap_route_15.html\n",
      "Combined map saved to data\\Mapped_task6.html\n",
      "Visualization completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import folium\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from shapely.geometry import LineString\n",
    "import os\n",
    "\n",
    "def parse_mgeom(value):\n",
    "    \"\"\"Parse LINESTRING format into a LineString object.\"\"\"\n",
    "    try:\n",
    "        if value.startswith(\"LINESTRING\"):\n",
    "            coords = value.replace(\"LINESTRING(\", \"\").replace(\")\", \"\").split(\",\")\n",
    "            coords = [(float(coord.split()[0]), float(coord.split()[1])) for coord in coords]\n",
    "            return LineString(coords)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing geometry: {e}\")\n",
    "        return None\n",
    "\n",
    "def calculate_boundaries(current_x_max, current_y_max, current_x_min, current_y_min, is_first, longitude, latitude):\n",
    "    if is_first:\n",
    "        current_x_min = current_x_max = longitude\n",
    "        current_y_min = current_y_max = latitude\n",
    "        is_first = False\n",
    "    else:\n",
    "        current_x_max = max(current_x_max, longitude)\n",
    "        current_x_min = min(current_x_min, longitude)\n",
    "        current_y_max = max(current_y_max, latitude)\n",
    "        current_y_min = min(current_y_min, latitude)\n",
    "    return current_x_max, current_y_max, current_x_min, current_y_min, is_first\n",
    "\n",
    "def visualize_routes(file_path, save_combined=True, save_individual=True, plot_lines=True, plot_points=True):\n",
    "    \"\"\"\n",
    "    Visualize the first 15 routes from the CSV file by creating a combined map and individual maps.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the CSV file containing route data.\n",
    "    - save_combined (bool): Whether to save the combined map.\n",
    "    - save_individual (bool): Whether to save individual maps for each route.\n",
    "    - plot_lines (bool): Whether to plot lines between points in the routes.\n",
    "    - plot_points (bool): Whether to plot individual points in the routes.\n",
    "    \"\"\"\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return\n",
    "\n",
    "    # Load and parse geometries\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['geometry'] = df['mgeom'].apply(parse_mgeom)\n",
    "    df = df.dropna(subset=['geometry'])\n",
    "    if df.empty:\n",
    "        print(\"No valid geometries found.\")\n",
    "        return\n",
    "\n",
    "    # Initialize boundaries and color mapping for routes\n",
    "    current_x_max = current_y_max = current_x_min = current_y_min = None\n",
    "    is_first = True\n",
    "    num_routes = min(15, len(df))\n",
    "    colors = plt.cm.jet(np.linspace(0, 1, num_routes))\n",
    "\n",
    "    # Calculate boundaries based on route data\n",
    "    for i, geometry in enumerate(df['geometry'].head(num_routes)):\n",
    "        x, y = geometry.xy\n",
    "        for lon, lat in zip(x, y):\n",
    "            current_x_max, current_y_max, current_x_min, current_y_min, is_first = calculate_boundaries(\n",
    "                current_x_max, current_y_max, current_x_min, current_y_min, is_first, lon, lat\n",
    "            )\n",
    "\n",
    "    # Adjust boundaries with padding for the map view\n",
    "    if is_first:\n",
    "        # Default boundaries if no points were processed\n",
    "        current_x_min, current_x_max = -8.7, -8.5  \n",
    "        current_y_min, current_y_max = 41.1, 41.3\n",
    "    else:\n",
    "        x_padding = (current_x_max - current_x_min) * 0.05\n",
    "        y_padding = (current_y_max - current_y_min) * 0.05\n",
    "        current_x_min -= x_padding\n",
    "        current_x_max += x_padding\n",
    "        current_y_min -= y_padding\n",
    "        current_y_max += y_padding\n",
    "\n",
    "    # Generate a color list in hex format for consistency with Folium\n",
    "    colors_hex = [\n",
    "        f'#{int(color[0]*255):02x}{int(color[1]*255):02x}{int(color[2]*255):02x}'\n",
    "        for color in colors\n",
    "    ]\n",
    "\n",
    "    # Directory to save individual route maps\n",
    "    individual_dir = 'data/Task6'\n",
    "    if not os.path.exists(individual_dir):\n",
    "        os.makedirs(individual_dir)\n",
    "\n",
    "    # Create a combined Folium map if required\n",
    "    if save_combined:\n",
    "        combined_map = folium.Map(\n",
    "            location=[(current_y_max + current_y_min) / 2, (current_x_max + current_x_min) / 2],\n",
    "            zoom_start=13,\n",
    "            # tiles=\"cartodbpositron\",\n",
    "            max_bounds=True\n",
    "        )\n",
    "        combined_map.fit_bounds([[current_y_min, current_x_min], [current_y_max, current_x_max]])\n",
    "\n",
    "    # Iterate through each route to add to the combined map and save individually\n",
    "    for idx, geometry in enumerate(df['geometry'].head(num_routes)):\n",
    "        lcoord = list(zip(*geometry.xy))  # Convert LineString to list of coordinates (lon, lat)\n",
    "        lcoord = [(lat, lon) for lon, lat in lcoord]  # Convert to (lat, lon) tuples for Folium\n",
    "        color = colors_hex[idx]\n",
    "\n",
    "        # Add to combined map\n",
    "        if save_combined:\n",
    "            feature_group = folium.FeatureGroup(name=f'Trip {idx + 1}')\n",
    "\n",
    "            # Add polyline for the route if plot_lines is enabled\n",
    "            if plot_lines:\n",
    "                folium.PolyLine(\n",
    "                    locations=lcoord,\n",
    "                    color=color,\n",
    "                    weight=3,\n",
    "                    opacity=0.7\n",
    "                ).add_to(feature_group)\n",
    "\n",
    "            # Add circle markers for each point if plot_points is enabled\n",
    "            if plot_points:\n",
    "                for point in lcoord:\n",
    "                    folium.CircleMarker(\n",
    "                        location=point,\n",
    "                        radius=3,\n",
    "                        color=color,\n",
    "                        fill=True,\n",
    "                        fill_color=color,\n",
    "                        fill_opacity=1\n",
    "                    ).add_to(feature_group)\n",
    "\n",
    "            # Highlight start and end points\n",
    "            folium.CircleMarker(\n",
    "                location=lcoord[0],\n",
    "                radius=5,\n",
    "                color='green',\n",
    "                fill=True,\n",
    "                fill_color='green',\n",
    "                fill_opacity=1,\n",
    "                tooltip=f'Start Trip {idx + 1}'\n",
    "            ).add_to(feature_group)\n",
    "\n",
    "            folium.CircleMarker(\n",
    "                location=lcoord[-1],\n",
    "                radius=5,\n",
    "                color='red',\n",
    "                fill=True,\n",
    "                fill_color='red',\n",
    "                fill_opacity=1,\n",
    "                tooltip=f'End Trip {idx + 1}'\n",
    "            ).add_to(feature_group)\n",
    "\n",
    "            feature_group.add_to(combined_map)\n",
    "\n",
    "        # Save individual map\n",
    "        if save_individual:\n",
    "            # Create a Folium map centered on the route's first point\n",
    "            individual_map = folium.Map(\n",
    "                location=[lcoord[0][0], lcoord[0][1]],\n",
    "                zoom_start=14,\n",
    "                # tiles=\"cartodbpositron\"\n",
    "            )\n",
    "\n",
    "            # Adjust map bounds to fit the route\n",
    "            latitudes = [point[0] for point in lcoord]\n",
    "            longitudes = [point[1] for point in lcoord]\n",
    "            bounds = [[min(latitudes), min(longitudes)], [max(latitudes), max(longitudes)]]\n",
    "            individual_map.fit_bounds(bounds)\n",
    "\n",
    "            # Create a feature group for the current route\n",
    "            individual_fg = folium.FeatureGroup(name=f'Trip {idx + 1}')\n",
    "\n",
    "            # Add polyline for the route if plot_lines is enabled\n",
    "            if plot_lines:\n",
    "                folium.PolyLine(\n",
    "                    locations=lcoord,\n",
    "                    color=color,\n",
    "                    weight=3,\n",
    "                    opacity=0.7\n",
    "                ).add_to(individual_fg)\n",
    "\n",
    "            # Add circle markers for each point if plot_points is enabled\n",
    "            if plot_points:\n",
    "                for point in lcoord:\n",
    "                    folium.CircleMarker(\n",
    "                        location=point,\n",
    "                        radius=3,\n",
    "                        color=color,\n",
    "                        fill=True,\n",
    "                        fill_color=color,\n",
    "                        fill_opacity=1\n",
    "                    ).add_to(individual_fg)\n",
    "\n",
    "            # Highlight start and end points\n",
    "            folium.CircleMarker(\n",
    "                location=lcoord[0],\n",
    "                radius=5,\n",
    "                color='green',\n",
    "                fill=True,\n",
    "                fill_color='green',\n",
    "                fill_opacity=1,\n",
    "                tooltip=f'Start Trip {idx + 1}'\n",
    "            ).add_to(individual_fg)\n",
    "\n",
    "            folium.CircleMarker(\n",
    "                location=lcoord[-1],\n",
    "                radius=5,\n",
    "                color='red',\n",
    "                fill=True,\n",
    "                fill_color='red',\n",
    "                fill_opacity=1,\n",
    "                tooltip=f'End Trip {idx + 1}'\n",
    "            ).add_to(individual_fg)\n",
    "\n",
    "            # Add feature group to the individual map\n",
    "            individual_fg.add_to(individual_map)\n",
    "\n",
    "            # Add layer control (optional for individual maps)\n",
    "            folium.LayerControl().add_to(individual_map)\n",
    "\n",
    "            # Define output path for individual map\n",
    "            individual_output_path = os.path.join(individual_dir, f'fmap_route_{idx + 1}.html')\n",
    "            individual_map.save(individual_output_path)\n",
    "            print(f\"Individual map for Trip {idx + 1} saved to {individual_output_path}\")\n",
    "\n",
    "    # Finalize and save the combined map\n",
    "    if save_combined:\n",
    "        # Add layer control to the combined map\n",
    "        folium.LayerControl().add_to(combined_map)\n",
    "\n",
    "        # Define output path for combined map\n",
    "        combined_output_dir = 'data'\n",
    "        if not os.path.exists(combined_output_dir):\n",
    "            os.makedirs(combined_output_dir)\n",
    "        combined_output_path = os.path.join(combined_output_dir, 'Mapped_task6.html')\n",
    "        combined_map.save(combined_output_path)\n",
    "        print(f\"Combined map saved to {combined_output_path}\")\n",
    "\n",
    "    print(\"Visualization completed successfully.\")\n",
    "\n",
    "# Run the function with the specified file path\n",
    "visualize_routes(\n",
    "    file_path=\"data/matched-results-1500_cleaned.csv\",\n",
    "    save_combined=True,      \n",
    "    save_individual=True,    \n",
    "    plot_lines=True,         \n",
    "    plot_points=True         \n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
