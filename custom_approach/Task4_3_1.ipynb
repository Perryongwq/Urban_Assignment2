{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import osmnx as ox\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, LineString, box\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import networkx as nx\n",
    "from dataclasses import dataclass\n",
    "import rtree\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from math import pi\n",
    "from typing import List, Dict, Optional\n",
    "import seaborn\n",
    "\n",
    "import ast\n",
    "import os\n",
    "import folium\n",
    "from datetime import datetime\n",
    "from folium import plugins\n",
    "from branca.colormap import LinearColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted Constants for improved confidence\n",
    "SIGMA_Z = 15.0  # Increased sigma_z for more tolerance in emission\n",
    "MAX_DISTANCE = 50.0  # Increased max_distance for broader candidate search\n",
    "TURN_ANGLE_THRESHOLD = pi / 4  # 45 degrees threshold for transition penalty\n",
    "MIN_TRANSITION_PROB = 1e-5  # Non-zero transition probability for flexibility\n",
    "\n",
    "def process_trajectory(polyline_str: str) -> List[tuple]:\n",
    "    \"\"\"Process trajectory string into coordinates with more lenient validation\"\"\"\n",
    "    try:\n",
    "        if not isinstance(polyline_str, str):\n",
    "            return None\n",
    "        coords = ast.literal_eval(polyline_str)\n",
    "        if not coords:\n",
    "            return None\n",
    "        \n",
    "        # More lenient validation - allow trajectories with at least 2 points\n",
    "        valid_coords = []\n",
    "        for coord in coords:\n",
    "            if len(coord) == 2:\n",
    "                # More forgiving coordinate validation\n",
    "                x, y = coord\n",
    "                if isinstance(x, (int, float)) and isinstance(y, (int, float)):\n",
    "                    # Wider coordinate bounds\n",
    "                    if -180 <= x <= 180 and -90 <= y <= 90:\n",
    "                        valid_coords.append(coord)\n",
    "        \n",
    "        return valid_coords if len(valid_coords) >= 2 else None\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error processing trajectory: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class EnhancedViterbiMatcher:\n",
    "    def __init__(self, graph, edges_gdf, config=None):\n",
    "        \"\"\"Initialize matcher with improved configuration\"\"\"\n",
    "        self.graph = graph\n",
    "        self.edges_gdf = edges_gdf.copy()\n",
    "        \n",
    "        if isinstance(self.edges_gdf.index, pd.MultiIndex):\n",
    "            self.edges_gdf = self.edges_gdf.reset_index(drop=True)\n",
    "        self.edges_gdf.index = range(len(self.edges_gdf))\n",
    "        \n",
    "        # Enhanced default configuration\n",
    "        default_config = {\n",
    "            'max_candidates': 20,          # Increased from 10\n",
    "            'max_distance': 100.0,         # Increased from 50.0\n",
    "            'sigma_z': 50.0,              # Adjusted for better GPS noise handling\n",
    "            'beta': 2.0,                  # Increased for better transition scoring\n",
    "            'min_prob_norm': 1e-7,        # Lowered for more flexibility\n",
    "            'max_speed': 50.0,            # Maximum expected speed (m/s)\n",
    "            'min_speed': 0.1,             # Minimum expected speed (m/s)\n",
    "            'angle_tolerance': np.pi/2,    # 90 degrees angle tolerance\n",
    "            'max_angle_penalty': 0.5,      # Maximum penalty for sharp turns\n",
    "            'distance_decay': 0.85,        # Distance decay factor\n",
    "            'sequential_matching': True    # Enable sequential matching for long trajectories\n",
    "        }\n",
    "        \n",
    "        if config:\n",
    "            default_config.update(config)\n",
    "        self.config = default_config\n",
    "        \n",
    "        self._init_spatial_index()\n",
    "        self.edge_to_nodes = self._build_edge_to_nodes()\n",
    "        self.node_to_edges = self._build_node_to_edges()\n",
    "        \n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def _init_spatial_index(self):\n",
    "        \"\"\"Initialize R-tree spatial index with improved error handling\"\"\"\n",
    "        try:\n",
    "            self.spatial_index = rtree.index.Index()\n",
    "            for idx, edge in self.edges_gdf.iterrows():\n",
    "                if edge.geometry is not None and not edge.geometry.is_empty:\n",
    "                    self.spatial_index.insert(idx, edge.geometry.bounds)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error initializing spatial index: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _build_edge_to_nodes(self) -> Dict[int, set]:\n",
    "        \"\"\"Build mapping from edge IDs to their endpoint nodes with validation\"\"\"\n",
    "        edge_to_nodes = {}\n",
    "        for idx, edge in self.edges_gdf.iterrows():\n",
    "            if edge.geometry is not None and not edge.geometry.is_empty:\n",
    "                coords = list(edge.geometry.coords)\n",
    "                if len(coords) >= 2:  # Ensure valid linestring\n",
    "                    edge_to_nodes[idx] = {\n",
    "                        self._get_node_id(coords[0]),\n",
    "                        self._get_node_id(coords[-1])\n",
    "                    }\n",
    "        return edge_to_nodes\n",
    "\n",
    "    def _build_node_to_edges(self) -> Dict[tuple, set]:\n",
    "        \"\"\"Build mapping from nodes to connected edge IDs\"\"\"\n",
    "        node_to_edges = {}\n",
    "        for edge_id, nodes in self.edge_to_nodes.items():\n",
    "            for node in nodes:\n",
    "                if node not in node_to_edges:\n",
    "                    node_to_edges[node] = set()\n",
    "                node_to_edges[node].add(edge_id)\n",
    "        return node_to_edges\n",
    "\n",
    "    def _get_node_id(self, coord: tuple) -> tuple:\n",
    "        \"\"\"Convert coordinate to node ID with improved precision\"\"\"\n",
    "        return tuple(round(x, 6) for x in coord)\n",
    "\n",
    "    def _find_candidates(self, point: Point) -> List[dict]:\n",
    "        \"\"\"Enhanced candidate finding with adaptive search radius\"\"\"\n",
    "        candidates = []\n",
    "        initial_distance = self.config['max_distance']\n",
    "        max_attempts = 3\n",
    "        current_distance = initial_distance\n",
    "        \n",
    "        for attempt in range(max_attempts):\n",
    "            bounds = (\n",
    "                point.x - current_distance,\n",
    "                point.y - current_distance,\n",
    "                point.x + current_distance,\n",
    "                point.y + current_distance\n",
    "            )\n",
    "            \n",
    "            for idx in self.spatial_index.intersection(bounds):\n",
    "                edge = self.edges_gdf.loc[idx]\n",
    "                if edge.geometry is not None:\n",
    "                    dist = point.distance(edge.geometry)\n",
    "                    if dist <= current_distance:\n",
    "                        proj_point = edge.geometry.interpolate(\n",
    "                            edge.geometry.project(point)\n",
    "                        )\n",
    "                        candidates.append({\n",
    "                            'edge_id': idx,\n",
    "                            'distance': dist,\n",
    "                            'proj_point': proj_point,\n",
    "                            'edge': edge\n",
    "                        })\n",
    "            \n",
    "            if candidates:\n",
    "                break\n",
    "                \n",
    "            current_distance *= 1.5  # Increase search radius for next attempt\n",
    "        \n",
    "        # Sort by distance and apply adaptive limit\n",
    "        candidates.sort(key=lambda x: x['distance'])\n",
    "        return candidates[:self.config['max_candidates']]\n",
    "\n",
    "    def _calculate_emission_prob(self, point: Point, candidate: dict) -> float:\n",
    "        \"\"\"Enhanced emission probability calculation with improved scaling\"\"\"\n",
    "        distance = candidate['distance']\n",
    "        sigma_z = self.config['sigma_z']\n",
    "        \n",
    "        # Distance-based probability with decay\n",
    "        distance_factor = np.exp(-distance * self.config['distance_decay'])\n",
    "        \n",
    "        # Gaussian probability\n",
    "        gaussian_prob = np.exp(-0.5 * (distance / sigma_z) ** 2)\n",
    "        \n",
    "        # Combined probability\n",
    "        prob = gaussian_prob * distance_factor\n",
    "        \n",
    "        return max(prob, self.config['min_prob_norm'])\n",
    "\n",
    "    def _calculate_transition_prob(self, prev_edge: int, curr_edge: int,\n",
    "                                 prev_point: Point, curr_point: Point) -> float:\n",
    "        \"\"\"Enhanced transition probability calculation with improved angle handling\"\"\"\n",
    "        prev_nodes = self.edge_to_nodes[prev_edge]\n",
    "        curr_nodes = self.edge_to_nodes[curr_edge]\n",
    "        \n",
    "        # Check connectivity with more flexibility\n",
    "        connected = bool(prev_nodes.intersection(curr_nodes))\n",
    "        connectivity_score = 1.0 if connected else 0.3\n",
    "        \n",
    "        # Calculate angle similarity\n",
    "        dir1 = np.array(prev_point.coords[-1]) - np.array(prev_point.coords[0])\n",
    "        dir2 = np.array(curr_point.coords[-1]) - np.array(curr_point.coords[0])\n",
    "        \n",
    "        norm1 = np.linalg.norm(dir1)\n",
    "        norm2 = np.linalg.norm(dir2)\n",
    "        \n",
    "        if norm1 == 0 or norm2 == 0:\n",
    "            angle_score = 1.0\n",
    "        else:\n",
    "            cos_angle = np.dot(dir1, dir2) / (norm1 * norm2)\n",
    "            angle = np.arccos(np.clip(cos_angle, -1.0, 1.0))\n",
    "            \n",
    "            # Smoother angle penalty\n",
    "            angle_score = 1.0 - (angle / self.config['angle_tolerance']) * self.config['max_angle_penalty']\n",
    "            angle_score = max(angle_score, 1.0 - self.config['max_angle_penalty'])\n",
    "        \n",
    "        # Combined probability\n",
    "        prob = connectivity_score * angle_score\n",
    "        \n",
    "        return max(prob, self.config['min_prob_norm'])\n",
    "\n",
    "    def _viterbi_matching(self, points: List[Point], candidates_by_point: List[List[dict]]) -> List[Dict]:\n",
    "        \"\"\"Improved Viterbi algorithm with better numerical stability\"\"\"\n",
    "        n_points = len(points)\n",
    "        states = [{} for _ in range(n_points)]\n",
    "        \n",
    "        # Initialize first state with log probabilities\n",
    "        for candidate in candidates_by_point[0]:\n",
    "            edge_id = candidate['edge_id']\n",
    "            log_emission = np.log(self._calculate_emission_prob(points[0], candidate))\n",
    "            states[0][edge_id] = {\n",
    "                'log_prob': log_emission,\n",
    "                'prev': None,\n",
    "                'emission': log_emission,\n",
    "                'transition': 0.0\n",
    "            }\n",
    "        \n",
    "        # Forward pass with log probabilities\n",
    "        for t in range(1, n_points):\n",
    "            for candidate in candidates_by_point[t]:\n",
    "                curr_edge = candidate['edge_id']\n",
    "                log_emission = np.log(self._calculate_emission_prob(points[t], candidate))\n",
    "                \n",
    "                best_log_prob = float('-inf')\n",
    "                best_prev = None\n",
    "                best_transition = None\n",
    "                \n",
    "                for prev_edge, prev_state in states[t-1].items():\n",
    "                    trans_prob = self._calculate_transition_prob(\n",
    "                        prev_edge, curr_edge, points[t-1], points[t]\n",
    "                    )\n",
    "                    log_transition = np.log(trans_prob)\n",
    "                    \n",
    "                    log_prob = prev_state['log_prob'] + log_transition + log_emission\n",
    "                    \n",
    "                    if log_prob > best_log_prob:\n",
    "                        best_log_prob = log_prob\n",
    "                        best_prev = prev_edge\n",
    "                        best_transition = log_transition\n",
    "                \n",
    "                if best_prev is not None:\n",
    "                    states[t][curr_edge] = {\n",
    "                        'log_prob': best_log_prob,\n",
    "                        'prev': best_prev,\n",
    "                        'emission': log_emission,\n",
    "                        'transition': best_transition\n",
    "                    }\n",
    "        \n",
    "        # Convert log probabilities to normalized confidence scores\n",
    "        if states[-1]:\n",
    "            log_probs = np.array([state['log_prob'] for state in states[-1].values()])\n",
    "            max_log_prob = np.max(log_probs)\n",
    "            normalized_probs = np.exp(log_probs - max_log_prob)\n",
    "            normalized_probs /= np.sum(normalized_probs)\n",
    "            \n",
    "            for edge_id, norm_prob in zip(states[-1].keys(), normalized_probs):\n",
    "                states[-1][edge_id]['confidence'] = norm_prob\n",
    "        \n",
    "        return states\n",
    "\n",
    "    def _backtrack(self, states: List[Dict]) -> List[int]:\n",
    "        \"\"\"Backtrack to find the best path with improved handling of edge cases\"\"\"\n",
    "        if not states or not states[-1]:\n",
    "            return []\n",
    "        \n",
    "        path = []\n",
    "        current_edge = max(states[-1].items(), key=lambda x: x[1]['log_prob'])[0]\n",
    "        \n",
    "        for t in range(len(states) - 1, -1, -1):\n",
    "            path.append(current_edge)\n",
    "            if t > 0 and states[t][current_edge]['prev'] is not None:\n",
    "                current_edge = states[t][current_edge]['prev']\n",
    "        \n",
    "        return list(reversed(path))\n",
    "\n",
    "    def _sequential_matching(self, points: List[Point]) -> Dict:\n",
    "        \"\"\"Match long trajectories in sequential segments with overlap\"\"\"\n",
    "        segment_size = 30\n",
    "        overlap = 10\n",
    "        all_edges = []\n",
    "        segment_confidences = []\n",
    "        \n",
    "        for i in range(0, len(points), segment_size - overlap):\n",
    "            segment = points[i:i + segment_size]\n",
    "            if len(segment) < 2:\n",
    "                continue\n",
    "                \n",
    "            candidates = [self._find_candidates(p) for p in segment]\n",
    "            if not all(candidates):\n",
    "                continue\n",
    "                \n",
    "            states = self._viterbi_matching(segment, candidates)\n",
    "            path = self._backtrack(states)\n",
    "            \n",
    "            if path:\n",
    "                if states[-1] and path[-1] in states[-1]:\n",
    "                    segment_confidences.append(states[-1][path[-1]].get('confidence', 0.0))\n",
    "                    \n",
    "                # Remove overlap with previous segment\n",
    "                if all_edges and overlap > 0:\n",
    "                    all_edges = all_edges[:-overlap]\n",
    "                all_edges.extend(path)\n",
    "        \n",
    "        if not all_edges:\n",
    "            return {'success': False, 'edges': [], 'confidence': 0.0}\n",
    "        \n",
    "        # Calculate overall confidence as average of segment confidences\n",
    "        overall_confidence = np.mean(segment_confidences) if segment_confidences else 0.0\n",
    "            \n",
    "        return {\n",
    "            'success': True,\n",
    "            'edges': all_edges,\n",
    "            'confidence': overall_confidence\n",
    "        }\n",
    "\n",
    "    def match_trajectory(self, points: List[Tuple[float, float]]) -> Dict:\n",
    "        \"\"\"Match trajectory with improved error handling and validation\"\"\"\n",
    "        try:\n",
    "            if len(points) < 2:\n",
    "                return {'success': False, 'edges': [], 'confidence': 0.0}\n",
    "\n",
    "            point_objects = [Point(p) for p in points]\n",
    "            \n",
    "            # Use sequential matching for long trajectories\n",
    "            if self.config['sequential_matching'] and len(points) > 50:\n",
    "                return self._sequential_matching(point_objects)\n",
    "            \n",
    "            # Standard matching for shorter trajectories\n",
    "            candidates_by_point = [self._find_candidates(p) for p in point_objects]\n",
    "            \n",
    "            if not all(candidates_by_point):\n",
    "                return {'success': False, 'edges': [], 'confidence': 0.0}\n",
    "            \n",
    "            states = self._viterbi_matching(point_objects, candidates_by_point)\n",
    "            path = self._backtrack(states)\n",
    "            \n",
    "            if not path:\n",
    "                return {'success': False, 'edges': [], 'confidence': 0.0}\n",
    "            \n",
    "            confidence = states[-1][path[-1]].get('confidence', 0.0)\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'edges': path,\n",
    "                'confidence': confidence\n",
    "            }\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in match_trajectory: {str(e)}\")\n",
    "            return {'success': False, 'edges': [], 'confidence': 0.0}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapMatchingReportGenerator:\n",
    "    \"\"\"Enhanced report generator for map matching results\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: str = 'map_matching_results'):\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    def generate_quality_metrics(self, matched_results: List[Dict], matcher) -> pd.DataFrame:\n",
    "        \"\"\"Generate quality metrics for matched trajectories\"\"\"\n",
    "        metrics = []\n",
    "        \n",
    "        for idx, result in enumerate(matched_results):\n",
    "            edges = result['match_result']['edges']\n",
    "            confidence = result['match_result']['confidence']\n",
    "            \n",
    "            # Calculate path metrics\n",
    "            path_length = sum(matcher.edges_gdf.iloc[edge_id].geometry.length \n",
    "                            for edge_id in edges)\n",
    "            \n",
    "            # Calculate continuity score\n",
    "            connected_pairs = 0\n",
    "            total_pairs = len(edges) - 1\n",
    "            for i in range(total_pairs):\n",
    "                edge1 = matcher.edges_gdf.iloc[edges[i]]\n",
    "                edge2 = matcher.edges_gdf.iloc[edges[i+1]]\n",
    "                nodes1 = set([edge1.geometry.coords[0], edge1.geometry.coords[-1]])\n",
    "                nodes2 = set([edge2.geometry.coords[0], edge2.geometry.coords[-1]])\n",
    "                if nodes1.intersection(nodes2):\n",
    "                    connected_pairs += 1\n",
    "            \n",
    "            continuity_score = connected_pairs / max(total_pairs, 1)\n",
    "            \n",
    "            metrics.append({\n",
    "                'trajectory_id': idx,\n",
    "                'confidence': confidence,\n",
    "                'path_length_meters': path_length,\n",
    "                'num_segments': len(edges),\n",
    "                'continuity_score': continuity_score,\n",
    "                'avg_segment_length': path_length / len(edges)\n",
    "            })\n",
    "            \n",
    "        df = pd.DataFrame(metrics)\n",
    "        df.to_csv(os.path.join(self.output_dir, 'quality_metrics.csv'), index=False)\n",
    "        return df\n",
    "\n",
    "    def generate_trajectory_analysis(self, matched_results: List[Dict]) -> pd.DataFrame:\n",
    "        \"\"\"Generate trajectory-specific analysis\"\"\"\n",
    "        analysis = []\n",
    "        \n",
    "        for idx, result in enumerate(matched_results):\n",
    "            coords = np.array(result['original_coords'])\n",
    "            \n",
    "            # Calculate metrics\n",
    "            distances = np.sqrt(np.sum((coords[1:] - coords[:-1])**2, axis=1))\n",
    "            avg_distance = np.mean(distances)\n",
    "            \n",
    "            angles = []\n",
    "            for i in range(len(coords) - 2):\n",
    "                v1 = coords[i+1] - coords[i]\n",
    "                v2 = coords[i+2] - coords[i+1]\n",
    "                angle = np.arctan2(np.cross(v1, v2), np.dot(v1, v2))\n",
    "                angles.append(abs(angle))\n",
    "            \n",
    "            analysis.append({\n",
    "                'trajectory_id': idx,\n",
    "                'num_points': len(coords),\n",
    "                'matched_segments': len(result['match_result']['edges']),\n",
    "                'confidence': result['match_result']['confidence'],\n",
    "                'avg_point_distance': avg_distance,\n",
    "                'avg_angle': np.mean(angles) if angles else 0,\n",
    "                'max_angle': np.max(angles) if angles else 0\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(analysis)\n",
    "        df.to_csv(os.path.join(self.output_dir, 'trajectory_analysis.csv'), index=False)\n",
    "        return df\n",
    "\n",
    "    def generate_summary_report(self, quality_metrics: pd.DataFrame, trajectory_analysis: pd.DataFrame):\n",
    "        \"\"\"Generate comprehensive summary report\"\"\"\n",
    "        report = \"Map Matching Summary Report\\n\"\n",
    "        report += \"========================\\n\\n\"\n",
    "        \n",
    "        # Overall statistics\n",
    "        report += \"Overall Statistics:\\n\"\n",
    "        report += \"-----------------\\n\"\n",
    "        report += f\"Total trajectories processed: {len(quality_metrics)}\\n\"\n",
    "        report += f\"Average confidence: {quality_metrics['confidence'].mean():.3f}\\n\"\n",
    "        report += f\"Average path length: {quality_metrics['path_length_meters'].mean():.1f} meters\\n\"\n",
    "        report += f\"Average continuity score: {quality_metrics['continuity_score'].mean():.3f}\\n\\n\"\n",
    "        \n",
    "        # Quality metrics summary\n",
    "        report += \"Quality Metrics Summary:\\n\"\n",
    "        report += \"---------------------\\n\"\n",
    "        report += quality_metrics.describe().to_string() + \"\\n\\n\"\n",
    "        \n",
    "        # Trajectory analysis summary\n",
    "        report += \"Trajectory Analysis Summary:\\n\"\n",
    "        report += \"-------------------------\\n\"\n",
    "        report += trajectory_analysis.describe().to_string() + \"\\n\"\n",
    "        \n",
    "        with open(os.path.join(self.output_dir, 'summary_report.txt'), 'w') as f:\n",
    "            f.write(report)\n",
    "\n",
    "    def generate_all_reports(self, matched_results: List[Dict], matcher):\n",
    "        \"\"\"Generate all reports in one go\"\"\"\n",
    "        # Generate quality metrics and trajectory analysis\n",
    "        quality_metrics = self.generate_quality_metrics(matched_results, matcher)\n",
    "        trajectory_analysis = self.generate_trajectory_analysis(matched_results)\n",
    "        \n",
    "        # Generate summary report\n",
    "        self.generate_summary_report(quality_metrics, trajectory_analysis)\n",
    "\n",
    "def generate_all_reports(matched_results: List[Dict], matcher, output_dir: str = 'map_matching_results'):\n",
    "    \"\"\"Main function to generate all reports\"\"\"\n",
    "    try:\n",
    "        # Initialize report generator\n",
    "        report_generator = MapMatchingReportGenerator(output_dir)\n",
    "        \n",
    "        # Generate all reports\n",
    "        report_generator.generate_all_reports(matched_results, matcher)\n",
    "        \n",
    "        print(f\"Successfully generated reports for {len(matched_results)} trajectories\")\n",
    "        print(f\"Reports saved in {output_dir} folder\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating reports: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedProbabilityReportGenerator:\n",
    "    \"\"\"Generate detailed probability analysis reports for map matching results\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: str = 'map_matching_results'):\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        self.prob_dir = os.path.join(output_dir, 'probability_analysis')\n",
    "        os.makedirs(self.prob_dir, exist_ok=True)\n",
    "        \n",
    "    def calculate_detailed_probabilities(self, matcher, trajectory_points: List[Point], \n",
    "                                      matched_edges: List[int], states: List[Dict]) -> Dict:\n",
    "        \"\"\"Calculate detailed probabilities for a matched trajectory\"\"\"\n",
    "        \n",
    "        detailed_probs = {\n",
    "            'emission_probs': [],\n",
    "            'transition_probs': [],\n",
    "            'path_probs': [],\n",
    "            'cumulative_probs': []\n",
    "        }\n",
    "        \n",
    "        # Calculate emission probabilities for each point\n",
    "        for point_idx, point in enumerate(trajectory_points):\n",
    "            point_emissions = []\n",
    "            edge_id = matched_edges[point_idx]\n",
    "            edge_geom = matcher.edges_gdf.loc[edge_id].geometry\n",
    "            \n",
    "            # Get candidate edges and their probabilities\n",
    "            candidates = matcher._find_candidates(point)\n",
    "            for candidate in candidates:\n",
    "                prob = matcher._calculate_emission_prob(point, candidate)\n",
    "                point_emissions.append({\n",
    "                    'edge_id': candidate['edge_id'],\n",
    "                    'probability': prob,\n",
    "                    'distance': candidate['distance'],\n",
    "                    'is_matched': candidate['edge_id'] == edge_id\n",
    "                })\n",
    "            \n",
    "            detailed_probs['emission_probs'].append({\n",
    "                'point_idx': point_idx,\n",
    "                'candidates': point_emissions\n",
    "            })\n",
    "        \n",
    "        # Calculate transition probabilities between consecutive points\n",
    "        for i in range(len(trajectory_points) - 1):\n",
    "            curr_edge = matched_edges[i]\n",
    "            next_edge = matched_edges[i + 1]\n",
    "            \n",
    "            trans_prob = matcher._calculate_transition_prob(\n",
    "                curr_edge, next_edge,\n",
    "                trajectory_points[i], trajectory_points[i + 1]\n",
    "            )\n",
    "            \n",
    "            # Get additional transition details\n",
    "            curr_nodes = matcher.edge_to_nodes[curr_edge]\n",
    "            next_nodes = matcher.edge_to_nodes[next_edge]\n",
    "            is_connected = bool(curr_nodes.intersection(next_nodes))\n",
    "            \n",
    "            detailed_probs['transition_probs'].append({\n",
    "                'segment_idx': i,\n",
    "                'from_edge': curr_edge,\n",
    "                'to_edge': next_edge,\n",
    "                'probability': trans_prob,\n",
    "                'is_connected': is_connected\n",
    "            })\n",
    "        \n",
    "        # Extract path probabilities from Viterbi states\n",
    "        for t, state in enumerate(states):\n",
    "            if matched_edges[t] in state:\n",
    "                state_info = state[matched_edges[t]]\n",
    "                detailed_probs['path_probs'].append({\n",
    "                    'step': t,\n",
    "                    'edge_id': matched_edges[t],\n",
    "                    'log_prob': state_info['log_prob'],\n",
    "                    'emission': state_info['emission'],\n",
    "                    'transition': state_info.get('transition', 0.0)\n",
    "                })\n",
    "                \n",
    "                # Calculate cumulative probability\n",
    "                cum_prob = np.exp(state_info['log_prob'])\n",
    "                detailed_probs['cumulative_probs'].append({\n",
    "                    'step': t,\n",
    "                    'cumulative_prob': cum_prob\n",
    "                })\n",
    "        \n",
    "        return detailed_probs\n",
    "    \n",
    "    def generate_probability_report(self, match_result: Dict, matched_edges: List[int], \n",
    "                                  detailed_probs: Dict, trajectory_id: int):\n",
    "        \"\"\"Generate detailed probability report for a single trajectory\"\"\"\n",
    "        \n",
    "        report = f\"Probability Analysis Report - Trajectory {trajectory_id}\\n\"\n",
    "        report += \"=\" * 50 + \"\\n\\n\"\n",
    "        \n",
    "        # Overall match statistics\n",
    "        report += \"Overall Match Statistics:\\n\"\n",
    "        report += \"-----------------------\\n\"\n",
    "        report += f\"Final confidence score: {match_result['confidence']:.4f}\\n\"\n",
    "        report += f\"Number of matched segments: {len(matched_edges)}\\n\"\n",
    "        report += f\"Match success: {match_result['success']}\\n\\n\"\n",
    "        \n",
    "        # Emission probability analysis\n",
    "        report += \"Emission Probability Analysis:\\n\"\n",
    "        report += \"---------------------------\\n\"\n",
    "        for point_data in detailed_probs['emission_probs']:\n",
    "            report += f\"\\nPoint {point_data['point_idx']}:\\n\"\n",
    "            matched_candidate = None\n",
    "            for candidate in point_data['candidates']:\n",
    "                if candidate['is_matched']:\n",
    "                    matched_candidate = candidate\n",
    "                    report += f\"* Selected edge {candidate['edge_id']}: \"\n",
    "                    report += f\"prob = {candidate['probability']:.4f}, \"\n",
    "                    report += f\"distance = {candidate['distance']:.2f}m\\n\"\n",
    "                    break\n",
    "            \n",
    "            # Calculate statistics for alternatives\n",
    "            alt_probs = [c['probability'] for c in point_data['candidates'] \n",
    "                        if not c['is_matched']]\n",
    "            if alt_probs:\n",
    "                report += f\"  Alternative candidates: {len(alt_probs)}\\n\"\n",
    "                report += f\"  Max alternative prob: {max(alt_probs):.4f}\\n\"\n",
    "                report += f\"  Avg alternative prob: {np.mean(alt_probs):.4f}\\n\"\n",
    "        \n",
    "        # Transition probability analysis\n",
    "        report += \"\\nTransition Probability Analysis:\\n\"\n",
    "        report += \"-----------------------------\\n\"\n",
    "        for trans in detailed_probs['transition_probs']:\n",
    "            report += f\"\\nSegment {trans['segment_idx']} → {trans['segment_idx']+1}:\\n\"\n",
    "            report += f\"* Edge {trans['from_edge']} → {trans['to_edge']}\\n\"\n",
    "            report += f\"* Probability: {trans['probability']:.4f}\\n\"\n",
    "            report += f\"* Connected: {'Yes' if trans['is_connected'] else 'No'}\\n\"\n",
    "        \n",
    "        # Path probability analysis\n",
    "        report += \"\\nPath Probability Analysis:\\n\"\n",
    "        report += \"----------------------\\n\"\n",
    "        for path_prob in detailed_probs['path_probs']:\n",
    "            report += f\"\\nStep {path_prob['step']}:\\n\"\n",
    "            report += f\"* Edge: {path_prob['edge_id']}\\n\"\n",
    "            report += f\"* Log probability: {path_prob['log_prob']:.4f}\\n\"\n",
    "            report += f\"* Emission contribution: {path_prob['emission']:.4f}\\n\"\n",
    "            report += f\"* Transition contribution: {path_prob['transition']:.4f}\\n\"\n",
    "        \n",
    "        # Save the report\n",
    "        filename = os.path.join(self.prob_dir, f'trajectory_{trajectory_id}_probability_analysis.txt')\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(report)\n",
    "            \n",
    "        return report\n",
    "    \n",
    "    def generate_summary_visualizations(self, all_detailed_probs: List[Dict], trajectory_ids: List[int]):\n",
    "        \"\"\"Generate summary visualizations of probability distributions\"\"\"\n",
    "        \n",
    "        # Set up the plotting environment\n",
    "        plt.style.use('seaborn-v0_8-whitegrid')\n",
    "        \n",
    "        # 1. Emission Probability Distribution\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        all_emissions = []\n",
    "        for probs in all_detailed_probs:\n",
    "            for point_data in probs['emission_probs']:\n",
    "                all_emissions.extend([c['probability'] for c in point_data['candidates']])\n",
    "        \n",
    "        plt.hist(all_emissions, bins=50, alpha=0.7)\n",
    "        plt.title('Distribution of Emission Probabilities')\n",
    "        plt.xlabel('Probability')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.savefig(os.path.join(self.prob_dir, 'emission_probability_distribution.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # 2. Transition Probability Distribution\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        all_transitions = []\n",
    "        for probs in all_detailed_probs:\n",
    "            all_transitions.extend([t['probability'] for t in probs['transition_probs']])\n",
    "        \n",
    "        plt.hist(all_transitions, bins=50, alpha=0.7)\n",
    "        plt.title('Distribution of Transition Probabilities')\n",
    "        plt.xlabel('Probability')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.savefig(os.path.join(self.prob_dir, 'transition_probability_distribution.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # 3. Cumulative Probability Evolution\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for idx, probs in enumerate(all_detailed_probs):\n",
    "            cum_probs = [p['cumulative_prob'] for p in probs['cumulative_probs']]\n",
    "            plt.plot(cum_probs, label=f'Trajectory {trajectory_ids[idx]}')\n",
    "        \n",
    "        plt.title('Evolution of Cumulative Probabilities')\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Cumulative Probability')\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(self.prob_dir, 'cumulative_probability_evolution.png'))\n",
    "        plt.close()\n",
    "\n",
    "def generate_probability_reports(matched_results: List[Dict], matcher, output_dir: str = 'map_matching_results'):\n",
    "    \"\"\"Main function to generate all probability reports\"\"\"\n",
    "    prob_reporter = EnhancedProbabilityReportGenerator(output_dir)\n",
    "    all_detailed_probs = []\n",
    "    trajectory_ids = []\n",
    "    \n",
    "    for idx, result in enumerate(matched_results):\n",
    "        try:\n",
    "            # Get the matched trajectory details\n",
    "            trajectory_points = [Point(p) for p in result['original_coords']]\n",
    "            matched_edges = result['match_result']['edges']\n",
    "            \n",
    "            # Calculate detailed probabilities\n",
    "            detailed_probs = prob_reporter.calculate_detailed_probabilities(\n",
    "                matcher, trajectory_points, matched_edges, result.get('states', [])\n",
    "            )\n",
    "            \n",
    "            # Generate individual report\n",
    "            prob_reporter.generate_probability_report(\n",
    "                result['match_result'], matched_edges, detailed_probs, idx\n",
    "            )\n",
    "            \n",
    "            all_detailed_probs.append(detailed_probs)\n",
    "            trajectory_ids.append(idx)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating probability report for trajectory {idx}: {str(e)}\")\n",
    "    \n",
    "    # Generate summary visualizations\n",
    "    prob_reporter.generate_summary_visualizations(all_detailed_probs, trajectory_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading road network...\n",
      "/var/folders/_6/2sxd3q_d65b9mp5y1l5_0g7r0000gn/T/ipykernel_56150/2454098061.py:216: FutureWarning: The buffer_dist argument has been deprecated and will be removed in the v2.0.0 release. Buffer your query area directly, if desired. See the OSMnx v2 migration guide: https://github.com/gboeing/osmnx/issues/1123\n",
      "  G = ox.graph_from_place('Porto, Portugal', network_type='drive', buffer_dist=2000)\n",
      "/Users/macbookpro/miniconda3/envs/urbancom2/lib/python3.11/site-packages/osmnx/graph.py:387: FutureWarning: The buffer_dist argument has been deprecated and will be removed in the v2.0.0 release. Buffer your results directly, if desired. See the OSMnx v2 migration guide: https://github.com/gboeing/osmnx/issues/1123\n",
      "  gdf_place = geocoder.geocode_to_gdf(\n",
      "INFO:__main__:Network bounds (WGS84): [-8.6968313 41.1205698 -8.5288196 41.2038855]\n",
      "INFO:__main__:Buffered bounds (WGS84): (-8.7068313, 41.1105698, -8.5188196, 41.213885499999996)\n",
      "INFO:__main__:Loading trajectory data...\n",
      "INFO:__main__:Loaded 15 trajectories\n",
      "INFO:__main__:Initializing matcher...\n",
      "INFO:__main__:Processing trajectories...\n",
      "Matching trajectories:   0%|          | 0/15 [00:00<?, ?it/s]INFO:__main__:Successfully matched trajectory 0 (confidence: 0.978)\n",
      "Matching trajectories:   7%|▋         | 1/15 [00:00<00:05,  2.39it/s]INFO:__main__:Successfully matched trajectory 1 (confidence: 0.499)\n",
      "Matching trajectories:  13%|█▎        | 2/15 [00:00<00:05,  2.55it/s]INFO:__main__:Successfully matched trajectory 2 (confidence: 0.335)\n",
      "Matching trajectories:  20%|██        | 3/15 [00:02<00:10,  1.14it/s]INFO:__main__:Successfully matched trajectory 3 (confidence: 0.491)\n",
      "Matching trajectories:  27%|██▋       | 4/15 [00:03<00:09,  1.13it/s]INFO:__main__:Successfully matched trajectory 4 (confidence: 0.911)\n",
      "Matching trajectories:  33%|███▎      | 5/15 [00:03<00:07,  1.32it/s]INFO:__main__:Successfully matched trajectory 5 (confidence: 0.499)\n",
      "Matching trajectories:  40%|████      | 6/15 [00:04<00:05,  1.53it/s]INFO:__main__:Successfully matched trajectory 6 (confidence: 0.440)\n",
      "Matching trajectories:  47%|████▋     | 7/15 [00:04<00:05,  1.51it/s]INFO:__main__:Successfully matched trajectory 7 (confidence: 0.497)\n",
      "Matching trajectories:  53%|█████▎    | 8/15 [00:05<00:04,  1.56it/s]INFO:__main__:Successfully matched trajectory 8 (confidence: 0.973)\n",
      "Matching trajectories:  60%|██████    | 9/15 [00:06<00:04,  1.48it/s]INFO:__main__:Successfully matched trajectory 9 (confidence: 0.526)\n",
      "Matching trajectories:  67%|██████▋   | 10/15 [00:06<00:03,  1.63it/s]INFO:__main__:Successfully matched trajectory 10 (confidence: 0.929)\n",
      "Matching trajectories:  73%|███████▎  | 11/15 [00:07<00:02,  1.83it/s]INFO:__main__:Successfully matched trajectory 11 (confidence: 0.988)\n",
      "Matching trajectories:  80%|████████  | 12/15 [00:07<00:01,  1.53it/s]INFO:__main__:Successfully matched trajectory 12 (confidence: 0.998)\n",
      "Matching trajectories:  87%|████████▋ | 13/15 [00:08<00:01,  1.53it/s]INFO:__main__:Successfully matched trajectory 13 (confidence: 0.496)\n",
      "Matching trajectories:  93%|█████████▎| 14/15 [00:09<00:00,  1.54it/s]INFO:__main__:Successfully matched trajectory 14 (confidence: 0.997)\n",
      "Matching trajectories: 100%|██████████| 15/15 [00:09<00:00,  1.54it/s]\n",
      "INFO:__main__:Generating reports...\n",
      "ERROR:root:Error generating probability report for trajectory 2: list index out of range\n",
      "INFO:__main__:Generating visualizations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated reports for 15 trajectories\n",
      "Reports saved in map_matching_results folder\n",
      "Successfully matched 15 trajectories\n",
      "Reports and visualizations saved in map_matching_results folder\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "BUFFER_SIZE = 0.01  # roughly 1km in degrees\n",
    "UTM_CRS = 'EPSG:32629'  # UTM zone 29N for Porto\n",
    "\n",
    "def process_trajectory_with_validation(polyline_str: str) -> Optional[List[tuple]]:\n",
    "    \"\"\"Enhanced trajectory processing with better validation\"\"\"\n",
    "    try:\n",
    "        if not isinstance(polyline_str, str):\n",
    "            return None\n",
    "            \n",
    "        # Clean string and handle potential formatting issues\n",
    "        cleaned_str = polyline_str.strip().replace(' ', '')\n",
    "        if not (cleaned_str.startswith('[') and cleaned_str.endswith(']')):\n",
    "            return None\n",
    "            \n",
    "        coords = ast.literal_eval(cleaned_str)\n",
    "        if not coords or not isinstance(coords, list):\n",
    "            return None\n",
    "        \n",
    "        valid_coords = []\n",
    "        prev_coord = None\n",
    "        \n",
    "        for coord in coords:\n",
    "            if not isinstance(coord, (list, tuple)) or len(coord) != 2:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                x, y = map(float, coord)\n",
    "                \n",
    "                # Basic coordinate validation\n",
    "                if not (-180 <= x <= 180 and -90 <= y <= 90):\n",
    "                    continue\n",
    "                    \n",
    "                # Check for duplicate points\n",
    "                if prev_coord and (x, y) == prev_coord:\n",
    "                    continue\n",
    "                    \n",
    "                # Check for unrealistic jumps\n",
    "                if prev_coord:\n",
    "                    dx = x - prev_coord[0]\n",
    "                    dy = y - prev_coord[1]\n",
    "                    dist = (dx * dx + dy * dy) ** 0.5\n",
    "                    if dist > 0.1:  # About 11km at the equator\n",
    "                        continue\n",
    "                        \n",
    "                valid_coords.append((x, y))\n",
    "                prev_coord = (x, y)\n",
    "                \n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "        \n",
    "        return valid_coords if len(valid_coords) >= 2 else None\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error processing trajectory: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def validate_trajectory_bounds(coords: List[tuple], bounds_wgs84_buffered: tuple) -> bool:\n",
    "    \"\"\"Validate if trajectory is within buffered network bounds\"\"\"\n",
    "    if not coords:\n",
    "        return False\n",
    "        \n",
    "    coords_array = np.array(coords)\n",
    "    min_x_coord = coords_array[:, 0].min()\n",
    "    max_x_coord = coords_array[:, 0].max()\n",
    "    min_y_coord = coords_array[:, 1].min()\n",
    "    max_y_coord = coords_array[:, 1].max()\n",
    "    \n",
    "    return (\n",
    "        bounds_wgs84_buffered[0] <= max_x_coord and \n",
    "        min_x_coord <= bounds_wgs84_buffered[2] and\n",
    "        bounds_wgs84_buffered[1] <= max_y_coord and \n",
    "        min_y_coord <= bounds_wgs84_buffered[3]\n",
    "    )\n",
    "\n",
    "def convert_to_utm(coords: List[tuple], utm_crs: str) -> List[tuple]:\n",
    "    \"\"\"Convert coordinates from WGS84 to UTM\"\"\"\n",
    "    point_gdf = gpd.GeoDataFrame(\n",
    "        geometry=[Point(x, y) for x, y in coords],\n",
    "        crs='EPSG:4326'\n",
    "    ).to_crs(utm_crs)\n",
    "    \n",
    "    return [(p.x, p.y) for p in point_gdf.geometry]\n",
    "\n",
    "\n",
    "\n",
    "def visualize_individual_matches(matcher, matched_results: List[Dict], output_dir: str):\n",
    "    \"\"\"Generate individual interactive visualizations for each matched route\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Convert edges to WGS84 for visualization\n",
    "    edges_wgs84 = matcher.edges_gdf.to_crs('EPSG:4326')\n",
    "    \n",
    "    # Center coordinates for Porto\n",
    "    center_lat, center_lon = 41.1579, -8.6291\n",
    "    \n",
    "    # Create color scheme\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, 15))  # Fixed for 15 trajectories\n",
    "    colors = ['#%02x%02x%02x' % tuple(map(int, c[:3] * 255)) for c in colors]\n",
    "    \n",
    "    # Process each trajectory index\n",
    "    processed_indices = set(result.get('trajectory_id', i) for i, result in enumerate(matched_results))\n",
    "    \n",
    "    # Generate individual maps for each trajectory\n",
    "    for idx in range(15):  # Process all 15 trajectories\n",
    "        # Create new base map for each trajectory\n",
    "        individual_map = folium.Map(\n",
    "            location=[center_lat, center_lon],\n",
    "            zoom_start=13,\n",
    "            tiles='cartodbpositron'\n",
    "        )\n",
    "        \n",
    "        # Add road network\n",
    "        for _, edge in edges_wgs84.iterrows():\n",
    "            if edge.geometry is not None:\n",
    "                coords = [(y, x) for x, y in edge.geometry.coords]\n",
    "                folium.PolyLine(\n",
    "                    coords,\n",
    "                    weight=1,\n",
    "                    color='gray',\n",
    "                    opacity=0.5\n",
    "                ).add_to(individual_map)\n",
    "        \n",
    "        # Find the matching result for this trajectory index\n",
    "        matched_result = None\n",
    "        for result in matched_results:\n",
    "            if result.get('trajectory_id', -1) == idx or (idx not in processed_indices and matched_results.index(result) == idx):\n",
    "                matched_result = result\n",
    "                break\n",
    "        \n",
    "        if matched_result:\n",
    "            # Draw original trajectory\n",
    "            coords = [(y, x) for x, y in matched_result['original_coords']]\n",
    "            folium.PolyLine(\n",
    "                coords,\n",
    "                weight=2,\n",
    "                color=colors[idx],\n",
    "                opacity=0.5,\n",
    "                popup='Original Trajectory'\n",
    "            ).add_to(individual_map)\n",
    "            \n",
    "            # Draw matched edges\n",
    "            for edge_id in matched_result['match_result']['edges']:\n",
    "                edge = edges_wgs84.iloc[edge_id]\n",
    "                if edge.geometry is not None:\n",
    "                    coords = [(y, x) for x, y in edge.geometry.coords]\n",
    "                    folium.PolyLine(\n",
    "                        coords,\n",
    "                        weight=3,\n",
    "                        color=colors[idx],\n",
    "                        opacity=0.8,\n",
    "                        popup='Matched Route'\n",
    "                    ).add_to(individual_map)\n",
    "            \n",
    "            # Add confidence score to legend if available\n",
    "            confidence = matched_result['match_result'].get('confidence', 'N/A')\n",
    "            status = \"Successfully Matched\"\n",
    "        else:\n",
    "            status = \"Matching Failed\"\n",
    "            confidence = 'N/A'\n",
    "        \n",
    "        # Add legend with match status and confidence\n",
    "        legend_html = f\"\"\"\n",
    "            <div style=\"position: fixed; \n",
    "                        bottom: 50px; left: 50px; \n",
    "                        background-color: white;\n",
    "                        border: 2px solid grey; \n",
    "                        z-index: 1000; \n",
    "                        padding: 10px\">\n",
    "                <p><strong>Trajectory {idx + 1}</strong></p>\n",
    "                <p><span style=\"color: gray\">―――</span> Road Network</p>\n",
    "                <p><span style=\"color: {colors[idx]}\">―――</span> Trajectory</p>\n",
    "                <p>Status: {status}</p>\n",
    "                <p>Confidence: {confidence}</p>\n",
    "            </div>\n",
    "        \"\"\"\n",
    "        individual_map.get_root().html.add_child(folium.Element(legend_html))\n",
    "        \n",
    "        # Save individual map\n",
    "        individual_map.save(os.path.join(output_dir, f'trajectory_{idx + 1}.html'))\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function for map matching pipeline\"\"\"\n",
    "    \n",
    "    # Set up logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    try:\n",
    "        # Load road network with a larger buffer\n",
    "        logger.info(\"Loading road network...\")\n",
    "        G = ox.graph_from_place('Porto, Portugal', network_type='drive', buffer_dist=2000)\n",
    "        nodes, edges = ox.graph_to_gdfs(G)\n",
    "        \n",
    "        # Get the original WGS84 bounds before conversion\n",
    "        edges_wgs84 = edges.copy()\n",
    "        bounds_wgs84 = edges_wgs84.total_bounds\n",
    "        logger.info(f\"Network bounds (WGS84): {bounds_wgs84}\")\n",
    "        \n",
    "        # Add buffer to bounds\n",
    "        bounds_wgs84_buffered = (\n",
    "            bounds_wgs84[0] - BUFFER_SIZE,\n",
    "            bounds_wgs84[1] - BUFFER_SIZE,\n",
    "            bounds_wgs84[2] + BUFFER_SIZE,\n",
    "            bounds_wgs84[3] + BUFFER_SIZE\n",
    "        )\n",
    "        logger.info(f\"Buffered bounds (WGS84): {bounds_wgs84_buffered}\")\n",
    "        \n",
    "        # Convert to UTM coordinates for accurate distance calculations\n",
    "        edges = edges.to_crs(UTM_CRS)\n",
    "        \n",
    "        # Load trajectory data\n",
    "        logger.info(\"Loading trajectory data...\")\n",
    "        df = pd.read_csv('kraggle_data/train/train.csv', nrows=15)\n",
    "        logger.info(f\"Loaded {len(df)} trajectories\")\n",
    "        \n",
    "        # Initialize matcher\n",
    "        logger.info(\"Initializing matcher...\")\n",
    "        config = {\n",
    "            'max_candidates': 20,\n",
    "            'max_distance': 300.0,\n",
    "            'sigma_z': 75.0,\n",
    "            'beta': 2.0,\n",
    "            'min_prob_norm': 1e-4,\n",
    "            'max_speed': 50.0,\n",
    "            'min_speed': 0.1\n",
    "        }\n",
    "        \n",
    "        matcher = EnhancedViterbiMatcher(G, edges, config)\n",
    "        \n",
    "        # Process trajectories\n",
    "        logger.info(\"Processing trajectories...\")\n",
    "        matched_results = []\n",
    "        failed_matches = []\n",
    "        \n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Matching trajectories\"):\n",
    "            try:\n",
    "                # Process and validate trajectory\n",
    "                coords = process_trajectory_with_validation(row['POLYLINE'])\n",
    "                if not coords:\n",
    "                    logger.warning(f\"Failed to process trajectory {idx}: Invalid coordinates\")\n",
    "                    failed_matches.append({'id': idx, 'reason': 'Invalid coordinates'})\n",
    "                    continue\n",
    "                \n",
    "                # Debug output for first few trajectories\n",
    "                if idx < 3:\n",
    "                    logger.debug(f\"Trajectory {idx} bounds: \"\n",
    "                               f\"X: [{min(x for x,_ in coords)}, {max(x for x,_ in coords)}], \"\n",
    "                               f\"Y: [{min(y for _,y in coords)}, {max(y for _,y in coords)}]\")\n",
    "                \n",
    "                # Validate trajectory bounds\n",
    "                if not validate_trajectory_bounds(coords, bounds_wgs84_buffered):\n",
    "                    logger.warning(f\"Trajectory {idx} outside network bounds\")\n",
    "                    failed_matches.append({'id': idx, 'reason': 'Outside bounds'})\n",
    "                    continue\n",
    "                \n",
    "                # Convert to UTM\n",
    "                try:\n",
    "                    utm_coords = convert_to_utm(coords, UTM_CRS)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Coordinate conversion error for trajectory {idx}: {str(e)}\")\n",
    "                    failed_matches.append({'id': idx, 'reason': 'Coordinate conversion error'})\n",
    "                    continue\n",
    "                \n",
    "                # Match trajectory\n",
    "                result = match_with_retry(matcher, utm_coords)\n",
    "                if result['success']:\n",
    "                        # Get the Viterbi states from the matcher\n",
    "                        point_objects = [Point(p) for p in utm_coords]\n",
    "                        candidates = [matcher._find_candidates(p) for p in point_objects]\n",
    "                        states = matcher._viterbi_matching(point_objects, candidates)\n",
    "                        \n",
    "                        matched_results.append({\n",
    "                            'match_result': result,\n",
    "                            'original_coords': coords,\n",
    "                            'trajectory_id': idx,\n",
    "                            'states': states  # Add states to results\n",
    "                        })\n",
    "                        logger.info(f\"Successfully matched trajectory {idx} \"\n",
    "                                f\"(confidence: {result['confidence']:.3f})\")\n",
    "                else:\n",
    "                        logger.warning(f\"Failed to match trajectory {idx}\")\n",
    "                        failed_matches.append({'id': idx, 'reason': 'Matching failed'})\n",
    "                    \n",
    "                \n",
    "            \n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing trajectory {idx}: {str(e)}\")\n",
    "                failed_matches.append({'id': idx, 'reason': str(e)})\n",
    "        \n",
    "        # Generate reports\n",
    "        logger.info(\"Generating reports...\")\n",
    "        # Add probability report generation\n",
    "        generate_probability_reports(matched_results, matcher, output_dir='map_matching_results')\n",
    "        # Generate failure analysis\n",
    "        generate_failure_analysis(failed_matches, output_dir='map_matching_results')\n",
    "        \n",
    "        \n",
    "        \n",
    "        # print(\"Generating reports...\")\n",
    "        \n",
    "        # print(\"Generating reports...\")\n",
    "        if matched_results:\n",
    "            output_dir = 'map_matching_results'\n",
    "            generate_all_reports(matched_results, matcher, output_dir)\n",
    "            \n",
    "            # Generate both overview and individual visualizations\n",
    "            logger.info(\"Generating visualizations...\")\n",
    "            #visualize_matches(matcher, matched_results, output_dir)  # Overall visualization\n",
    "            visualize_individual_matches(matcher, matched_results, output_dir)  # Individual visualizations\n",
    "            \n",
    "            print(f\"Successfully matched {len(matched_results)} trajectories\")\n",
    "            print(f\"Reports and visualizations saved in {output_dir} folder\")\n",
    "        else:\n",
    "            print(\"No trajectories were successfully matched\")\n",
    "        \n",
    "        \n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Critical error in main process: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "def process_trajectory_with_validation(polyline_str: str) -> Optional[List[tuple]]:\n",
    "    \"\"\"Enhanced trajectory processing with better validation\"\"\"\n",
    "    try:\n",
    "        if not isinstance(polyline_str, str):\n",
    "            return None\n",
    "            \n",
    "        # Clean string and handle potential formatting issues\n",
    "        cleaned_str = polyline_str.strip().replace(' ', '')\n",
    "        if not (cleaned_str.startswith('[') and cleaned_str.endswith(']')):\n",
    "            return None\n",
    "            \n",
    "        coords = ast.literal_eval(cleaned_str)\n",
    "        if not coords or not isinstance(coords, list):\n",
    "            return None\n",
    "        \n",
    "        valid_coords = []\n",
    "        prev_coord = None\n",
    "        \n",
    "        for coord in coords:\n",
    "            if not isinstance(coord, (list, tuple)) or len(coord) != 2:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                x, y = map(float, coord)\n",
    "                \n",
    "                # Basic coordinate validation\n",
    "                if not (-180 <= x <= 180 and -90 <= y <= 90):\n",
    "                    continue\n",
    "                    \n",
    "                # Check for duplicate points\n",
    "                if prev_coord and (x, y) == prev_coord:\n",
    "                    continue\n",
    "                    \n",
    "                # Check for unrealistic jumps\n",
    "                if prev_coord:\n",
    "                    dx = x - prev_coord[0]\n",
    "                    dy = y - prev_coord[1]\n",
    "                    dist = (dx * dx + dy * dy) ** 0.5\n",
    "                    if dist > 0.1:  # About 11km at the equator\n",
    "                        continue\n",
    "                        \n",
    "                valid_coords.append((x, y))\n",
    "                prev_coord = (x, y)\n",
    "                \n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "        \n",
    "        return valid_coords if len(valid_coords) >= 2 else None\n",
    "        \n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def match_with_retry(matcher: EnhancedViterbiMatcher, coords: List[tuple], max_attempts: int = 3) -> Dict:\n",
    "    \"\"\"Attempt matching with different parameters if initial attempt fails\"\"\"\n",
    "    result = matcher.match_trajectory(coords)\n",
    "    \n",
    "    if result['success']:\n",
    "        return result\n",
    "    \n",
    "    # Try with increased search radius\n",
    "    for attempt in range(max_attempts - 1):\n",
    "        increased_distance = matcher.config['max_distance'] * (1.5 ** (attempt + 1))\n",
    "        increased_sigma = matcher.config['sigma_z'] * (1.2 ** (attempt + 1))\n",
    "        \n",
    "        temp_config = matcher.config.copy()\n",
    "        temp_config.update({\n",
    "            'max_distance': increased_distance,\n",
    "            'sigma_z': increased_sigma\n",
    "        })\n",
    "        \n",
    "        temp_matcher = EnhancedViterbiMatcher(\n",
    "            matcher.graph, \n",
    "            matcher.edges_gdf,\n",
    "            config=temp_config\n",
    "        )\n",
    "        \n",
    "        result = temp_matcher.match_trajectory(coords)\n",
    "        if result['success']:\n",
    "            return result\n",
    "    \n",
    "    return {'success': False, 'edges': [], 'confidence': 0.0}\n",
    "\n",
    "def generate_failure_analysis(failed_matches: List[Dict], output_dir: str):\n",
    "    \"\"\"Generate detailed analysis of matching failures\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    report = \"Matching Failure Analysis\\n\"\n",
    "    report += \"=======================\\n\\n\"\n",
    "    \n",
    "    # Group failures by reason\n",
    "    failure_reasons = {}\n",
    "    for failure in failed_matches:\n",
    "        reason = failure['reason']\n",
    "        if reason not in failure_reasons:\n",
    "            failure_reasons[reason] = []\n",
    "        failure_reasons[reason].append(failure['id'])\n",
    "    \n",
    "    # Generate summary\n",
    "    report += \"Summary of Failures:\\n\"\n",
    "    report += \"-----------------\\n\"\n",
    "    total_failures = len(failed_matches)\n",
    "    \n",
    "    for reason, trajectories in failure_reasons.items():\n",
    "        count = len(trajectories)\n",
    "        percentage = (count / total_failures) * 100\n",
    "        report += f\"\\n{reason}:\\n\"\n",
    "        report += f\"Count: {count} ({percentage:.1f}%)\\n\"\n",
    "        report += f\"Affected trajectories: {sorted(trajectories)}\\n\"\n",
    "    \n",
    "    # Add overall statistics\n",
    "    report += \"\\nOverall Statistics:\\n\"\n",
    "    report += \"-----------------\\n\"\n",
    "    report += f\"Total failures: {total_failures}\\n\"\n",
    "    report += f\"Unique failure reasons: {len(failure_reasons)}\\n\"\n",
    "    \n",
    "    with open(os.path.join(output_dir, 'failure_analysis.txt'), 'w') as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    # Create a visualization of failure distribution\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        reasons = list(failure_reasons.keys())\n",
    "        counts = [len(trajectories) for trajectories in failure_reasons.values()]\n",
    "        \n",
    "        plt.bar(reasons, counts)\n",
    "        plt.title('Distribution of Matching Failures')\n",
    "        plt.xlabel('Failure Reason')\n",
    "        plt.ylabel('Number of Failures')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plt.savefig(os.path.join(output_dir, 'failure_distribution.png'))\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not generate failure distribution plot: {str(e)}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Analysis of Failed Trajectories\n",
      "===================================\n",
      "\n",
      "Trajectory 2:\n",
      "--------------------\n",
      "Coordinate bounds:\n",
      "  X: [-8.650395, -8.596530]\n",
      "  Y: [41.140278, 41.154507]\n",
      "\n",
      "Trajectory characteristics:\n",
      "  Number of points: 65\n",
      "  Average distance between points: 0.003155 degrees\n",
      "  Maximum distance between points: 0.055281 degrees\n",
      "  Average angle between segments: 28.49 degrees\n",
      "  Maximum angle between segments: 172.08 degrees\n",
      "  WARNING: Contains large jumps between points\n",
      "\n",
      "Trajectory 8:\n",
      "--------------------\n",
      "Coordinate bounds:\n",
      "  X: [-8.615844, -8.589402]\n",
      "  Y: [41.140341, 41.163453]\n",
      "\n",
      "Trajectory characteristics:\n",
      "  Number of points: 38\n",
      "  Average distance between points: 0.001234 degrees\n",
      "  Maximum distance between points: 0.002874 degrees\n",
      "  Average angle between segments: 29.28 degrees\n",
      "  Maximum angle between segments: 134.96 degrees\n",
      "\n",
      "Trajectory 14:\n",
      "--------------------\n",
      "Coordinate bounds:\n",
      "  X: [-8.616024, -8.606772]\n",
      "  Y: [41.140287, 41.158260]\n",
      "\n",
      "Trajectory characteristics:\n",
      "  Number of points: 28\n",
      "  Average distance between points: 0.001065 degrees\n",
      "  Maximum distance between points: 0.002080 degrees\n",
      "  Average angle between segments: 33.22 degrees\n",
      "  Maximum angle between segments: 98.40 degrees\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def analyze_failed_trajectories():\n",
    "    \"\"\"Analyze failed trajectories from the training data\"\"\"\n",
    "    try:\n",
    "        # Load the original data\n",
    "        df = pd.read_csv('kraggle_data/train/train.csv', nrows=15)\n",
    "        \n",
    "        # Initialize report\n",
    "        report = \"Detailed Analysis of Failed Trajectories\\n\"\n",
    "        report += \"===================================\\n\\n\"\n",
    "        \n",
    "        # Known failed indices from the logs\n",
    "        failed_indices = [2, 8, 14]\n",
    "        \n",
    "        for idx in failed_indices:\n",
    "            try:\n",
    "                trajectory = df.iloc[idx]\n",
    "                coords = process_trajectory_with_validation(trajectory['POLYLINE'])\n",
    "                \n",
    "                report += f\"Trajectory {idx}:\\n\"\n",
    "                report += \"-\" * 20 + \"\\n\"\n",
    "                \n",
    "                if coords is None:\n",
    "                    report += \"Failed validation - Invalid coordinate format\\n\\n\"\n",
    "                    continue\n",
    "                    \n",
    "                coords_array = np.array(coords)\n",
    "                \n",
    "                # Analyze coordinate bounds\n",
    "                x_min, y_min = coords_array.min(axis=0)\n",
    "                x_max, y_max = coords_array.max(axis=0)\n",
    "                \n",
    "                # Network bounds from the logs\n",
    "                network_bounds = {\n",
    "                    'x_min': -8.7068313,\n",
    "                    'x_max': -8.5188196,\n",
    "                    'y_min': 41.1105698,\n",
    "                    'y_max': 41.2138855\n",
    "                }\n",
    "                \n",
    "                report += f\"Coordinate bounds:\\n\"\n",
    "                report += f\"  X: [{x_min:.6f}, {x_max:.6f}]\\n\"\n",
    "                report += f\"  Y: [{y_min:.6f}, {y_max:.6f}]\\n\"\n",
    "                \n",
    "                # Check if trajectory is within network bounds\n",
    "                within_bounds = (\n",
    "                    network_bounds['x_min'] <= x_min <= network_bounds['x_max'] and\n",
    "                    network_bounds['x_min'] <= x_max <= network_bounds['x_max'] and\n",
    "                    network_bounds['y_min'] <= y_min <= network_bounds['y_max'] and\n",
    "                    network_bounds['y_min'] <= y_max <= network_bounds['y_max']\n",
    "                )\n",
    "                \n",
    "                if not within_bounds:\n",
    "                    report += \"  WARNING: Trajectory outside network bounds\\n\"\n",
    "                    # Calculate how far outside bounds\n",
    "                    x_outside = max(0, network_bounds['x_min'] - x_min,\n",
    "                                  x_max - network_bounds['x_max'])\n",
    "                    y_outside = max(0, network_bounds['y_min'] - y_min,\n",
    "                                  y_max - network_bounds['y_max'])\n",
    "                    if x_outside > 0:\n",
    "                        report += f\"    X out of bounds by: {x_outside:.6f} degrees\\n\"\n",
    "                    if y_outside > 0:\n",
    "                        report += f\"    Y out of bounds by: {y_outside:.6f} degrees\\n\"\n",
    "                \n",
    "                # Analyze trajectory characteristics\n",
    "                num_points = len(coords)\n",
    "                distances = np.sqrt(np.sum((coords_array[1:] - coords_array[:-1])**2, axis=1))\n",
    "                avg_distance = np.mean(distances)\n",
    "                max_distance = np.max(distances)\n",
    "                \n",
    "                report += f\"\\nTrajectory characteristics:\\n\"\n",
    "                report += f\"  Number of points: {num_points}\\n\"\n",
    "                report += f\"  Average distance between points: {avg_distance:.6f} degrees\\n\"\n",
    "                report += f\"  Maximum distance between points: {max_distance:.6f} degrees\\n\"\n",
    "                \n",
    "                # Calculate angles between consecutive segments\n",
    "                if len(coords) >= 3:\n",
    "                    angles = []\n",
    "                    for i in range(len(coords) - 2):\n",
    "                        v1 = coords_array[i+1] - coords_array[i]\n",
    "                        v2 = coords_array[i+2] - coords_array[i+1]\n",
    "                        angle = np.arctan2(np.cross(v1, v2), np.dot(v1, v2))\n",
    "                        angles.append(abs(angle))\n",
    "                    \n",
    "                    max_angle = max(angles)\n",
    "                    avg_angle = np.mean(angles)\n",
    "                    report += f\"  Average angle between segments: {np.degrees(avg_angle):.2f} degrees\\n\"\n",
    "                    report += f\"  Maximum angle between segments: {np.degrees(max_angle):.2f} degrees\\n\"\n",
    "                \n",
    "                # Check for potential issues\n",
    "                if max_distance > 0.01:  # ~ 1km\n",
    "                    report += \"  WARNING: Contains large jumps between points\\n\"\n",
    "                    \n",
    "                if num_points < 3:\n",
    "                    report += \"  WARNING: Very few points for matching\\n\"\n",
    "                    \n",
    "                if any(d < 1e-5 for d in distances):  # Very close points\n",
    "                    report += \"  WARNING: Contains nearly duplicate points\\n\"\n",
    "                \n",
    "                report += \"\\n\"\n",
    "                \n",
    "            except Exception as e:\n",
    "                report += f\"Error analyzing trajectory {idx}: {str(e)}\\n\\n\"\n",
    "        \n",
    "        # Save report\n",
    "        output_dir = 'map_matching_results'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        with open(os.path.join(output_dir, 'failed_trajectories_analysis.txt'), 'w') as f:\n",
    "            f.write(report)\n",
    "            \n",
    "        return report\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error loading or processing data: {str(e)}\"\n",
    "\n",
    "# Generate the analysis\n",
    "failure_analysis = analyze_failed_trajectories()\n",
    "print(failure_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urbancom2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
