{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import osmnx as ox\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, LineString, box\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import networkx as nx\n",
    "from dataclasses import dataclass\n",
    "import ast\n",
    "import rtree\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from math import pi\n",
    "from typing import List, Dict, Optional\n",
    "import seaborn\n",
    "import os\n",
    "import folium\n",
    "from branca.colormap import LinearColormap\n",
    "from folium.features import DivIcon\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Viterbi Matcher Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SIGMA_Z = 15.0  # Increased sigma_z for more tolerance in emission\n",
    "MAX_DISTANCE = 50.0  # Increased max_distance for broader candidate search\n",
    "TURN_ANGLE_THRESHOLD = pi / 4  # 45 degrees threshold for transition penalty\n",
    "MIN_TRANSITION_PROB = 1e-5  # Non-zero transition probability for flexibility\n",
    "\n",
    "def process_trajectory(polyline_str: str) -> List[tuple]:\n",
    "    \"\"\"Process trajectory string into coordinates with more lenient validation\"\"\"\n",
    "    try:\n",
    "        if not isinstance(polyline_str, str):\n",
    "            return None\n",
    "        coords = ast.literal_eval(polyline_str)\n",
    "        if not coords:\n",
    "            return None\n",
    "        \n",
    "        # More lenient validation - allow trajectories with at least 2 points\n",
    "        valid_coords = []\n",
    "        for coord in coords:\n",
    "            if len(coord) == 2:\n",
    "                # More forgiving coordinate validation\n",
    "                x, y = coord\n",
    "                if isinstance(x, (int, float)) and isinstance(y, (int, float)):\n",
    "                    # Wider coordinate bounds\n",
    "                    if -180 <= x <= 180 and -90 <= y <= 90:\n",
    "                        valid_coords.append(coord)\n",
    "        \n",
    "        return valid_coords if len(valid_coords) >= 2 else None\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error processing trajectory: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "class EnhancedViterbiMatcher:\n",
    "    def __init__(self, graph, edges_gdf, config=None):\n",
    "        \"\"\"Initialize matcher with improved configuration\"\"\"\n",
    "        self.graph = graph\n",
    "        self.edges_gdf = edges_gdf.copy()\n",
    "        \n",
    "        if isinstance(self.edges_gdf.index, pd.MultiIndex):\n",
    "            self.edges_gdf = self.edges_gdf.reset_index(drop=True)\n",
    "        self.edges_gdf.index = range(len(self.edges_gdf))\n",
    "        \n",
    "        # Enhanced default configuration\n",
    "        default_config = {\n",
    "            'max_candidates': 20,          # Increased from 10\n",
    "            'max_distance': 100.0,         # Increased from 50.0\n",
    "            'sigma_z': 50.0,              # Adjusted for better GPS noise handling\n",
    "            'beta': 2.0,                  # Increased for better transition scoring\n",
    "            'min_prob_norm': 1e-7,        # Lowered for more flexibility\n",
    "            'max_speed': 50.0,            # Maximum expected speed (m/s)\n",
    "            'min_speed': 0.1,             # Minimum expected speed (m/s)\n",
    "            'angle_tolerance': np.pi/2,    # 90 degrees angle tolerance\n",
    "            'max_angle_penalty': 0.5,      # Maximum penalty for sharp turns\n",
    "            'distance_decay': 0.85,        # Distance decay factor\n",
    "            'sequential_matching': True    # Enable sequential matching for long trajectories\n",
    "        }\n",
    "        \n",
    "        if config:\n",
    "            default_config.update(config)\n",
    "        self.config = default_config\n",
    "        \n",
    "        self._init_spatial_index()\n",
    "        self.edge_to_nodes = self._build_edge_to_nodes()\n",
    "        self.node_to_edges = self._build_node_to_edges()\n",
    "        \n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def _init_spatial_index(self):\n",
    "        \"\"\"Initialize R-tree spatial index with improved error handling\"\"\"\n",
    "        try:\n",
    "            self.spatial_index = rtree.index.Index()\n",
    "            for idx, edge in self.edges_gdf.iterrows():\n",
    "                if edge.geometry is not None and not edge.geometry.is_empty:\n",
    "                    self.spatial_index.insert(idx, edge.geometry.bounds)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error initializing spatial index: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _build_edge_to_nodes(self) -> Dict[int, set]:\n",
    "        \"\"\"Build mapping from edge IDs to their endpoint nodes with validation\"\"\"\n",
    "        edge_to_nodes = {}\n",
    "        for idx, edge in self.edges_gdf.iterrows():\n",
    "            if edge.geometry is not None and not edge.geometry.is_empty:\n",
    "                coords = list(edge.geometry.coords)\n",
    "                if len(coords) >= 2:  # Ensure valid linestring\n",
    "                    edge_to_nodes[idx] = {\n",
    "                        self._get_node_id(coords[0]),\n",
    "                        self._get_node_id(coords[-1])\n",
    "                    }\n",
    "        return edge_to_nodes\n",
    "\n",
    "    def _build_node_to_edges(self) -> Dict[tuple, set]:\n",
    "        \"\"\"Build mapping from nodes to connected edge IDs\"\"\"\n",
    "        node_to_edges = {}\n",
    "        for edge_id, nodes in self.edge_to_nodes.items():\n",
    "            for node in nodes:\n",
    "                if node not in node_to_edges:\n",
    "                    node_to_edges[node] = set()\n",
    "                node_to_edges[node].add(edge_id)\n",
    "        return node_to_edges\n",
    "\n",
    "    def _get_node_id(self, coord: tuple) -> tuple:\n",
    "        \"\"\"Convert coordinate to node ID with improved precision\"\"\"\n",
    "        return tuple(round(x, 6) for x in coord)\n",
    "\n",
    "    def _find_candidates(self, point: Point) -> List[dict]:\n",
    "        \"\"\"Enhanced candidate finding with adaptive search radius and reduced distance threshold.\"\"\"\n",
    "        candidates = []\n",
    "        initial_distance = 30.0  # Start with a reduced search radius of 30 meters\n",
    "        max_attempts = 3\n",
    "        current_distance = initial_distance\n",
    "        \n",
    "        for attempt in range(max_attempts):\n",
    "            bounds = (\n",
    "                point.x - current_distance,\n",
    "                point.y - current_distance,\n",
    "                point.x + current_distance,\n",
    "                point.y + current_distance\n",
    "            )\n",
    "            \n",
    "            for idx in self.spatial_index.intersection(bounds):\n",
    "                edge = self.edges_gdf.loc[idx]\n",
    "                if edge.geometry is not None:\n",
    "                    dist = point.distance(edge.geometry)\n",
    "                    if dist <= current_distance:\n",
    "                        proj_point = edge.geometry.interpolate(\n",
    "                            edge.geometry.project(point)\n",
    "                        )\n",
    "                        candidates.append({\n",
    "                            'edge_id': idx,\n",
    "                            'distance': dist,\n",
    "                            'proj_point': proj_point,\n",
    "                            'edge': edge\n",
    "                        })\n",
    "            \n",
    "            if candidates:\n",
    "                break\n",
    "                \n",
    "            current_distance *= 1.5  # Increase search radius for next attempt\n",
    "        \n",
    "        # Sort by distance and apply adaptive limit\n",
    "        candidates.sort(key=lambda x: x['distance'])\n",
    "        return candidates[:self.config['max_candidates']]\n",
    "    \n",
    "    \n",
    "\n",
    "    def _calculate_emission_prob(self, point: Point, candidate: dict) -> float:\n",
    "        \"\"\"Enhanced emission probability calculation with improved scaling\"\"\"\n",
    "        distance = candidate['distance']\n",
    "        sigma_z = self.config['sigma_z']\n",
    "        \n",
    "        # Distance-based probability with decay\n",
    "        distance_factor = np.exp(-distance * self.config['distance_decay'])\n",
    "        \n",
    "        # Gaussian probability\n",
    "        gaussian_prob = np.exp(-0.5 * (distance / sigma_z) ** 2)\n",
    "        \n",
    "        # Combined probability\n",
    "        prob = gaussian_prob * distance_factor\n",
    "        \n",
    "        return max(prob, self.config['min_prob_norm'])\n",
    "\n",
    "    def _calculate_transition_prob(self, prev_edge: int, curr_edge: int,\n",
    "                                 prev_point: Point, curr_point: Point) -> float:\n",
    "        \"\"\"Enhanced transition probability calculation with improved angle handling\"\"\"\n",
    "        prev_nodes = self.edge_to_nodes[prev_edge]\n",
    "        curr_nodes = self.edge_to_nodes[curr_edge]\n",
    "        \n",
    "        # Check connectivity with more flexibility\n",
    "        connected = bool(prev_nodes.intersection(curr_nodes))\n",
    "        connectivity_score = 1.0 if connected else 0.3\n",
    "        \n",
    "        # Calculate angle similarity\n",
    "        dir1 = np.array(prev_point.coords[-1]) - np.array(prev_point.coords[0])\n",
    "        dir2 = np.array(curr_point.coords[-1]) - np.array(curr_point.coords[0])\n",
    "        \n",
    "        norm1 = np.linalg.norm(dir1)\n",
    "        norm2 = np.linalg.norm(dir2)\n",
    "        \n",
    "        if norm1 == 0 or norm2 == 0:\n",
    "            angle_score = 1.0\n",
    "        else:\n",
    "            cos_angle = np.dot(dir1, dir2) / (norm1 * norm2)\n",
    "            angle = np.arccos(np.clip(cos_angle, -1.0, 1.0))\n",
    "            \n",
    "            # Smoother angle penalty\n",
    "            angle_score = 1.0 - (angle / self.config['angle_tolerance']) * self.config['max_angle_penalty']\n",
    "            angle_score = max(angle_score, 1.0 - self.config['max_angle_penalty'])\n",
    "        \n",
    "        # Combined probability\n",
    "        prob = connectivity_score * angle_score\n",
    "        \n",
    "        return max(prob, self.config['min_prob_norm'])\n",
    "\n",
    "    def _viterbi_matching(self, points: List[Point], candidates_by_point: List[List[dict]]) -> List[Dict]:\n",
    "        \"\"\"Improved Viterbi algorithm with better numerical stability\"\"\"\n",
    "        n_points = len(points)\n",
    "        states = [{} for _ in range(n_points)]\n",
    "        \n",
    "        # Initialize first state with log probabilities\n",
    "        for candidate in candidates_by_point[0]:\n",
    "            edge_id = candidate['edge_id']\n",
    "            log_emission = np.log(self._calculate_emission_prob(points[0], candidate))\n",
    "            states[0][edge_id] = {\n",
    "                'log_prob': log_emission,\n",
    "                'prev': None,\n",
    "                'emission': log_emission,\n",
    "                'transition': 0.0\n",
    "            }\n",
    "        \n",
    "        # Forward pass with log probabilities\n",
    "        for t in range(1, n_points):\n",
    "            for candidate in candidates_by_point[t]:\n",
    "                curr_edge = candidate['edge_id']\n",
    "                log_emission = np.log(self._calculate_emission_prob(points[t], candidate))\n",
    "                \n",
    "                best_log_prob = float('-inf')\n",
    "                best_prev = None\n",
    "                best_transition = None\n",
    "                \n",
    "                for prev_edge, prev_state in states[t-1].items():\n",
    "                    trans_prob = self._calculate_transition_prob(\n",
    "                        prev_edge, curr_edge, points[t-1], points[t]\n",
    "                    )\n",
    "                    log_transition = np.log(trans_prob)\n",
    "                    \n",
    "                    log_prob = prev_state['log_prob'] + log_transition + log_emission\n",
    "                    \n",
    "                    if log_prob > best_log_prob:\n",
    "                        best_log_prob = log_prob\n",
    "                        best_prev = prev_edge\n",
    "                        best_transition = log_transition\n",
    "                \n",
    "                if best_prev is not None:\n",
    "                    states[t][curr_edge] = {\n",
    "                        'log_prob': best_log_prob,\n",
    "                        'prev': best_prev,\n",
    "                        'emission': log_emission,\n",
    "                        'transition': best_transition\n",
    "                    }\n",
    "        \n",
    "        # Convert log probabilities to normalized confidence scores\n",
    "        if states[-1]:\n",
    "            log_probs = np.array([state['log_prob'] for state in states[-1].values()])\n",
    "            max_log_prob = np.max(log_probs)\n",
    "            normalized_probs = np.exp(log_probs - max_log_prob)\n",
    "            normalized_probs /= np.sum(normalized_probs)\n",
    "            \n",
    "            for edge_id, norm_prob in zip(states[-1].keys(), normalized_probs):\n",
    "                states[-1][edge_id]['confidence'] = norm_prob\n",
    "        \n",
    "        return states\n",
    "\n",
    "    def _backtrack(self, states: List[Dict]) -> List[int]:\n",
    "        \"\"\"Backtrack to find the best path with improved handling of edge cases\"\"\"\n",
    "        if not states or not states[-1]:\n",
    "            return []\n",
    "        \n",
    "        path = []\n",
    "        current_edge = max(states[-1].items(), key=lambda x: x[1]['log_prob'])[0]\n",
    "        \n",
    "        for t in range(len(states) - 1, -1, -1):\n",
    "            path.append(current_edge)\n",
    "            if t > 0 and states[t][current_edge]['prev'] is not None:\n",
    "                current_edge = states[t][current_edge]['prev']\n",
    "        \n",
    "        return list(reversed(path))\n",
    "\n",
    "    def _sequential_matching(self, points: List[Point]) -> Dict:\n",
    "        \"\"\"Match long trajectories in sequential segments with overlap\"\"\"\n",
    "        segment_size = 30\n",
    "        overlap = 10\n",
    "        all_edges = []\n",
    "        segment_confidences = []\n",
    "        \n",
    "        for i in range(0, len(points), segment_size - overlap):\n",
    "            segment = points[i:i + segment_size]\n",
    "            if len(segment) < 2:\n",
    "                continue\n",
    "                \n",
    "            candidates = [self._find_candidates(p) for p in segment]\n",
    "            if not all(candidates):\n",
    "                continue\n",
    "                \n",
    "            states = self._viterbi_matching(segment, candidates)\n",
    "            path = self._backtrack(states)\n",
    "            \n",
    "            if path:\n",
    "                if states[-1] and path[-1] in states[-1]:\n",
    "                    segment_confidences.append(states[-1][path[-1]].get('confidence', 0.0))\n",
    "                    \n",
    "                # Remove overlap with previous segment\n",
    "                if all_edges and overlap > 0:\n",
    "                    all_edges = all_edges[:-overlap]\n",
    "                all_edges.extend(path)\n",
    "        \n",
    "        if not all_edges:\n",
    "            return {'success': False, 'edges': [], 'confidence': 0.0}\n",
    "        \n",
    "        # Calculate overall confidence as average of segment confidences\n",
    "        overall_confidence = np.mean(segment_confidences) if segment_confidences else 0.0\n",
    "            \n",
    "        return {\n",
    "            'success': True,\n",
    "            'edges': all_edges,\n",
    "            'confidence': overall_confidence\n",
    "        }\n",
    "\n",
    "    def match_trajectory(self, points: List[Tuple[float, float]]) -> Dict:\n",
    "        \"\"\"Match trajectory with improved error handling and validation\"\"\"\n",
    "        try:\n",
    "            if len(points) < 2:\n",
    "                return {'success': False, 'edges': [], 'confidence': 0.0}\n",
    "\n",
    "            point_objects = [Point(p) for p in points]\n",
    "            \n",
    "            # Use sequential matching for long trajectories\n",
    "            if self.config['sequential_matching'] and len(points) > 50:\n",
    "                return self._sequential_matching(point_objects)\n",
    "            \n",
    "            # Standard matching for shorter trajectories\n",
    "            candidates_by_point = [self._find_candidates(p) for p in point_objects]\n",
    "            \n",
    "            if not all(candidates_by_point):\n",
    "                return {'success': False, 'edges': [], 'confidence': 0.0}\n",
    "            \n",
    "            states = self._viterbi_matching(point_objects, candidates_by_point)\n",
    "            path = self._backtrack(states)\n",
    "            \n",
    "            if not path:\n",
    "                return {'success': False, 'edges': [], 'confidence': 0.0}\n",
    "            \n",
    "            confidence = states[-1][path[-1]].get('confidence', 0.0)\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'edges': path,\n",
    "                'confidence': confidence\n",
    "            }\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in match_trajectory: {str(e)}\")\n",
    "            return {'success': False, 'edges': [], 'confidence': 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Route Analyzer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RouteAnalyzer:\n",
    "    \n",
    "    \"\"\"Analyze mapped routes for frequently traversed and slow segments with enhanced analysis\"\"\"\n",
    "    def __init__(self, matcher, matched_results: List[Dict], output_dir: str = 'map_matching_results'):\n",
    "        self.matcher = matcher\n",
    "        self.matched_results = matched_results\n",
    "        self.output_dir = output_dir\n",
    "        self.analysis_dir = os.path.join(output_dir, 'route_analysis')\n",
    "        os.makedirs(self.analysis_dir, exist_ok=True)\n",
    "        \n",
    "        # Convert edges to WGS84 for visualization\n",
    "        self.edges_wgs84 = matcher.edges_gdf.to_crs('EPSG:4326')\n",
    "        \n",
    "        # Initialize segment statistics\n",
    "        self.segment_stats = self._initialize_segment_stats()\n",
    "    \n",
    "    \n",
    "    def _analyze_trajectory_segment(self, edge_id: int, edge_geom, coords: List[tuple], timestamps: List[int]) -> tuple:\n",
    "        \"\"\"Analyze trajectory segment using actual timestamps between trajectory points.\"\"\"\n",
    "        edge_length = edge_geom.length\n",
    "        if len(coords) < 2 or len(timestamps) < 2:\n",
    "            return 0, 0\n",
    "\n",
    "        # Track time intervals and calculate travel time using timestamps\n",
    "        time_diffs = [timestamps[i+1] - timestamps[i] for i in range(len(timestamps) - 1)]\n",
    "        total_time = sum(time_diffs)\n",
    "\n",
    "        if total_time <= 0:\n",
    "            return 0, 0\n",
    "        \n",
    "        # Calculate speed (m/s)\n",
    "        speed = edge_length / total_time\n",
    "\n",
    "        # Validate speed to be within realistic ranges for urban areas\n",
    "        MIN_SPEED = 1.389  # 5 km/h in m/s\n",
    "        MAX_SPEED = 13.89  # 50 km/h in m/s\n",
    "\n",
    "        if speed < MIN_SPEED:\n",
    "            speed = MIN_SPEED\n",
    "            total_time = edge_length / MIN_SPEED\n",
    "        elif speed > MAX_SPEED:\n",
    "            speed = MAX_SPEED\n",
    "            total_time = edge_length / MAX_SPEED\n",
    "        \n",
    "        return speed, total_time\n",
    "    \n",
    "    def _initialize_segment_stats(self) -> Dict:\n",
    "        \"\"\"Initialize statistics for each road segment using actual timestamps\"\"\"\n",
    "        stats = {}\n",
    "        \n",
    "        for result in self.matched_results:\n",
    "            if not result['match_result']['success']:\n",
    "                continue\n",
    "            \n",
    "            coords = result['original_coords']\n",
    "            edges = result['match_result']['edges']\n",
    "            timestamps = result['timestamps']\n",
    "            \n",
    "            if len(coords) < 2 or not edges or timestamps is None:\n",
    "                continue\n",
    "            \n",
    "            # Process each edge in the matched path\n",
    "            for edge_id in edges:\n",
    "                if edge_id not in stats:\n",
    "                    stats[edge_id] = {\n",
    "                        'traverse_count': 0,\n",
    "                        'length': 0,\n",
    "                        'speeds': [],\n",
    "                        'times': [],\n",
    "                        'distance_traversed': 0  # Changed from total_traversed to distance_traversed\n",
    "                    }\n",
    "                \n",
    "                edge_geom = self.matcher.edges_gdf.loc[edge_id].geometry\n",
    "                stats[edge_id]['length'] = edge_geom.length\n",
    "                stats[edge_id]['distance_traversed'] += edge_geom.length\n",
    "                \n",
    "                speed, time = self._analyze_trajectory_segment(\n",
    "                    edge_id, \n",
    "                    edge_geom, \n",
    "                    coords,\n",
    "                    timestamps\n",
    "                )\n",
    "                \n",
    "                if speed > 0 and time > 0:\n",
    "                    stats[edge_id]['traverse_count'] += 1\n",
    "                    stats[edge_id]['speeds'].append(speed)\n",
    "                    stats[edge_id]['times'].append(time)\n",
    "        \n",
    "        # Calculate aggregate statistics\n",
    "        for edge_id, edge_stats in stats.items():\n",
    "            if edge_stats['traverse_count'] > 0 and edge_stats['speeds']:\n",
    "                edge_stats['avg_speed'] = np.mean(edge_stats['speeds'])\n",
    "                edge_stats['speed_std'] = np.std(edge_stats['speeds']) if len(edge_stats['speeds']) > 1 else 0\n",
    "                edge_stats['avg_time'] = np.mean(edge_stats['times'])\n",
    "                edge_stats['congestion_index'] = (edge_stats['speed_std'] / edge_stats['avg_speed'] \n",
    "                                                if edge_stats['avg_speed'] > 0 else 0)\n",
    "        \n",
    "        return stats\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    def get_most_traversed_segments(self, n: int = 10) -> List[Dict]:\n",
    "        \"\"\"Return the n most frequently traversed road segments\"\"\"\n",
    "        segments = []\n",
    "        for edge_id, stats in self.segment_stats.items():\n",
    "            if stats['traverse_count'] > 0:\n",
    "                segments.append({\n",
    "                    'edge_id': edge_id,\n",
    "                    'count': stats['traverse_count'],\n",
    "                    'geometry': self.edges_wgs84.loc[edge_id].geometry,\n",
    "                    'avg_speed': stats.get('avg_speed', 0),\n",
    "                    'avg_time': stats.get('avg_time', 0),\n",
    "                    'length': stats['length'],\n",
    "                    'speed_std': stats.get('speed_std', 0),\n",
    "                    'congestion_index': stats.get('congestion_index', 0),\n",
    "                    'distance_traversed': stats['distance_traversed']  # Added this line\n",
    "                })\n",
    "        \n",
    "        segments.sort(key=lambda x: x['count'], reverse=True)\n",
    "        return segments[:n]\n",
    "    \n",
    "    def get_slowest_segments(self, n: int = 10) -> List[Dict]:\n",
    "        \"\"\"Return the n slowest road segments based on average speed\"\"\"\n",
    "        segments = []\n",
    "        min_length = 50  # Only consider segments longer than 50m\n",
    "        \n",
    "        for edge_id, stats in self.segment_stats.items():\n",
    "            if (stats['traverse_count'] > 0 and \n",
    "                stats.get('avg_speed', 0) > 0 and \n",
    "                stats['length'] >= min_length):\n",
    "                segments.append({\n",
    "                    'edge_id': edge_id,\n",
    "                    'avg_speed': stats['avg_speed'],\n",
    "                    'avg_time': stats['avg_time'],\n",
    "                    'count': stats['traverse_count'],\n",
    "                    'geometry': self.edges_wgs84.loc[edge_id].geometry,\n",
    "                    'length': stats['length'],\n",
    "                    'speed_std': stats['speed_std'],\n",
    "                    'congestion_index': stats['congestion_index']\n",
    "                })\n",
    "        \n",
    "        segments.sort(key=lambda x: x['avg_speed'])\n",
    "        return segments[:n]\n",
    "\n",
    "    def generate_enhanced_report(self, most_traversed: List[Dict], slowest_segments: List[Dict]) -> str:\n",
    "        \"\"\"Generate a comprehensive analysis report\"\"\"\n",
    "        report = \"Enhanced Route Analysis Report\\n\"\n",
    "        report += \"===========================\\n\\n\"\n",
    "        \n",
    "        # Most Traversed Segments Analysis\n",
    "        report += \"Most Frequently Traversed Segments:\\n\"\n",
    "        report += \"--------------------------------\\n\"\n",
    "        for idx, segment in enumerate(most_traversed, 1):\n",
    "            speed_kmh = segment['avg_speed'] * 3.6  # Convert to km/h\n",
    "            \n",
    "            report += f\"Rank {idx}:\\n\"\n",
    "            report += f\"  Edge ID: {segment['edge_id']}\\n\"\n",
    "            report += f\"  Traverse Count: {segment['count']}\\n\"\n",
    "            report += f\"  Length: {segment['length']:.2f} meters\\n\"\n",
    "            report += f\"  Average Travel Time: {segment['avg_time']:.2f} seconds\\n\"\n",
    "            report += f\"  Average Speed: {segment['avg_speed']:.2f} m/s\\n\"\n",
    "            report += f\"  Average Speed (km/h): {speed_kmh:.2f} km/h\\n\"\n",
    "            report += f\"  Speed Std Dev: {segment['speed_std']:.2f} m/s\\n\"\n",
    "            report += f\"  Total Distance Traversed: {segment['distance_traversed']:.2f} meters\\n\"\n",
    "            report += f\"  Congestion Index: {segment['congestion_index']:.3f}\\n\\n\"\n",
    "        \n",
    "        # Slowest Segments Analysis\n",
    "        report += \"\\nSlowest Segments Analysis:\\n\"\n",
    "        report += \"------------------------\\n\"\n",
    "        for idx, segment in enumerate(slowest_segments, 1):\n",
    "            speed_kmh = segment['avg_speed'] * 3.6\n",
    "            \n",
    "            report += f\"Rank {idx}:\\n\"\n",
    "            report += f\"  Edge ID: {segment['edge_id']}\\n\"\n",
    "            report += f\"  Length: {segment['length']:.2f} meters\\n\"\n",
    "            report += f\"  Average Travel Time: {segment['avg_time']:.2f} seconds\\n\"\n",
    "            report += f\"  Average Speed: {segment['avg_speed']:.2f} m/s\\n\"\n",
    "            report += f\"  Average Speed (km/h): {speed_kmh:.2f} km/h\\n\"\n",
    "            report += f\"  Traverse Count: {segment['count']}\\n\"\n",
    "            report += f\"  Speed Std Dev: {segment['speed_std']:.2f} m/s\\n\"\n",
    "            report += f\"  Congestion Index: {segment['congestion_index']:.3f}\\n\\n\"\n",
    "        \n",
    "        # Overall Network Statistics\n",
    "        report += \"\\nOverall Network Statistics:\\n\"\n",
    "        report += \"-------------------------\\n\"\n",
    "        all_segments = len(self.segment_stats)\n",
    "        traversed_segments = sum(1 for stats in self.segment_stats.values() if stats['traverse_count'] > 0)\n",
    "        \n",
    "        # Calculate average network speed (excluding zero speeds)\n",
    "        valid_speeds = [stats['avg_speed'] for stats in self.segment_stats.values() \n",
    "                       if stats['traverse_count'] > 0 and stats['avg_speed'] > 0]\n",
    "        avg_network_speed = np.mean(valid_speeds) if valid_speeds else 0\n",
    "        \n",
    "        report += f\"Total Road Segments: {all_segments}\\n\"\n",
    "        report += f\"Traversed Segments: {traversed_segments} ({(traversed_segments/all_segments)*100:.1f}%)\\n\"\n",
    "        report += f\"Average Traversal Count: {np.mean([stats['traverse_count'] for stats in self.segment_stats.values()]):.2f}\\n\"\n",
    "        report += f\"Average Network Speed: {avg_network_speed:.2f} m/s ({avg_network_speed * 3.6:.2f} km/h)\\n\"\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    \n",
    "    \n",
    "    def analyze_and_visualize_enhanced(self):\n",
    "        \"\"\"Perform complete route analysis with error handling\"\"\"\n",
    "        try:\n",
    "            # Get analyzed segments\n",
    "            most_traversed = self.get_most_traversed_segments()\n",
    "            slowest_segments = self.get_slowest_segments()\n",
    "            \n",
    "            if not most_traversed and not slowest_segments:\n",
    "                logging.warning(\"No valid segments found for analysis\")\n",
    "                return [], [], None\n",
    "            \n",
    "            # Generate enhanced report\n",
    "            report = self.generate_enhanced_report(most_traversed, slowest_segments)\n",
    "            \n",
    "            # Save report\n",
    "            report_path = os.path.join(self.analysis_dir, 'enhanced_route_analysis_report.txt')\n",
    "            with open(report_path, 'w') as f:\n",
    "                f.write(report)\n",
    "            \n",
    "            # Create visualizations if we have segments\n",
    "            if most_traversed:\n",
    "                self.visualize_segments_enhanced(\n",
    "                    most_traversed,\n",
    "                    \"Most Frequently Traversed Road Segments\",\n",
    "                    \"most_traversed_segments_enhanced.html\",\n",
    "                    \"traverse frequency\",\n",
    "                    ['#fff7ec', '#fee8c8', '#fdd49e', '#fdbb84', '#fc8d59', '#ef6548', '#d7301f', '#990000']\n",
    "                )\n",
    "            \n",
    "            if slowest_segments:\n",
    "                self.visualize_segments_enhanced(\n",
    "                    slowest_segments,\n",
    "                    \"Road Segments with Highest Average Travel Time\",\n",
    "                    \"slowest_segments_enhanced.html\",\n",
    "                    \"average travel time\",\n",
    "                    ['#f7fcfd', '#e0ecf4', '#bfd3e6', '#9ebcda', '#8c96c6', '#8c6bb1', '#88419d', '#6e016b']\n",
    "                )\n",
    "            \n",
    "            return most_traversed, slowest_segments, report_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in analyze_and_visualize_enhanced: {str(e)}\")\n",
    "            return [], [], None\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def visualize_segments_enhanced(self, segments: List[Dict], title: str, filename: str, \n",
    "                                  metric_name: str, color_scheme: List[str]):\n",
    "        \"\"\"Create an enhanced interactive visualization with error handling\"\"\"\n",
    "        # Check if we have segments to visualize\n",
    "        if not segments:\n",
    "            logging.warning(f\"No segments to visualize for {title}\")\n",
    "            return\n",
    "        \n",
    "        # Create base map centered on Porto\n",
    "        center_lat, center_lon = 41.1579, -8.6291\n",
    "        m = folium.Map(\n",
    "            location=[center_lat, center_lon],\n",
    "            zoom_start=13,\n",
    "            tiles='cartodbpositron'\n",
    "        )\n",
    "        \n",
    "        # Add all road network in very light gray\n",
    "        for _, edge in self.edges_wgs84.iterrows():\n",
    "            if edge.geometry is not None:\n",
    "                coords = [(y, x) for x, y in edge.geometry.coords]\n",
    "                folium.PolyLine(\n",
    "                    coords,\n",
    "                    weight=1,\n",
    "                    color='lightgray',\n",
    "                    opacity=0.2\n",
    "                ).add_to(m)\n",
    "        \n",
    "        # Create color scale for highlighted segments\n",
    "        metric_display = 'Traverse Count'\n",
    "        if segments:  # Check if we have any segments\n",
    "            if 'count' in segments[0]:\n",
    "                values = [seg['count'] for seg in segments]\n",
    "            else:\n",
    "                values = [seg.get('avg_time', 0) for seg in segments]\n",
    "                metric_display = 'Average Travel Time'\n",
    "                \n",
    "            max_value = max(values) if values else 1\n",
    "            min_value = min(values) if values else 0\n",
    "            \n",
    "            colormap = LinearColormap(\n",
    "                colors=color_scheme,\n",
    "                vmin=min_value,\n",
    "                vmax=max_value,\n",
    "            )\n",
    "            \n",
    "            # Add highlighted segments with rank numbers\n",
    "            for rank, segment in enumerate(segments, 1):\n",
    "                # Get coordinates and create line\n",
    "                coords = [(y, x) for x, y in segment['geometry'].coords]\n",
    "                value = segment.get('count', 0) or segment.get('avg_time', 0)\n",
    "                \n",
    "                # Create detailed popup text\n",
    "                if 'count' in segment:\n",
    "                    popup_text = (\n",
    "                        f\"<div style='font-family: Arial; font-size: 12px;'>\"\n",
    "                        f\"<strong>Rank: {rank}</strong><br>\"\n",
    "                        f\"Edge ID: {segment['edge_id']}<br>\"\n",
    "                        f\"Traverse count: {segment['count']}<br>\"\n",
    "                        f\"Average Speed: {segment['avg_speed']:.2f} m/s<br>\"\n",
    "                        f\"Average Speed: {(segment['avg_speed'] * 3.6):.2f} km/h<br>\"\n",
    "                        f\"Length: {segment['length']:.2f} m<br>\"\n",
    "                        f\"</div>\"\n",
    "                    )\n",
    "                else:\n",
    "                    popup_text = (\n",
    "                        f\"<div style='font-family: Arial; font-size: 12px;'>\"\n",
    "                        f\"<strong>Rank: {rank}</strong><br>\"\n",
    "                        f\"Edge ID: {segment['edge_id']}<br>\"\n",
    "                        f\"Avg time: {segment['avg_time']:.2f} s<br>\"\n",
    "                        f\"Average Speed: {segment['avg_speed']:.2f} m/s<br>\"\n",
    "                        f\"Average Speed: {(segment['avg_speed'] * 3.6):.2f} km/h<br>\"\n",
    "                        f\"Traverse count: {segment['count']}<br>\"\n",
    "                        f\"</div>\"\n",
    "                    )\n",
    "                \n",
    "                # Add the segment line\n",
    "                folium.PolyLine(\n",
    "                    coords,\n",
    "                    weight=5,\n",
    "                    color=colormap(value),\n",
    "                    opacity=0.8,\n",
    "                    popup=folium.Popup(popup_text, max_width=200)\n",
    "                ).add_to(m)\n",
    "                \n",
    "                # Calculate midpoint\n",
    "                if len(coords) > 1:\n",
    "                    mid_lat = sum(coord[0] for coord in coords) / len(coords)\n",
    "                    mid_lon = sum(coord[1] for coord in coords) / len(coords)\n",
    "                    midpoint = (mid_lat, mid_lon)\n",
    "                    \n",
    "                    # Create a single marker with the circle and rank number\n",
    "                    folium.Marker(\n",
    "                        location=midpoint,\n",
    "                        icon=DivIcon(\n",
    "                            icon_size=(24, 24),\n",
    "                            icon_anchor=(12, 12),\n",
    "                            html=f'''\n",
    "                                <div style=\"\n",
    "                                    width: 24px;\n",
    "                                    height: 24px;\n",
    "                                    background-color: transparent;\n",
    "                                    border: 2px solid black;\n",
    "                                    border-radius: 50%;\n",
    "                                    display: flex;\n",
    "                                    align-items: center;\n",
    "                                    justify-content: center;\n",
    "                                    font-size: 14px;\n",
    "                                    font-weight: bold;\n",
    "                                    font-family: Arial;\n",
    "                                    color: black;\n",
    "                                \">\n",
    "                                    {rank}\n",
    "                                </div>\n",
    "                            '''\n",
    "                        )\n",
    "                    ).add_to(m)\n",
    "            \n",
    "            # Add color scale\n",
    "            colormap.add_to(m)\n",
    "            colormap.caption = metric_display\n",
    "        \n",
    "        # Add enhanced title and legend\n",
    "        title_html = f'''\n",
    "            <div style=\"position: fixed; \n",
    "                        top: 10px; left: 50%; \n",
    "                        transform: translateX(-50%);\n",
    "                        background-color: white;\n",
    "                        border-radius: 5px;\n",
    "                        padding: 10px;\n",
    "                        z-index: 1000;\n",
    "                        box-shadow: 0 2px 5px rgba(0,0,0,0.2);\">\n",
    "                <h4 style=\"margin: 0; color: #2c3e50;\">{title}</h4>\n",
    "                <p style=\"margin: 5px 0 0 0; font-size: 12px; color: #7f8c8d;\">\n",
    "                    Top {len(segments)} segments ranked by {metric_name}\n",
    "                </p>\n",
    "            </div>\n",
    "        '''\n",
    "        m.get_root().html.add_child(folium.Element(title_html))\n",
    "        \n",
    "        # Save map\n",
    "        m.save(os.path.join(self.analysis_dir, filename))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading road network...\n",
      "Loading trajectory data...\n",
      "Initializing matcher...\n",
      "Processing trajectories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [00:34<00:00, 43.37it/s]\n"
     ]
    }
   ],
   "source": [
    "def process_trajectory_with_time(row):\n",
    "    \"\"\"Process trajectory with actual timestamps from the data.\"\"\"\n",
    "    try:\n",
    "        coords = ast.literal_eval(row['POLYLINE'])\n",
    "        if not coords or len(coords) < 2:\n",
    "            return None\n",
    "            \n",
    "        # Get the starting timestamp from data\n",
    "        start_timestamp = int(row['TIMESTAMP'])  # Convert to int explicitly\n",
    "        \n",
    "        # Generate timestamps for each coordinate assuming 15-second intervals between points\n",
    "        timestamps = [start_timestamp + i * 15 for i in range(len(coords))]\n",
    "        \n",
    "        return {\n",
    "            'coords': coords,\n",
    "            'timestamps': timestamps,\n",
    "            'start_timestamp': start_timestamp  # Keep the start_timestamp as a separate entry\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error processing trajectory: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Load road network\n",
    "    print(\"Loading road network...\")\n",
    "    G = ox.graph_from_place('Porto, Portugal', network_type='drive')\n",
    "    nodes, edges = ox.graph_to_gdfs(G)\n",
    "    \n",
    "    # Convert to UTM coordinates for accurate distance calculations\n",
    "    utm_crs = 'EPSG:32629'  # UTM zone 29N for Porto\n",
    "    edges = edges.to_crs(utm_crs)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    # Load trajectory data\n",
    "    print(\"Loading trajectory data...\")\n",
    "    df = pd.read_csv('kraggle_data/train/train.csv', nrows=1500)\n",
    "    \n",
    "    # Initialize matcher\n",
    "    print(\"Initializing matcher...\")\n",
    "    config = {\n",
    "        'max_candidates': 8,\n",
    "        'max_distance': 100.0,\n",
    "        'sigma_z': 10.0,\n",
    "        'beta': 1.5\n",
    "    }\n",
    "    matcher = EnhancedViterbiMatcher(G, edges, config)\n",
    "    \n",
    "    # Process trajectories\n",
    "    print(\"Processing trajectories...\")\n",
    "    matched_results = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        trajectory_data = process_trajectory_with_time(row)\n",
    "        if trajectory_data:\n",
    "            # Convert coordinates to UTM\n",
    "            point_gdf = gpd.GeoDataFrame(\n",
    "                geometry=[Point(x, y) for x, y in trajectory_data['coords']],\n",
    "                crs='EPSG:4326'\n",
    "            ).to_crs(utm_crs)\n",
    "            \n",
    "            utm_coords = [(p.x, p.y) for p in point_gdf.geometry]\n",
    "            \n",
    "            # Match trajectory\n",
    "            result = matcher.match_trajectory(utm_coords)\n",
    "            \n",
    "            if result['success']:\n",
    "                matched_results.append({\n",
    "                    'match_result': result,\n",
    "                    'original_coords': trajectory_data['coords'],\n",
    "                    'timestamps': trajectory_data['timestamps'],  # Add timestamps here\n",
    "                    'start_timestamp': trajectory_data['start_timestamp']\n",
    "                })\n",
    "                #print(f\"Successfully matched trajectory {idx}\")\n",
    "    \n",
    "    if matched_results:\n",
    "        output_dir = 'map_matching_results'\n",
    "        \n",
    "        # Perform route analysis\n",
    "        logger.info(\"Performing route analysis...\")\n",
    "        # Create analyzer instance\n",
    "        analyzer = RouteAnalyzer(matcher, matched_results, output_dir)\n",
    "        # Perform enhanced analysis and visualization\n",
    "        most_traversed, slowest_segments, report_path = analyzer.analyze_and_visualize_enhanced()\n",
    "        \n",
    "        logger.info(f\"Successfully analyzed {len(matched_results)} trajectories\")\n",
    "        logger.info(f\"Reports and visualizations saved in {output_dir} folder\")\n",
    "    else:\n",
    "        logger.warning(\"No trajectories were successfully matched\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urbancom2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
